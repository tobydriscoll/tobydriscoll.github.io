# Adaptive Runge--Kutta

The derivation and analysis of methods for initial-value problems usually assumes a fixed step size $h$. While the error behavior $O(h^p)$ is guaranteed by \thmref{onestepGTE} as $h\rightarrow 0$, this bound comes with an unknowable constant, and it is not very useful as a guide to the numerical value of the error at any particular value of $h$. Furthermore, as we saw in {doc}`../localapprox/adaptive` with numerical integration, in many problems a fixed value of $h$ throughout $a\le t \le b$ is far from the most efficient strategy.

In response we will employ the basic strategy of {doc}`../localapprox/adaptive`: adapt the time step size in order to reach an accuracy goal, as measured by an error estimate formed from computing multiple approximations. The details are quite different, however.

## Error estimation

Suppose that, starting from a given value $u_i$ and using a step size $h$, we run one step of *two* RK methods simultaneously: one method with order $p$, producing $u_{i+1}$, and the other method with order $p+1$, producing $\widetilde{u}_{i+1}$. In most circumstances, we can expect that $\widetilde{\mathbf{u}}_{i+1}$ is a much better approximation to the solution than $\mathbf{u}_{i+1}$ is. So it seems reasonable to use $E_i(h)=|\widetilde{\mathbf{u}}_{i+1}-\mathbf{u}_{i+1}|$ (in the vector case, a norm) as an estimate of the actual local error made by the $p$th order method. 

If our goal is to keep error less than some predetermined value $\epsilon$, we could decide to accept the new solution value if $E_i<\epsilon$ and otherwise reject it. (Even though the estimate $E_i$ is meant to go with the *less* accurate proposed value $\mathbf{u}_{i+1}$, it's hard to resist the temptation to keep the more accurate value $\widetilde{\mathbf{u}}_{i+1}$ instead, and this is common in practice.)

Regardless of whether $E_i<\epsilon$ and we accept the step, we now ask a question: looking back, what step size *should* we have taken to just meet our error target $\epsilon$? Let's speculate that $E_i(h)\approx C h^{p+1}$ for an unknown constant $C$, given the behavior of local truncation error as $h\rightarrow 0$. If we had used a step size $qh$ for some $q>0$, then trivially, $E_i(qh)\approx C q^{p+1}h^{p+1}$ is what we would expect to get. Our best guess for $q$ would be to set $E_i(qh)\approx \epsilon$, or

```{math}
  :label: adaptRKlocal
  q \approx \left(\frac{\epsilon}{E_i}\right)^{1/(p+1)}.
```

Whether or not we accepted the value proposed for $t=t_{i+1}$, we will adjust the step size to $qh$ for the next attempted step.

Given what we know about the connection between local and global errors, we might instead decide that controlling the normalized contribution to *global* error, which is closer to $E_i(qh)/(qh)$, is more reasonable. Then we end up with

```{math}
  :label: adaptRKglobal
  q \le \left(\frac{\epsilon}{E_i}\right)^{1/p}.
```

Experts have different recommendations about whether to use {eq}`adaptRKlocal` or {eq}`adaptRKglobal`. Even though {eq}`adaptRKglobal` appears to be more in keeping with our assumptions about global errors, modern practice seems to favor {eq}`adaptRKlocal`.

## Embedded formulas

We have derived two useful pieces of information: a reasonable estimate of the actual value of the local (or global) error, and a prediction how the step size will affect that error. Together they can be used to adapt step size and keep errors near some target level. But there remains one more important twist to the story.

```{margin}
Embedded RK formulas are a pair of RK methods whose stages share the same internal $f$ evaluations.
```

At first glance, it would seem that to use (for example) any pair of second- and third-order RK methods to get the $\mathbf{u}_{i+1}$ and $\widetilde{\mathbf{u}}_{i+1}$ needed for adaptive error control, we need at least $2+3=5$ evaluations of $f(t,y)$ for each attempted time step.  This is more than double the computational work needed by the second-order method without adaptivity. Fortunately, the marginal cost of adaptation can be substantially reduced by using **embedded Runge--Kutta** Embedded RK formulas are a pair of RK methods whose stages share the same internal $f$ evaluations, combining them differently in order to get estimates of two different orders of accuracy.

A good example of an embedded method is the **Bogacki--Shampine** (BS23) formula, given by the table

```{math}
  :label: bs23
  \begin{array}{r|cccc}
    0                  & \rule{0pt}{2.75ex} &                    &                    &                    \\
    \frac{1}{2}        & \frac{1}{2}        & \rule{0pt}{2.75ex} &                    &                    \\
    \frac{3}{4}        & 0                  & \frac{3}{4}        & \rule{0pt}{2.75ex} &                    \\
     1                 & \frac{2}{9}        & \frac{1}{3}        & \frac{4}{9}        & \rule{0pt}{2.75ex} \\[2pt] \hline
    \rule{0pt}{2.75ex} & \frac{2}{9}        & \frac{1}{3}        & \frac{4}{9}        & 0                  \\[2pt] \hline
    \rule{0pt}{2.75ex} & \frac{7}{24}       & \frac{1}{4}        & \frac{1}{3}        & \frac{1}{8}
  \end{array}
```

The top part of the table describes four stages in the usual RK fashion. The last two rows describe how to construct a second-order estimate $\mathbf{u}_{i+1}$ and a third-order estimate $\widetilde{\mathbf{u}}_{i+1}$ by taking different combinations of those stages.

Both the "ode23" and the "ode45" solvers built into MATLAB are based on embedded RK formulas.

## Implementation

(function-rk23)=

````{proof:function} rk23
**Adaptive IVP solver based on embedded RK formulas.**

```{code-block} julia
:lineno-start: 1
```
````

Our implementation of an embedded second/third order (RK23) code is given in {ref}`function-rk23`. It has a few details that are worth explaining.

First, as in {eq}`absreltolerance`, we use a combination of absolute and relative tolerances to judge the acceptability of a solution value. Second, we have a check whether $t_i+h$ equals $t_i$, which looks odd. This check is purely about roundoff error, because $h$ can become so small that it no longer changes the floating point value of $t_i$. When this happens, it's often a sign that the underlying exact solution has a singularity near $t=t_i$. Third, some adjustments are made to the step size prediction factor $q$. We use a smaller value than {eq}`adaptRKlocal`, to be conservative about the many assumptions that were made to derive it. We also prevent a huge jump in the step size for the same reason. And, we make sure that our final step doesn't take us past the requested end of the domain.

Finally, there is some careful programming done to avoid redundant evaluations of $f$. As written in {eq}`bs23`, there seem to be four stages needed to find the paired second- and third-order estimates. This is unfortunate, since there are three-stage formulas of order three. But BS23 has a special property called "first same as last" (FSAL). If the proposed step is accepted, the final stage computed in stepping from $t_i$ to $t_{i+1}$ is identical to the *first* stage needed to step from $t_{i+1}$ to $t_{i+2}$, so in that sense one of the stage evaluations comes at no cost. This detail is addressed in our code.

```{proof:demo}
{doc}`demos/adapt-basic`
```

````{proof:example}
In {doc}`demos/basics-sing` we found an IVP that appears to blow up in a finite amount of time. Because the solution increases so rapidly as it approaches the blowup, adaptive stepping is required to even get close. In fact it's the failure of adaptivity that is used to get an idea of when the singularity occurs.
```{proof:demo}
{doc}`demos/adapt-sing`
```
````

Often the steps chosen adaptively clearly correspond to identifiable features of the solution. However, there are so-called *stiff problems* in which the time steps seem unreasonably small in relation to the observable behavior of the solution. These problems benefit from a particular type of solver and will be taken up in {doc}`implicit` and, in more mathematical detail, in \secref{stiffness}.

<!-- \begin{exercises}
  \input{ivpns/exercises/Adaptive}
\end{exercises} -->
