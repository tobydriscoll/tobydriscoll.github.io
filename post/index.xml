<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | Toby Driscoll</title><link>https://tobydriscoll.net/post/</link><atom:link href="https://tobydriscoll.net/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2023 by Tobin A. Driscoll</copyright><lastBuildDate>Tue, 16 Jul 2019 11:55:12 -0400</lastBuildDate><image><url>https://tobydriscoll.net/media/logo_hueac33246f157bcd904a645b7aca24b63_24178_300x300_fit_lanczos_3.png</url><title>Posts</title><link>https://tobydriscoll.net/post/</link></image><item><title>The sorry state of teaching ODEs</title><link>https://tobydriscoll.net/blog/the-sorry-state-of-teaching-odes/</link><pubDate>Tue, 16 Jul 2019 11:55:12 -0400</pubDate><guid>https://tobydriscoll.net/blog/the-sorry-state-of-teaching-odes/</guid><description>&lt;p>I&amp;rsquo;ve spent the last two spring semesters teaching ODEs (ordinary differential equations) to a total of about 170 biomedical and chemical engineering majors. The content is dictated by a number of constraints: the perceived desires of the client departments, multiple instructors, all of whom have more experience with the course than I do, and traditional expectations. Based on a limited survey of popular textbooks (&lt;a href="https://www.amazon.com/Differential-Equations-Engineers-David-Kalbaugh/dp/1498798810" target="_blank" rel="noopener">this&lt;/a>, &lt;a href="https://www.amazon.com/Engineering-Differential-Equations-Theory-Applications/dp/1441979182" target="_blank" rel="noopener">this&lt;/a>, and our choice, &lt;a href="https://www.amazon.com/Differential-Equations-Introduction-Methods-Applications/dp/1118531779" target="_blank" rel="noopener">Brannan and Boyce&lt;/a>), many courses like this are quite similar. (You can look through my &lt;a href="https://tobydriscoll.net/udmath305/notes.html" target="_blank" rel="noopener">notes&lt;/a>, rough as they are.)&lt;/p>
&lt;p>What I find particularly disheartening about these books, and the courses they imply, is that they share an overwhelming emphasis on hand computations of formulas to produce &amp;ldquo;solutions&amp;rdquo; of particular problems. I scare-quote &amp;ldquo;solutions&amp;rdquo; here because it&amp;rsquo;s become clear that the majority of students can&amp;rsquo;t readily comprehend their own outputs. And really, why should they? Most homework (and, by implication, exam) problems are going to simply ask them to produce the output, not interpret it.&lt;/p>
&lt;p>Not long ago, this approach was not hard to justify. What could be more important to building a bridge or &lt;a href="https://www.bbc.co.uk/programmes/w13xttx2" target="_blank" rel="noopener">going to the Moon&lt;/a> than getting the right answer? Of course it&amp;rsquo;s absurd to suggest that getting the right answer no longer matters. But what &lt;em>does&lt;/em> no longer matter is whether the student can crank that answer out by hand.&lt;/p>
&lt;p>I&amp;rsquo;m old enough to remember a middle school lecture on finding values of logarithms by linearly interpolating values from a table. I sincerely hope this is no longer done, though I can imagine the objections when it was proposed for elimination: &amp;ldquo;Isn&amp;rsquo;t it good for them to learn linear interpolation? What if they&amp;rsquo;re stuck on a desert island without a calculator? How will they recognize when the calculator gives a clearly wrong answer?&amp;rdquo; You&amp;rsquo;ll hear much the same said about many other dubious lessons that are still very much alive today, such as days of calculus devoted to drawing (bad) graphs by hand.&lt;/p>
&lt;p>Long after the original driving need to compute or do something by hand has vanished, we&amp;rsquo;re able to supply alternative reasons to do it. These reasons seem to come in two flavors: hypothetical utility and tradition. Claims that students &amp;ldquo;learn concepts better&amp;rdquo; by hand computing are almost never substantiated by evidence, and in any case tend to beg the question of which concepts are in play. I think we can conclusively retire the &amp;ldquo;they may not always have a calculator&amp;rdquo; form of manufactured need. As for grimly continuing a tradition, sometimes disguised within a beguiling &amp;ldquo;what&amp;rsquo;s the harm?&amp;rdquo; phrasing, what you get is a recipe for stagnation and irrelevance. The cost is always a lost opportunity to teach something else.&lt;/p>
&lt;p>I would not suggest that students never solve an example problem by hand. An ODE course graduate who cannot solve, say, $y^{\prime\prime} + 4y=1$ has missed something vital. But I don&amp;rsquo;t see what&amp;rsquo;s to be gained by practicing on $y^{\prime\prime} + 5y&amp;rsquo; + 4y=t^2e^{-2t}$. All the time and focus needed to wrest an answer from that problem is purely mechanical, never going beyond the application of a rigid algorithm.&lt;/p>
&lt;p>Not only are such algorithms much better performed by a computer, they are incredibly fragile. The &lt;a href="https://www.wolframalpha.com/input/?i=solve&amp;#43;y%27%27&amp;#43;%2B&amp;#43;t*y&amp;#43;%3D&amp;#43;1" target="_blank" rel="noopener">analytical solution&lt;/a> of $y^{\prime\prime} + ty=1$ is &lt;em>so much&lt;/em> harder to produce and interpret than $y^{\prime\prime} + 4y=1$ that it might as well be of a different universe altogether. And that&amp;rsquo;s still just a linear problem!&lt;/p>
&lt;p>It&amp;rsquo;s sensible to focus on the most fundamental ODE problems with analytical solutions, like the linearly damped oscillator and the logistic equation. We can totally dissect those problems, and they represent the simplest form of more complicated and realistic models. But when we give the impression that analytical solutions are the primary objective of the ODE world, we grossly distort the true picture. Already when I was a graduate student over 25 years ago, there were &lt;a href="https://dsweb.siam.org/Software/dstool" target="_blank" rel="noopener">great computational ODE tools&lt;/a> that would give you a fast and detailed understanding of dynamics. If you were inclined and able, you might go on to make some exact analytical (qualitative) conclusions suggested by the numerical and graphical explorations. But that&amp;rsquo;s a pursuit for a modestly sized cadre of mathematically advanced academics. It&amp;rsquo;s not the use case for over 99% of our undergraduates taking their one lifetime ODE course.&lt;/p>
&lt;p>Rather than trying to turn our students into slow and error-prone Mathematica simulators, we ought to be equipping our students with ODE fluency. Modeling, nondimensionalization, stability, and resonance are all more fundamental and vital than knowing &lt;a href="http://tutorial.math.lamar.edu/Classes/DE/UndeterminedCoefficients.aspx" target="_blank" rel="noopener">every last case of the method of undetermined coefficients&lt;/a>. Knowing the stability implications of poles in a &lt;a href="https://en.wikipedia.org/wiki/Transfer_function" target="_blank" rel="noopener">transfer function&lt;/a> is more important than performing partial fraction decompositions to invert Laplace transforms by hand.&lt;/p>
&lt;p>What&amp;rsquo;s a more relevant use of time: solving artificial scalar problems carefully selected to have compact analytical solutions, or learning how to simulate &lt;a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2050-0416.1994.tb00830.x" target="_blank" rel="noopener">ODE models of beer fermentation&lt;/a> or &lt;a href="https://www.ncbi.nlm.nih.gov/pubmed/10412391" target="_blank" rel="noopener">passive walking down a ramp&lt;/a>?&lt;/p>
&lt;p>Someday, perhaps, I will get to teach an ODE course that follows the &lt;a href="https://tobydriscoll.net/project/explode/">textbook I co-authored with Trefethen and Birkisson&lt;/a>. We used &lt;a href="http://www.chebfun.org" target="_blank" rel="noopener">Chebfun&lt;/a> to do the heavy lifting of solving problems and discussed at length how to read an ODE and investigate its behavior thoroughly. By not dwelling on now-archaic analytical solution methods, we were able to include introductions to chaos, stochastics, eigenvalues, bifurcations, and boundary layers, all essential phenomena that never make it into a traditional class.&lt;/p>
&lt;p>Until that happy day arrives, I guess you can find me trying to motivate 70 engineers to learn how to compute the exponential of 2-by-2 matrices.&lt;/p></description></item><item><title>Matlab vs. Julia vs. Python</title><link>https://tobydriscoll.net/blog/matlab-vs.-julia-vs.-python/</link><pubDate>Fri, 28 Jun 2019 14:57:36 -0400</pubDate><guid>https://tobydriscoll.net/blog/matlab-vs.-julia-vs.-python/</guid><description>&lt;p>I&amp;rsquo;ve used &lt;a href="https://www.mathworks.com/matlab" target="_blank" rel="noopener">MATLAB&lt;/a> for over 25 years. (And before that, I even used &lt;a href="https://www.sciencedirect.com/science/article/pii/S1474667017616793" target="_blank" rel="noopener">MATRIXx&lt;/a>, a late, unlamented attempt at a spinoff, or maybe a ripoff.) It&amp;rsquo;s not the &lt;a href="https://en.wikipedia.org/wiki/BASIC" target="_blank" rel="noopener">first language I learned to program in&lt;/a>, but it&amp;rsquo;s the one that I came of age with mathematically. Knowing MATLAB has been very good to my career.&lt;/p>
&lt;p>However, it&amp;rsquo;s impossible to ignore the rise of Python in scientific computing. MathWorks must feel the same way: not only did they add the ability to &lt;a href="https://www.mathworks.com/help/matlab/call-python-libraries.html" target="_blank" rel="noopener">call Python directly from within MATLAB&lt;/a>, but they&amp;rsquo;ve adopted borrowed some of its language features, such as more aggressive &lt;a href="https://www.mathworks.com/help/matlab/matlab_prog/compatible-array-sizes-for-basic-operations.html" target="_blank" rel="noopener">broadcasting&lt;/a> for operands of binary operators.&lt;/p>
&lt;p>It&amp;rsquo;s reached a point where I have been questioning my continued use of MATLAB in both research and teaching. Yet so much comes easily to me there, and I have so much invested in materials for it, that it was hard to rally motivation to really learn something new.&lt;/p>
&lt;p>Enter the MATLAB-based &lt;a href="https://tobydriscoll.net/FNC" target="_blank" rel="noopener">textbook&lt;/a> I&amp;rsquo;ve co-written for introductory computational math. The book has over 40 functions and 160 computational examples, and it covers what I think is a thorough grounding in the use of MATLAB for numerical scientific computing. So partly as self-improvement, and partly to increase the usefulness of the book, I set out this year to translate the codes into &lt;a href="https://github.com/tobydriscoll/fnc-extras/tree/master/julia" target="_blank" rel="noopener">Julia&lt;/a> and &lt;a href="https://github.com/tobydriscoll/fnc-extras/tree/master/python" target="_blank" rel="noopener">Python&lt;/a>. This experience has led me to a particular perspective on the three languages in relation to scientific computing, which I attempt to capture below.&lt;/p>
&lt;p>I will mostly set aside the issues of cost and openness. MATLAB, unlike Python and Julia, is neither beer-free nor speech-free. This is indeed a huge distinction—for some, a dispositive one–but I want to consider the technical merits. For many years, MATLAB was well beyond any free product in a number of highly useful ways, and if you wanted to be productive, then cost be damned. It&amp;rsquo;s a separate consideration from the Platonic appeal of a language and ecosystem.&lt;/p>
&lt;p>When you do set cost aside, a useful frame for a lot of the differences among these languages lies in their origins. MATLAB, the oldest of the efforts, prioritized math, particularly numerically oriented math. Python, which began in earnest in the late 1980s, made computer science its central focus. Julia, which began in 2009, set out to strike more of a balance between these sides.&lt;/p>
&lt;h2 id="matlab">MATLAB&lt;/h2>
&lt;p>Originally, every value in MATLAB was an array of double-precision floating point numbers. Both aspects of this choice, arrays and floating point, were inspired design decisions.&lt;/p>
&lt;p>The IEEE 754 standard for floating point wasn&amp;rsquo;t even adopted until 1985, and memory was measured in K, not G. Floating point doubles weren&amp;rsquo;t the most efficient way to represent characters or integers, but they were what scientists, engineers, and, increasingly, mathematicians wanted to use most of the time. Furthermore, variables did not have to declared and memory did not have to be explicitly allocated. Letting the computer handle those tasks, and whisking data types out of the way, freed up your brain to think about the algorithms that would operate on the data.&lt;/p>
&lt;p>Arrays were important because numerical algorithms in linear algebra were coming into their own, in the form of &lt;a href="https://en.wikipedia.org/wiki/LINPACK" target="_blank" rel="noopener">LINPACK&lt;/a> and &lt;a href="https://en.wikipedia.org/wiki/EISPACK" target="_blank" rel="noopener">EISPACK&lt;/a>. But accessing them with the standard bearer in scientific computing, FORTRAN 77, was a multistep process that involved declaring variables, calling cryptically named routines, compiling code, and then examining data and output files. Writing a matrix multiplication as &lt;code>A*B&lt;/code> and getting the answer printed out right away was a game-changer.&lt;/p>
&lt;p>MATLAB also made graphics easy and far more accessible. No fiddly machine-specific libraries with low-level calls, just &lt;code>plot(x,y)&lt;/code> and you saw pretty much what anyone else with MATLAB would see. There were more innovations, like baked-in complex numbers, sparse matrices, tools to build cross-platform graphical user interfaces, and a leading-edge suite of ODE solvers, that made MATLAB &lt;em>the&lt;/em> place to do scientific computing at the speed of thought.&lt;/p>
&lt;p>However, design that was ideal for interactive computations, even lengthy ones, was not always conducive to writing good and performant software. Moving data around between many functions required juggling lots of variables and frequent consultation of documentation about input and output arguments. One function per disk file in a flat namespace was refreshingly simple for a small project, but a headache for a large one. Certain programming patterns (vectorization, memory preallocation) had to be applied if you wanted to avoid speed bottlenecks. Scientific computing was now being applied to far more domains, with vast amounts of different native types of data. Etc.&lt;/p>
&lt;p>MathWorks responded by continuing to innovate within MATLAB: inline functions, nested functions, variable closures, numerous data types, object-oriented features, unit testing frameworks, and on and on. Each innovation was probably the solution to an important problem. But the accumulation of 40 years of these changes has had the side effect of eroding the simplicity and unity of concept. In 2009 I wrote a &lt;a href="https://tobydriscoll.net/project/learning-matlab/" target="_blank" rel="noopener">book&lt;/a> that pretty well covered what I considered the essentials of MATLAB in less than 100 pages. As far as I know, all of those things are still available. But you need to know a lot more now to call yourself proficient.&lt;/p>
&lt;h2 id="python">Python&lt;/h2>
&lt;p>In a sense the history of Python seems to be almost a mirror image of MATLAB&amp;rsquo;s. Both featured an interactive command line (now widely called a REPL, for &amp;ldquo;read-eval-print loop&amp;rdquo;) and freedom from variable declarations and compilation. But MATLAB was created as a playground for numerical analysts, while Python was created with hackers in mind. Each then grew toward the other audience through revisions and extensions.&lt;/p>
&lt;p>To my eye, Python still lacks mathematical appeal. You have ugliness and small annoyances such as &lt;code>**&lt;/code> instead of &lt;code>^&lt;/code>, &lt;code>@&lt;/code> for matrix multiplication (a recent innovation!), a &lt;code>shape&lt;/code> rather than size of a matrix, row-oriented storage, etc. If you believe that &lt;code>V.conj().T@D**3@V&lt;/code> is an elegant way to write $V^*D^3V$, then you may need to see a doctor. And there&amp;rsquo;s zero-indexing (as opposed to indexes that start at 1). I&amp;rsquo;ve &lt;a href="https://www.cs.utexas.edu/users/EWD/transcriptions/EWD08xx/EWD831.html" target="_blank" rel="noopener">read the arguments&lt;/a>, and I don&amp;rsquo;t find them decisive. It&amp;rsquo;s clearly a matter of preference—the stuff of online holy wars—because you can cite ungainly examples for either convention. What I find decisive is that we have decades of mathematical practice indexing vectors and matrices from one, and most pseudocode makes that assumption.&lt;/p>
&lt;p>Beyond the petty annoyances, I find the Python+NumPy+SciPy ecosystem to be kludgy and inconsistent. Exhibit A is the fact that despite the language being rather devoted to object orientation, there exists a matrix class, and yet its use is &lt;a href="https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html" target="_blank" rel="noopener">discouraged and will be deprecated&lt;/a>. Perhaps MATLAB has simply corrupted me, but I find matrices to be an important enough type of object to keep around and promote. Isn&amp;rsquo;t a major selling point of OOP that you can have &lt;code>*&lt;/code> do different things for arrays and matrices? There are many other infelicities in this regard. (Why do I need a command called &lt;a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.spsolve.html#scipy.sparse.linalg.spsolve" target="_blank" rel="noopener">spsolve&lt;/a>? Can&amp;rsquo;t I just call &lt;code>solve&lt;/code> on a sparse matrix? And on and on.)&lt;/p>
&lt;p>There are also places where the numerical ecosystem looks a little thin to me. For instance, the &lt;a href="https://docs.scipy.org/doc/scipy/reference/integrate.html" target="_blank" rel="noopener">quadrature and ODE solvers&lt;/a> look like a minimal set in 2019. AFAICT there are no methods for DAEs, DDEs, symplectic solvers, or implicit solvers that allow inner Krylov iterations. Have a look at the references for these functions; they&amp;rsquo;re mostly 30 or more years old—still good, but very far from complete. The Matplotlib package is an amazing piece of work, and for a while it looked better than MATLAB, but I find it quite lacking in 3D still.&lt;/p>
&lt;p>Some experts argue that there are deep reasons why Python code struggles to keep up in execution speed with compiled languages. I&amp;rsquo;m amused by the results of searching for &lt;a href="https://www.google.com/search?q=python&amp;#43;is&amp;#43;too&amp;#43;slow&amp;amp;oq=python&amp;#43;is&amp;#43;too&amp;#43;slow" target="_blank" rel="noopener">&amp;ldquo;python is too slow&amp;rdquo;&lt;/a>. The champions of Python make a lot of the same arguments/apologies that folks did for MATLAB back in the day. That doesn&amp;rsquo;t mean they&amp;rsquo;re wrong, but &lt;a href="https://modelingguru.nasa.gov/docs/DOC-2676" target="_blank" rel="noopener">there&amp;rsquo;s more than just a perception problem&lt;/a>.&lt;/p>
&lt;p>I think I get why Python has been so exciting to many people in scientific computing. It has a some MATLAB-ish syntax and power, available from a REPL. It has great tools around it and plays well with other languages and areas of computing. It offered that at no cost and with much better long-term reproducibility. Clearly, it works well for a lot of people who probably see little reason to change.&lt;/p>
&lt;p>But for the things I know how to do in scientific computing, Python feels much more like a chore to learn and use than I&amp;rsquo;m used to. We won&amp;rsquo;t know for a while whether it will continue to sweep through the community or has already neared its peak. I have no special predictive powers, but I&amp;rsquo;m bearish.&lt;/p>
&lt;h2 id="julia">Julia&lt;/h2>
&lt;p>Julia has the advantages and disadvantages of being a latecomer. I applaud the Julia creators for &lt;a href="https://julialang.org/blog/2012/02/why-we-created-julia" target="_blank" rel="noopener">thinking they could do better&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>We want a language that’s open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that’s homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled.&lt;/p>
&lt;/blockquote>
&lt;p>To a great extent, I believe they have succeeded. Late along the road to version 1.0 they seemed to downplay the REPL a bit, and there were some almost gratuitous lurches away from MATLAB. (How exactly is &lt;code>LinRange&lt;/code> better than &lt;code>linspace&lt;/code>?) These are quibbles, though.&lt;/p>
&lt;p>This is the first language I&amp;rsquo;ve used that goes beyond ASCII. I still get an unreasonable amount of satisfaction from using variables like &lt;code>ϕ&lt;/code> and operators like &lt;code>≈&lt;/code>. It&amp;rsquo;s more than cosmetic; being able to look more like the mathematical expressions we write is real plus, though it does complicate teaching and documentation a bit.&lt;/p>
&lt;p>Working in Julia exposed to me that I picked up some programming habits because of MATLAB&amp;rsquo;s choices, not inherent superiority. Vectorization is not natural for many things. It&amp;rsquo;s eye-opening to find in Julia that you can vectorize any function just by adding a dot to its name. Constructing a matrix through a &lt;a href="https://docs.julialang.org/en/v1/manual/arrays/#Comprehensions-1" target="_blank" rel="noopener">comprehension&lt;/a> makes nested loops (or &lt;code>meshgrid&lt;/code> tricks) look like buggy whips in comparison, and avoiding a matrix altogether via a &lt;a href="https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions-1" target="_blank" rel="noopener">generator&lt;/a> for a simple summation feels like getting something for nothing. (I&amp;rsquo;m aware that Python has similar language features.)&lt;/p>
&lt;p>The big feature of &lt;em>multiple dispatch&lt;/em> makes some things a lot easier and clearer than object orientation does. For instance, suppose you have Wall and Ball classes in a traditional object-oriented language. Which class should detect a collision of a Ball with a Wall? Or do you need a Room class to play referee? These kinds of questions can drive me to distraction. With multiple dispatch, data is packaged into object types, but the methods that operate on data are not bound to a class. So&lt;/p>
&lt;pre>&lt;code class="language-julia">function detect_collision(B::Ball,W::Wall)
&lt;/code>&lt;/pre>
&lt;p>knows about the types but is defined independently of them. It&amp;rsquo;s taken quite a bit of programming for me to appreciate how interesting and potentially important the notion of multiple dispatch is for extending the language.&lt;/p>
&lt;p>The numerical ecosystem has been evolving rapidly. My number one example is &lt;a href="http://docs.juliadiffeq.org/latest/index.html" target="_blank" rel="noopener">DifferentialEquations.jl&lt;/a>, written by the amazing &lt;a href="http://chrisrackauckas.com/" target="_blank" rel="noopener">Chris Rackauckas&lt;/a>. If this software doesn&amp;rsquo;t win the Wilkinson prize soon, the system is broken. Just go to the site and prepare to be converted.&lt;/p>
&lt;p>I have yet to see the big speed gains over MATLAB that Julia promises. Partly that&amp;rsquo;s my relative inexperience and the kinds of tasks I do, but it&amp;rsquo;s also partly because MathWorks has done an incredible job automatically optimizing code. It&amp;rsquo;s not an aspect of coding that I focus on most of the time, anyway.&lt;/p>
&lt;p>Programming in Julia has taken me a while to feel comfortable with (perhaps I&amp;rsquo;m just getting old and crystallized). It makes me think about data types more than I would want, and there&amp;rsquo;s always the sneaking suspicion that I&amp;rsquo;ve missed the Right Way to do something. But for daily use, I&amp;rsquo;m about as likely to turn to Julia as MATLAB now.&lt;/p>
&lt;h2 id="the-bottom-line">The bottom line&lt;/h2>
&lt;p>MATLAB is the corporate solution, especially for engineering. It&amp;rsquo;s probably still the easiest to learn for basic numerical tasks. Meticulous documentation and decades of contributed learning tools definitely matter.&lt;/p>
&lt;p>MATLAB is the BMW sedan of the scientific computing world. It&amp;rsquo;s expensive, and that&amp;rsquo;s before you start talking about accessories (toolboxes). You&amp;rsquo;re paying for a rock-solid, smooth performance and service. It also attracts a &lt;a href="https://www.google.com/search?q=i&amp;#43;hate&amp;#43;matlab" target="_blank" rel="noopener">disproportionate amount of hate&lt;/a>.&lt;/p>
&lt;p>Python is a Ford pickup. It&amp;rsquo;s ubiquitous and beloved by many (in the USA). It can do everything you want, and it&amp;rsquo;s built to do some things that other vehicles can&amp;rsquo;t. Chances are you&amp;rsquo;re going to want to borrow one now and then. But it doesn&amp;rsquo;t offer a great pure driving experience.&lt;/p>
&lt;p>Julia is a Tesla. It&amp;rsquo;s built with an audacious goal of changing the future, and it might. It may also become just a footnote. But in the meantime you&amp;rsquo;ll get where you are going in style, and with power to spare.&lt;/p></description></item><item><title>New look, new tech</title><link>https://tobydriscoll.net/blog/new-look-new-tech/</link><pubDate>Thu, 27 Jun 2019 15:20:05 -0400</pubDate><guid>https://tobydriscoll.net/blog/new-look-new-tech/</guid><description>&lt;p>At long last, I&amp;rsquo;ve refreshed the look for this site. Previously it was based on a &amp;ldquo;Metro UI&amp;rdquo; style for HTML, which looked nice to me at the time. Actually it still looks pretty nice, but it was named for the Metro design introduced with Windows 8, which tells you that it wasn&amp;rsquo;t exactly a modern look.&lt;/p>
&lt;p>More importantly to me, I&amp;rsquo;ve ditched writing raw HTML for creating a site using the &lt;a href="https://gohugo.io" target="_blank" rel="noopener">Hugo&lt;/a> content creation system. It&amp;rsquo;s such a headache trying to maintain menus, headers, footers, etc. in raw HTML that updating the old site became too painful to contemplate. On the blogging side, I had been using &lt;a href="https://wordpress.com" target="_blank" rel="noopener">Wordpress&lt;/a>, which I found incredibly slow, wonky, and frustrating. Including snippets of code or math was way too hard, even with extension modules loaded that were supposed to handle it for me, and the results were inconsistent.&lt;/p>
&lt;p>In Hugo you create content within a folder hierarchy that holds plain text files. These can be in HTML as necessary, but the real workhorse is Markdown, which lets you enter content with simply annotated text. As with all markup languages, the idea is to label the structure of the content and let the formatting be specified separately. Unlike HTML, though, the syntax is concise and easy to type, though of course limited to just the most important element types.&lt;/p>
&lt;p>Hugo then converts the Markdown sources into a working site. It&amp;rsquo;s static, not dynamic, meaning that all the pages are created at once rather than in response to a browser request. This keeps it fast and lean, which is appropriate for a modest personal site.&lt;/p>
&lt;p>One strength of Hugo is that it supports tons of &lt;a href="http://themes.gohugo.io/" target="_blank" rel="noopener">themes&lt;/a>. Of these, the &lt;a href="https://themes.gohugo.io/academic/" target="_blank" rel="noopener">Academic&lt;/a> theme quickly jumped out at me. It&amp;rsquo;s written to provide what a typical personal page in academia requires. To be honest, I feel as though it took me too long to grok how to use the theme, and Hugo in general. You can see the &lt;a href="https://github.com/tobydriscoll/academic-kickstart" target="_blank" rel="noopener">sources for this site&lt;/a> if you want to get a feeling by example, which is what I recommend.&lt;/p>
&lt;p>The last piece was to generate a screen-native CV. I&amp;rsquo;ve been getting increasingly frustrated with PDF online. It was a huge step forward when it was introduced in 1993, when hardcopy was still the norm. But while we haven&amp;rsquo;t gone fully paperless, lots of systems such as job applications or promotion documentation simply can&amp;rsquo;t function any way but fully online now. Yet as pointed out in this great &lt;a href="https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/" target="_blank" rel="noopener">&lt;em>Atlantic&lt;/em> piece on the obsolescence of the scientific paper&lt;/a>, PDF is intrinsically a simulation of a piece of paper. Because it prioritizes display over structure, it&amp;rsquo;s often hard to extract information from, and it can&amp;rsquo;t do something as fundamental as wrap lines of text to fit a screen width.&lt;/p>
&lt;p>For decades I maintained my CV in LaTeX, with PDF as the output format. This was okay, but getting output from LaTeX in a truly screen-friendly form (basically, HTML) Is. Not. Fun. While the first 80% is easy, the last 20% can be excruciating. Fortunately, with a little regexp replacement, I was able to convert the source to Markdown with manageable effort. Then, thanks to &lt;a href="http://elipapa.github.io/markdown-cv" target="_blank" rel="noopener">markdown-cv&lt;/a>, I&amp;rsquo;m able to render this as a &lt;a href="https://tobydriscoll.net/markdown-cv/" target="_blank" rel="noopener">great-looking document&lt;/a>. Yes, I make a &lt;a href="https://tobydriscoll.net/cv.pdf" target="_blank" rel="noopener">PDF version&lt;/a> available still—by printing the &amp;ldquo;real&amp;rdquo; version from a browser.&lt;/p>
&lt;p>My dream had been to have the publications part of the CV auto-generated from the data in my &lt;a href="https://tobydriscoll.net/publication/" target="_blank" rel="noopener">publications&lt;/a> website section. If I were a more dedicated Hugo hacker, I&amp;rsquo;m sure I could do it, but I threw in the towel. Besides, there&amp;rsquo;s something to be said for making sure the CV really looks the way you expect it, and the work of double-entering the publications isn&amp;rsquo;t &lt;em>that&lt;/em> odious.&lt;/p>
&lt;p>This being summer, I haven&amp;rsquo;t yet migrated whatever teaching content from the old site I feel is still worthwhile. But it feels nice to have my online presence up to snuff once more. I hope it motivates me to renew a dedication to blogging.&lt;/p></description></item><item><title>Jekyll for clicker questions</title><link>https://tobydriscoll.net/blog/jekyll-for-clicker-questions/</link><pubDate>Fri, 02 Feb 2018 20:54:56 +0000</pubDate><guid>https://tobydriscoll.net/blog/jekyll-for-clicker-questions/</guid><description>&lt;p>For a few years, I&amp;rsquo;ve been a fan of clickers (aka personal response systems) for large lecture sections. Clickers are a simple&amp;ndash;and scalable&amp;ndash;way to incorporate a little widespread active learning in the classroom. They can&amp;rsquo;t work miracles, but they do allow me to reward attendance, rouse the students once in a while, and give good feedback to all of us about how well the latest concepts are sinking in. I like the accountability: If you got the question wrong when 80% of the class got it right, that&amp;rsquo;s on you, but if 20% of the class got it right, that&amp;rsquo;s on me.&lt;/p>
&lt;p>UD is an &lt;a href="http://www.iclicker.com" target="_blank" rel="noopener">iclicker&lt;/a> shop. When I want to poll the class, I click a &amp;ldquo;go&amp;rdquo; button on a small toolbar that overlays any other application. When I&amp;rsquo;m done, I click &amp;ldquo;stop.&amp;rdquo; I can show the results and designate the correct answer on the spot, or I can go back later and pick the right answer while looking at a screenshot from when the question started.&lt;/p>
&lt;p>In the past I&amp;rsquo;ve used clickers with handwritten questions projected using a document camera. I don&amp;rsquo;t get the screenshot this way, but it works fine. However, in the best case I&amp;rsquo;m left to manage 50-100 sheets of paper for a course. That&amp;rsquo;s something I&amp;rsquo;m increasingly cranky about doing in my life overall, and I&amp;rsquo;m likely to fail at it during the heat of a lecture, especially when (as I like to do) I start replaying questions from past weeks or months. Plus, if I later decide to tweak a question or the answer choices, I&amp;rsquo;ve got to scrap a page and rewrite it.&lt;/p>
&lt;p>Enter &lt;a href="http://jekyllrb.com" target="_blank" rel="noopener">Jekyll&lt;/a>. This is a brilliant software t0ol that converts lightly marked data files into a website. It&amp;rsquo;s blog-centric, but it can be used for other kinds of data as well, and I&amp;rsquo;ve customized it for collecting clicker questions. You can get it for yourself from &lt;a href="https://github.com/tobydriscoll/clicker-quiz" target="_blank" rel="noopener">this Github repo&lt;/a>. It requires being comfortable with a command line, but it&amp;rsquo;s not otherwise technically challenging.&lt;/p>
&lt;p>For instance, in one file I have&lt;/p>
&lt;pre>---
layout: question
chapter: Introduction
title: Derivative
---
{::comment}
The \dd macro is defined in /_includes/texmacros.md.
{:/comment}
What is $\dd{}{x}\left(e^x\right)$?
1. *$e^x$*{: #correct}
1. $x$
1. $1$
1. $\ln(x)$
1. $\tan(x)$&lt;/pre>
&lt;p>The page that results can be viewed &lt;a href="https://tobydriscoll.github.io/clicker-quiz/questions/introduction/q02.html" target="_blank" rel="noopener">here&lt;/a>.  It&amp;rsquo;s pretty easy to see how the output arises from the input. All I do is make one file per question, putting them into subdirectories if I want. They&amp;rsquo;re collected and indexed by the &amp;ldquo;chapter&amp;rdquo; property at the top of the file.&lt;/p>
&lt;p>Having maintained more than one &lt;a href="http://tobydriscoll.net" target="_blank" rel="noopener">website&lt;/a> of HTML files by hand, I found Jekyll to be a revelation. Headers and footers can be included automatically on all pages set to a certain style. (I use this to define MathJax macros in one file that get copied into all the output question pages.)  Content, such as an index or table of contents, can be generated programmatically based on properties of the data. There&amp;rsquo;s a nice &lt;a href="https://www.chronicle.com/blogs/profhacker/jekyll1" target="_blank" rel="noopener">step-by-step series for getting started with Jekyll&lt;/a> on the ProfHacker blog.&lt;/p>
&lt;p>Jekyll versus raw HTML is like using a power drill/driver versus the Craftsman screwdriver with the hard plastic handle that digs divots into your palm when there&amp;rsquo;s a job of any decent size. I&amp;rsquo;ll probably move my personal site this blog over to Jekyll at some point.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia: Iterative methods</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-iterative-methods/</link><pubDate>Fri, 03 Feb 2017 20:16:10 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-iterative-methods/</guid><description>&lt;p>I&amp;rsquo;m going to wrap up the long-paused MATLAB versus Julia comparison on Trefethen &amp;amp; Bau by chugging through all the lectures on iterative methods in one post.&lt;/p>
&lt;p>I&amp;rsquo;m back to using gists&amp;ndash;not thrilled with any of the mechanisms for sharing this stuff.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gist.github.com/tobydriscoll/d1fe4e61d05e2b423a55979982a2d38a" target="_blank" rel="noopener">Lecture 32 (sparse matrices and simple iterations)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gist.github.com/tobydriscoll/204bbc93b984c9ddf17bbe51e162399d" target="_blank" rel="noopener">Lecture 33 (Arnoldi iteration)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gist.github.com/tobydriscoll/63212a0c32c473daae5a81a3f6888476" target="_blank" rel="noopener">Lecture 34 (Arnoldi eigenvalues)&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>These are remarkable mainly in that they have such striking similarity in both languages. Aside from square brackets and working around the &lt;a href="https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-2/">1x1/scalar distinction&lt;/a> in Julia, little differs besides the syntax of the &lt;code>eigs&lt;/code> command.&lt;/p>
&lt;p>One frustration, though. I decided to try an interesting alternative to PyPlot in Julia, the &lt;a href="https://juliaplots.github.io/" target="_blank" rel="noopener">Plots package&lt;/a>. Actually Plots tries to be a generalization of and alternative route to using PyPlot/matplotlib. I decided to try the PlotlyJS backend instead, however. It makes lovely graphics with very responsive interaction. Since the rendering is in Javascript, I thought it would be perfectly portable, but you can&amp;rsquo;t see the output in the gist above, even though it should be embedded in the notebook.&lt;/p>
&lt;p>I liked using Plots OK; for the most part it&amp;rsquo;s just different, not better or worse that I could see. I found it awkward to work with subplots. I ended up creating 4 plots individually and then displaying them in a table using another call to &lt;code>plot&lt;/code>. I find MATLAB&amp;rsquo;s setup more convenient. I also could not figure out how to coax a contour plot with a contour at a specified value, which seems like a big lack.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gist.github.com/tobydriscoll/f5815ce26dec0f010b4fc481573f3e4b" target="_blank" rel="noopener">Lecture 35 (GMRES)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gist.github.com/tobydriscoll/a7ecf0c147fa02a4c6156074da0ccd38" target="_blank" rel="noopener">Lecture 36 (Lanczos and MINRES)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gist.github.com/tobydriscoll/2511c290c58f98c6989672082897d47e" target="_blank" rel="noopener">Lecture 37 (Conjugate gradients)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gist.github.com/tobydriscoll/d11baaeb88f8145f175d3ea7eac87a95" target="_blank" rel="noopener">Lecture 40 (Preconditioning)&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Again the differences are minor. In sparse and iterative methods I found Julia to place a greater emphasis on keyword arguments. For example,&lt;/p>
&lt;pre>&lt;code class="language-julia">(xCG,~,~,~,resnorm) = cg(A,b,tol=1e-14,maxIter=100);
&lt;/code>&lt;/pre>
&lt;p>There are default values for &lt;code>tol&lt;/code> and &lt;code>maxIter&lt;/code>, but if you want to override them you must type the keyword. On the other hand, MATLAB&amp;rsquo;s arguments are purely positional:&lt;/p>
&lt;pre>&lt;code class="language-matlab">[xCG,~,~,~,resnorm] = pcg(A,b,1e-14,100);
&lt;/code>&lt;/pre>
&lt;p>If I wanted to specify the maximum number of iterations without changing the default tolerance, then I would need to use an empty matrix in the third position. When one uses a command that does take named parameters as inputs, it&amp;rsquo;s typically done using &lt;code>'propname',propval&lt;/code> pairs. Except when it isn&amp;rsquo;t, such as for ODEs and optimization. Confusing! As a user I don&amp;rsquo;t love typing out the keywords, but Julia at least lets me skip the quote marks. I also know from experience that Julia&amp;rsquo;s version is a lot easier and clearer to implement on the other side.&lt;/p>
&lt;p>So that&amp;rsquo;s that. I feel that I am at least ready to get off the bunny slopes with Julia. I haven&amp;rsquo;t found a compelling reason to switch to it, aside from supporting open source software for science (no small thing). Of course I&amp;rsquo;ve barely scratched the surface. On the flip side, MATLAB has a lot of well-designed and -maintained packages, and its environment still makes a smoother experience for newcomers. If you can afford it, it&amp;rsquo;s still a great option for interactive numerical computing.&lt;/p>
&lt;p>I wonder about the future of Julia. Had Python not gotten a head start, I could see an outpouring of effort to make high-quality Julia packages and Julia being a complete MATLAB reboot. But numpy and scipy do exist, and despite their flaws, they have a huge first-mover advantage. It&amp;rsquo;s a snap to use Python packages in Julia, so there&amp;rsquo;s not a dichotomy here. But if the package you want to use a lot exists only in Python, the case for Julia weakens. Overall though, it&amp;rsquo;s a nice thing that we have several strong, expressive high-level environments for numerical computing. Happy coding!&lt;/p></description></item><item><title>It already has happened here</title><link>https://tobydriscoll.net/blog/it-already-has-happened-here/</link><pubDate>Sun, 18 Dec 2016 15:28:34 +0000</pubDate><guid>https://tobydriscoll.net/blog/it-already-has-happened-here/</guid><description>&lt;p>I&amp;rsquo;ve just finished one of the most remarkable fiction reading experiences I&amp;rsquo;ve had in quite some time: &lt;a href="https://en.wikipedia.org/wiki/It_Can%27t_Happen_Here" target="_blank" rel="noopener">&lt;em>It Can&amp;rsquo;t Happen Here&lt;/em>&lt;/a>, by &lt;a href="https://en.wikipedia.org/wiki/Sinclair_Lewis" target="_blank" rel="noopener">Sinclair Lewis&lt;/a>.&lt;/p>
&lt;p>&lt;em>ICHH&lt;/em> is a satirical novel written and set in 1935 America. It describes the rise of a populist dictatorship modeled closely along the rise of the Nazis in Germany.(Lewis&amp;rsquo; wife, &lt;a href="https://en.wikipedia.org/wiki/Dorothy_Thompson" target="_blank" rel="noopener">Dorothy Thompson&lt;/a>, was the first American journalist expelled from Nazi Germany and was clearly responsible for much of the shape of the book.)&lt;/p>
&lt;p>This was near the height of the Depression, and FDR&amp;rsquo;s New Deal was controversial and, at that point unsuccessful (or not yet successful, if you want to look at it that way). There were multiple signs of unrest and populism, not least of which was &lt;a href="https://en.wikipedia.org/wiki/Huey_Long" target="_blank" rel="noopener">Huey Long&lt;/a>, then governor of Louisiana and apparently plotting to hijack the Democratic party to get elected President in 1936 or 1940. (Long was assassinated in 1935, as the book was being finished.)&lt;/p>
&lt;p>&lt;em>ICHH&lt;/em> is not great as a novel. The characters are essentially allegorical, and the plot barely needs them. The core of the book is as a dystopian speculation about the near future of the USA. Lewis himself called it &amp;ldquo;propaganda for American democracy.&amp;rdquo; As such, its relevance to 2016 is astonishing. I could pick out dozens of quotes. Here are just three extended ones.&lt;/p>
&lt;blockquote>
&lt;p>&amp;hellip;what burns me up [isn&amp;rsquo;t] that old soap-boxer&amp;rsquo;s old chestnut about how one tenth of one percent of the population at the top have an aggregate income equal to 42 percent at the bottom&amp;hellip;.[It&amp;rsquo;s] the fact that even before this Depression, in what you folks called prosperous times, 7 per cent of all the families in the country earned $500 a year or less&amp;mdash;remember, those weren&amp;rsquo;t the unemployed, on relief; those were the guys that still had the honor of doing honest labor.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>The most confusing thing about the campaign of 1936 was the relationship of the two leading parties. Old-Guard Republicans were complaining that their proud party was begging for office, hat in hand; veteran Democrats that their traditional Covered Wagons were jammed with college professors, city slickers, and yachtsmen.&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>Most Americans had learned in school that God had supplanted the Jews as chosen people by the Americans, and this time done the job much better, so that we were the richest, kindest, and cleverest nation living; that depressions were but passing headaches and that labor unions&amp;hellip;must not set up an ugly class struggle by combining politically; that, though foreigners tried to make a bogus mystery of them, politics were really so simple that any village attorney or any clerk in the office of a metropolitan sheriff was quite adequately trained for them; and that if John D. Rockefeller or Henry Ford had set his mind to it, he could have become the most distinguished statesman, composer, physicist, or poet in the land.&lt;/p>
&lt;/blockquote>
&lt;p>So much is in the book: the rural/urban dynamic, the American disdain for and distrust of intellectualism, the cluelessness of intellectuals, racism, anti-Semitism, antifeminism, the lust for a &amp;ldquo;ringmaster-revolutionist&amp;rdquo;, the contrast between a boring, calculating candidate and a hot populist who draws big rallies, the failure of newspapers (i.e. the media in 1935), the use of radio for disintermediation (i.e., Twitter), the belief by the banking establishment that things would soon moderate and work in their favor, etc. Lewis also spells out the horrors of a concentration camp&amp;mdash;familiar ground to us today, but sensational to much of the public in 1935.&lt;/p>
&lt;p>In 1935 the bogeyman was communism, not Islam, and the threat seems to be the extreme left, not the right. But that hardly matters. What &lt;em>ICHH&lt;/em> made so clear to me is how America, through its history, culture, and politics, is a host susceptible to a certain pattern of symptoms. And maybe it&amp;rsquo;s not just America, and not just liberal democracy, but literally part of our DNA.&lt;/p>
&lt;p>I&amp;rsquo;m quickly going over in my head in political science here. My reaction to the book is complicated. I recommend that you pick up a copy and at least get through the fictional election, though it&amp;rsquo;s neither easy nor fun to read.&lt;/p>
&lt;p>The book was apparently adapted to a smash hit play in 1937. I would be surprised if it doesn&amp;rsquo;t undergo a revival in the near future.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lectures 24-29: Eigenvalue stuff</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-24-29-eigenvalue-stuff/</link><pubDate>Thu, 27 Oct 2016 14:16:39 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-24-29-eigenvalue-stuff/</guid><description>&lt;p>Part V of T&amp;amp;B is on dense methods for eigenvalue and singular value problems. For my course, this is the part of the text that I condense most severely. In part that&amp;rsquo;s due to the need to cover unconstrained nonlinear solving and optimization stuff later on. But I also find that this is the least compelling part of the text for my purposes.&lt;/p>
&lt;p>It&amp;rsquo;s heavily weighted toward the hermitian case. That&amp;rsquo;s the cleanest situation, so I see the rationale. But it&amp;rsquo;s pretty surprising that the lead author of &lt;a href="http://press.princeton.edu/titles/8113.html" target="_blank" rel="noopener">&lt;em>Spectra and Pseudospectra&lt;/em>&lt;/a> mentions eigenvalue conditioning and sensitivity only in a single exercise! (The exercises not in the lecture named &amp;ldquo;Eigenvalue problems,&amp;rdquo; nor the one named &amp;ldquo;Overview of eigenvalue algorithms.&amp;rdquo; It&amp;rsquo;s under &amp;ldquo;Reduction to Hessenberg or tridiagonal form.&amp;rdquo;) In contrast with the tone of earlier parts of the book, one could study the methods of these sections thoroughly and yet not appreciate when the answers are inaccurate, or possibly irrelevant. Because I took this course from Trefethen at a crucial time in the development of his thinking on the subject, my perception of the issues behind computing eigenvalues is quite different from what the text itself conveys.&lt;/p>
&lt;p>(EDIT: If I had but read a few sections more before writing the above, I would have recalled that there is discussion about this in Lecture 34, under &amp;ldquo;A Note of Caution: Nonnormality.&amp;rdquo; It&amp;rsquo;s all laid out in clear language, so mea culpa. The ordering still feels a little awkward. I&amp;rsquo;ll probably have a half or full class period just on nonnormality.)&lt;/p>
&lt;p>So. In my class I touched on 24-29, and you can find my related &lt;a href="https://www.dropbox.com/sh/kxyc1on3k4f3sh0/AACnyHY2FmXgUpHmJvSYV6Qaa?dl=0" target="_blank" rel="noopener">MATLAB notebooks&lt;/a> and &lt;a href="https://www.dropbox.com/sh/gq3a0nr1gm4p87a/AABlOcb33OAjO40PFG6tkYSva?dl=0" target="_blank" rel="noopener">Julia notebooks&lt;/a> on them. (I&amp;rsquo;ve given up on using Gists for these. The web interface can&amp;rsquo;t seem to handle having a lot of notebooks in one Gist, the rendering is slow, and I see no advantage for me beyond static HTML.) They&amp;rsquo;re a little rough in places, as it&amp;rsquo;s been challenging to keep up the pace.&lt;/p>
&lt;p>There aren&amp;rsquo;t big MATLAB/Julia issues to report. If anything, I think Julia has cleaned up and rationalized some of the quirkiness of the MATLAB versions. In MATLAB, one uses &lt;code>eig&lt;/code> for everything. The results depend on the number of output arguments.&lt;/p>
&lt;pre>&lt;code class="language-matlab">&amp;gt;&amp;gt; A = hilb(3);
&amp;gt;&amp;gt; lambda = eig(A)
lambda =
0.0027
0.1223
1.4083
&amp;gt;&amp;gt; [X,D] = eig(A)
X =
-0.1277 0.5474 0.8270
0.7137 -0.5283 0.4599
-0.6887 -0.6490 0.3233
D =
0.0027 0 0
0 0.1223 0
0 0 1.4083
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s a bit awkward that the position of the eigenvalue output changes, and that it&amp;rsquo;s a vector in one case and a matrix in the other. And the difference goes beyond cosmetics: the calculation can be significantly faster if eigenvectors are not required. Julia gives you three variants, so you can retrieve exactly what you want.&lt;/p>
&lt;pre>&lt;code class="language-julia">julia&amp;gt; A = [1/(i+j) for i=1:3, j=1:3];
julia&amp;gt; (λ,X) = eig(A)
([0.000646659,0.0409049,0.875115],
[0.19925 -0.638787 -0.743136; ... -0.411255])
julia&amp;gt; λ = eigvals(A)
3-element Array{Float64,1}:
0.000646659
0.0409049
0.875115
julia&amp;gt; D = eigvecs(A)
3×3 Array{Float64,2}:
0.19925 -0.638787 -0.743136
-0.761278 0.376612 -0.527843
0.617053 0.670906 -0.411255
&lt;/code>&lt;/pre>
&lt;p>You even have &lt;code>eigmax&lt;/code> and &lt;code>eigmin&lt;/code> when the spectrum is real. One thing neither language gives you is an easy way to specify a sort order for the results. In MATLAB, for instance, one ends up doing things like:&lt;/p>
&lt;pre>&lt;code class="language-matlab">&amp;gt;&amp;gt; [X,D] = eig(A);
&amp;gt;&amp;gt; lambda = diag(D);
&amp;gt;&amp;gt; [~,idx] = sort(real(lambda));
&amp;gt;&amp;gt; X = X(:,idx); lambda = lambda(idx)
lambda =
-2.1898 + 1.4354i
-2.1898 - 1.4354i
0.0301 + 0.6095i
0.0301 - 0.6095i
1.2276 + 2.2020i
1.2276 - 2.2020i
1.8278 + 0.0000i
&lt;/code>&lt;/pre>
&lt;p>Meh. It&amp;rsquo;s not a lot better in Julia, as far as I can tell.&lt;/p>
&lt;pre>&lt;code class="language-julia">julia&amp;gt; A = randn(7,7);
julia&amp;gt; (λ,X) = eig(A);
julia&amp;gt; idx = sortperm(real(λ));
julia&amp;gt; X = X[:,idx]; λ = λ[idx]
7-element Array{Complex{Float64},1}:
-3.38359+0.0im
-2.33084+0.233909im
-2.33084-0.233909im
0.415007+0.0im
1.03098+0.0im
1.11426+2.34596im
1.11426-2.34596im
&lt;/code>&lt;/pre>
&lt;p>Altogether, Julia is feeling less like a foreign country and more like a province. Sometimes I even remember to use square brackets on the first try.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia: Lectures 20, 21, 23: Solving square systems</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-20-21-23-solving-square-systems/</link><pubDate>Sat, 22 Oct 2016 16:20:15 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-20-21-23-solving-square-systems/</guid><description>&lt;p>Three in one this time: &lt;a href="https://gist.github.com/tobydriscoll/e2f44e7756b864050b52c6773aa83b70" target="_blank" rel="noopener">Lecture 20&lt;/a>, which is on Gaussian elimination / LU factorization, &lt;a href="https://gist.github.com/tobydriscoll/2b74da569406da55c599a61afe99cf56" target="_blank" rel="noopener">Lecture 21&lt;/a>, on row pivoting, and &lt;a href="https://gist.github.com/tobydriscoll/17ce89eb2c827e0b870877219ec64fe8" target="_blank" rel="noopener">Lecture 23&lt;/a>, on Cholesky factorization. I mainly skipped over Lecture 22, about the curious case of the stability of pivoted LU, but the main example is dropped into the end of my coverage of pivoting.&lt;/p>
&lt;p>The Julia surprises are, not surprisingly, coming less frequently. In Lecture 20 I had some fun with rational representations. I like using MATLAB&amp;rsquo;s &lt;code>format rat&lt;/code> when presenting Gaussian elimination, as it allows me to recall the way the process looks when learned by hand. It&amp;rsquo;s a fun trick, but of course the underlying values are still all double precision, and the rational approximations to them are found ex post facto. By contrast, Julia offers true rational numbers, constructed and shown using the &lt;code>//&lt;/code> operation.&lt;/p>
&lt;p>Compare the MATLAB&lt;/p>
&lt;pre>&lt;code class="language-matlab">format rat
I = eye(4);
L21 = I + (-5/17)*I(:,2)*I(:,1)';
L31 = I + (-9/17)*I(:,3)*I(:,1)';
L41 = I + (-4/17)*I(:,4)*I(:,1)';
&lt;/code>&lt;/pre>
&lt;p>to the Julia&lt;/p>
&lt;pre>&lt;code class="language-julia">I = eye(Rational,4);
L21 = copy(I); L21[2,1] = -5//17;
L31 = copy(I); L31[3,1] = -9//17;
L41 = copy(I); L41[4,1] = -4//17;
&lt;/code>&lt;/pre>
&lt;p>The MATLAB code requires only the &lt;code>format&lt;/code> call, because it&amp;rsquo;s only the display of results that is affected. The Julia code is doing something deeper and needs more changes as a result.&lt;/p>
&lt;p>Julia could use something like a &lt;code>format&lt;/code> command. I almost always find MATLAB&amp;rsquo;s terminal output more readable, or at least easier to manipulate into a good form. Here&amp;rsquo;s one example using the rational output. First, MATLAB:&lt;/p>
&lt;pre>&lt;code class="language-matlab">17 2 3 13 93
0 194/17 155/17 71/17 963/17
0 101/17 92/17 87/17 574/17
0 230/17 243/17 -18/17 1158/17
&lt;/code>&lt;/pre>
&lt;p>And the Julia:&lt;/p>
&lt;pre>&lt;code class="language-julia">4x5 Array{Rational{T&amp;amp;lt;:Integer},2}:
17//1 2//1 3//1 13//1 93//2
0//1 194//17 155//17 71//17 963//34
0//1 101//17 92//17 87//17 287//17
0//1 230//17 243//17 -18//17 579//17
&lt;/code>&lt;/pre>
&lt;p>I almost never need that header line that Julia gives. The numbers are already showing themselves to be Rational, and the shape of the array is self-evident. (Though I now see that MATLAB 2016b is adding such headers to non-float output.) The zero structure also jumps out more clearly in the MATLAB case, though it&amp;rsquo;s profligate with whitespace.&lt;/p>
&lt;p>Another comparison, of MATLAB (using the default format):&lt;/p>
&lt;pre>&lt;code class="language-matlab">1 0 0 0 0 1
0 1 0 0 0 2
0 0 1 0 0 4
0 0 0 1 0 8
0 0 0 0 1 16
0 0 0 0 0 32
&lt;/code>&lt;/pre>
&lt;p>versus Julia:&lt;/p>
&lt;pre>&lt;code class="language-julia">6×6 Array{Float64,2}:
1.0 0.0 0.0 0.0 0.0 1.0
0.0 1.0 0.0 0.0 0.0 2.0
0.0 0.0 1.0 0.0 0.0 4.0
0.0 0.0 0.0 1.0 0.0 8.0
0.0 0.0 0.0 0.0 1.0 16.0
0.0 0.0 0.0 0.0 0.0 32.0
&lt;/code>&lt;/pre>
&lt;p>There&amp;rsquo;s nothing wrong per se about Julia&amp;rsquo;s. But which version would you write down, or expect to see in print? One last case, of a matrix that is supposed to be triangular but for a little roundoff. First, Julia:&lt;/p>
&lt;pre>&lt;code class="language-julia">4×4 Array{Float64,2}:
17.0 2.0 3.0 13.0
0.0 13.5294 14.2941 -1.05882
0.0 0.0 -2.93913 5.06957
5.55112e-16 0.0 -4.44089e-16 4.09024
&lt;/code>&lt;/pre>
&lt;p>And MATLAB, using the default format:&lt;/p>
&lt;pre>&lt;code class="language-matlab">17.0000 2.0000 3.0000 13.0000
0 13.5294 14.2941 -1.0588
0 0 -2.9391 5.0696
0.0000 0 -0.0000 4.0902
&lt;/code>&lt;/pre>
&lt;p>Julia has chosen to align on the decimal point. It&amp;rsquo;s also suppressing trailing zeros, except for the first, giving an odd and false impression of values that have a precise number of significant digits. MATLAB&amp;rsquo;s choice of right alignment is visually superior, and only exact zero gets a special display. True, you might want that exponential notation for the tiny values; you can get it by changing the format.&lt;/p>
&lt;pre>&lt;code class="language-matlab">&amp;gt;&amp;gt; format short e
&amp;gt;&amp;gt; U
U =
1.7000e+01 2.0000e+00 3.0000e+00 1.3000e+01
0 1.3529e+01 1.4294e+01 -1.0588e+00
0 0 -2.9391e+00 5.0696e+00
5.5511e-16 0 -4.4409e-16 4.0902e+00
&amp;gt;&amp;gt; format short g
&amp;gt;&amp;gt; U
U =
17 2 3 13
0 13.529 14.294 -1.0588
0 0 -2.9391 5.0696
5.5511e-16 0 -4.4409e-16 4.0902
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s nice to have options.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia: Lecture 19, Stability of least squares</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-19-stability-of-least-squares/</link><pubDate>Tue, 11 Oct 2016 21:16:35 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-19-stability-of-least-squares/</guid><description>&lt;p>Here are &lt;a href="https://gist.github.com/tobydriscoll/dfb794e2c6891944790e628f68058ba4" target="_blank" rel="noopener">the notebooks&lt;/a> in MATLAB and Julia.&lt;/p>
&lt;p>The new wrinkle in these codes is extended precision. In MATLAB you need to have the Symbolic Math toolbox to do this in the form of &lt;code>vpa&lt;/code>. In Julia, you have to use version 0.5 or (presumably) later, which had a surprising side effect I&amp;rsquo;ll get to below.&lt;/p>
&lt;p>The reason for extended precision is that this lecture presents experiments on the accuracy of different algorithms for linear least squares problems. In order to demonstrate this on a fairly ill conditioned problem, the answer is supposed to be computed in extended precision, yielding a normalization constant that sets the desired quantity to be 1 for at least 16 significant digits.&lt;/p>
&lt;p>The least squares problem comes from fitting exp(sin(4t)) to a polynomial of degree 14. I see two ways to define how extended precision is to be used. Option (1) is to form the matrix $A$ and the vector $b$ in double precision, then solve the least squares problem with them, but in extended precision. Option (2) is to build in extended precision from the beginning of the problem, creating $A$ and $b$ that differ in the extended digits. I was first attracted to option (1), but option (2) has the clear advantage that the result should be independent of machine and language, whereas in the other case the data could be rounded or computed differently to double precision.&lt;/p>
&lt;p>Here&amp;rsquo;s how this looks in MATLAB.&lt;/p>
&lt;pre>&lt;code class="language-matlab">t = vpa(0:m-1,64)'/vpa(m-1,64); % 64 sig. digits!
A = t.^0;
for j = 1:14, A=[A,t.*A(:,j)]; end
b = exp(sin(4*t));
[Q,R] = qr(A,0); % Householder QR
x1 = R\ (Q'*b);
[Q,R] = mgs([A b]); % Gram-Schmidt QR
x2 = R(1:15,1:15) \ R(1:15,16);
&lt;/code>&lt;/pre>
&lt;p>Here are the outputs for the last element of x in the four methods:&lt;/p>
&lt;pre>&lt;code>2006.7874531048518338761038143559
2006.7874531048518338761038143553
2006.7874531048518338766907539159
2006.7874531048518338761038143555
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s not a problem that the third result disagrees in the last 10 or so digits, since that&amp;rsquo;s an unstable method.&lt;/p>
&lt;p>Here&amp;rsquo;s how it went in Julia.&lt;/p>
&lt;pre>&lt;code class="language-julia">setprecision(BigFloat,128); # use 128-bit floats
t = convert(Array{BigFloat},collect(0:m-1))/convert(BigFloat,m-1);
A = [t[i].^j for i=1:m, j=0:n-1];
b = exp(sin(4*t));
(Q,R) = qr(A);
x1 = R\ (Q'*b);
(Q,R) = mgs([A b]);
x2 = R[1:15,1:15] \ R[1:15,16];
x3 = (A'*A)$$A'*b);
x4 = A\b;
&lt;/code>&lt;/pre>
&lt;p>That first line isn&amp;rsquo;t pretty, but after that it&amp;rsquo;s quite natural. I found Juila&amp;rsquo;s extended precision to be fast compared to MATLAB&amp;rsquo;s. The results:&lt;/p>
&lt;pre>&lt;code>2.006787453104851833876103814338068195207e+03
2.006787453104851833876103814355358077263e+03
2.006787453104851834342923924263804001505e+03
2.006787453104851833876103814376793404332e+03
&lt;/code>&lt;/pre>
&lt;p>These are the same up to the last couple of digits of MATLAB&amp;rsquo;s answer. Unfortunately, my values don&amp;rsquo;t agree with what&amp;rsquo;s in T&amp;amp;B, which is 2006.787453080206. The text doesn&amp;rsquo;t say much about how this was done, so it&amp;rsquo;s impossible for me to say why.&lt;/p>
&lt;p>I probably don&amp;rsquo;t pay enough attention to extended precision. I know some people in the radial basis function community who use it to overcome the very poor conditioning of those bases. They seem quite happy with it. It&amp;rsquo;s always felt like cheating to me, but that&amp;rsquo;s hardly a rational argument.&lt;/p>
&lt;p>Above I said that there was an unexpected side effect related to my using extended precision in Julia. I discovered that (a) it became available in base Julia in version 0.5 and (b) the homebrew Julia I had installed was version 0.4.3, even though 0.5 had apparently been out for a while. Upon upgrading, I found that my MGS routine throwing an error! The offending line was&lt;/p>
&lt;pre>&lt;code class="language-julia">A[:,j+1:n] -= Q[:,j]*R[j,j+1:n];
&lt;/code>&lt;/pre>
&lt;p>The issue is that now both of the references on the right-hand side are vectors, which have only one dimension. Therefore the implied outer product is considered undefined. I had to switch to&lt;/p>
&lt;pre>&lt;code class="language-julia">A[:,j+1:n] -= Q[:,j:j]*R[j:j,j+1:n];
&lt;/code>&lt;/pre>
&lt;p>Because &lt;code>j:j&lt;/code> is a range, not a scalar, the submatrix references are two-dimensional matrices with appropriate singleton dimensions, so the outer product proceeds.&lt;/p>
&lt;p>I&amp;rsquo;m not sure how to feel about this. It&amp;rsquo;s disturbing to extract a row of a matrix and get an object without a row shape. In fact you can even say it&amp;rsquo;s got a column shape, because you are allowed to transpose it into a 1-by-n matrix! On the other hand, there are consistent rules governing the indexing, and 0D, 1D, and 2D extractions are all possible. I&amp;rsquo;m starting to think that the true problem is that I learned and conceptualize linear algebra in a way that works up to dimension 2 but contains some implied hacks that break multilinear algebra. I wish I knew this stuff better.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lectures 12-13: Conditioning and floating point</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-12-13-conditioning-and-floating-point/</link><pubDate>Tue, 04 Oct 2016 17:57:57 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-12-13-conditioning-and-floating-point/</guid><description>&lt;p>I&amp;rsquo;ve run into trouble managing gists with lots of files in them, so I&amp;rsquo;m back to doing one per lecture. Here are &lt;a href="https://gist.github.com/tobydriscoll/df92e76d369617e2f5e56cf2fdab8117" target="_blank" rel="noopener">Lecture 12&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/7449b8d30704663835214af91d2b4d90" target="_blank" rel="noopener">Lecture 13&lt;/a>.&lt;/p>
&lt;p>We&amp;rsquo;ve entered Part 3 of the book, which is on conditioning and stability matters. The lectures in this part are heavily theoretical and often abstract, so I find a little occasional computer time helps to clear the cobwebs.&lt;/p>
&lt;p>Right off the top, in reproducing Figure 12.1, I ran right into the trap I worried about in &lt;a href="https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-11-least-squares/">my last post&lt;/a> regarding polynomials in Julia. In MATLAB, the polynomial coefficients are just a plain vector. That makes perturbing them trivial:&lt;/p>
&lt;pre>&lt;code class="language-matlab">p = poly([1,1,1,0.4,2.2]); % polynomial with these roots
q = p + 1e-9*randn(size(p)); % perturb its coefficients
&lt;/code>&lt;/pre>
&lt;p>In Julia, you can use the &lt;code>Polynomials&lt;/code> package and get polynomial objects. Behold:&lt;/p>
&lt;pre>&lt;code class="language-julia">using Polynomials
p = poly([1,1,1,0.4,2.2]); # polynomial with these roots
q = p + Poly(1e-9*randn(6)); # perturb coefficients
&lt;/code>&lt;/pre>
&lt;p>Note that &lt;code>poly&lt;/code> constructs a polynomial from a vector of &lt;em>roots&lt;/em>, while &lt;code>Poly&lt;/code> constructs one from a vector of &lt;em>coefficients&lt;/em>. Sure enough, I used &lt;code>poly&lt;/code> in both lines the first time around. It&amp;rsquo;s a pernicious mistake, because it produces no error&amp;mdash;the polynomials can be added no matter what. The mistake was mine, but I think this is an unfortunate design.&lt;/p>
&lt;p>The only other notable usage in Lecture 12 is my first use of a &lt;a href="http://docs.julialang.org/en/release-0.5/manual/arrays/?highlight=matrix%20comprehension#comprehensions" target="_blank" rel="noopener">comprehension&lt;/a>:&lt;/p>
&lt;pre>&lt;code class="language-julia">hilb(n) = [ 1.0/(i+j) for i=1:n, j=1:n ];
&lt;/code>&lt;/pre>
&lt;p>This is a pretty handy way to create a matrix.&lt;/p>
&lt;p>In Lecture 13 I had some fun dissecting floating point numbers in both systems. There was only one area in which Julia didn&amp;rsquo;t go as smoothly as I would hope. MATLAB offers &lt;code>realmin&lt;/code> and &lt;code>realmax&lt;/code> , which give the smallest and largest normalized floating point numbers. While Julia has similar-sounding commands, they are interpreted differently:&lt;/p>
&lt;pre>&lt;code class="language-julia">julia&amp;gt; typemin(Float64), typemax(Float64)
(-Inf,Inf)
&lt;/code>&lt;/pre>
&lt;p>Eh, not so much. There is even one more layer of subtlety. Consider&lt;/p>
&lt;pre>&lt;code class="language-julia">julia&amp;gt; (prevfloat(Inf),nextfloat(0.0))
(1.7976931348623157e308,5.0e-324)
&lt;/code>&lt;/pre>
&lt;p>The first of these values is exactly the same as &lt;code>realmax&lt;/code>, but the second is not &lt;code>realmin&lt;/code>. &lt;a href="https://en.wikipedia.org/wiki/IEEE_floating_point" target="_blank" rel="noopener">IEEE 754 double precision&lt;/a> has &amp;ldquo;denormalized&amp;rdquo; numbers that let you trade away bits of precision to get closer to zero in magnitude. Julia is reporting the smallest denormalized number, not the smallest full-precision number. Julia&amp;rsquo;s not wrong, but access to the extreme finite double precision values isn&amp;rsquo;t as straightforward as it could be.&lt;/p>
&lt;p>One last observation. Trefethen &amp;amp; Bau refer to the value $2^{-53}$ as &amp;ldquo;machine epsilon.&amp;rdquo; This isn&amp;rsquo;t what MATLAB and Julia use, which is $2^{-52}$. Nick Higham&amp;rsquo;s &lt;em>Accuracy and Stability of Numerical Algorithms&lt;/em> also has &amp;ldquo;machine epsilon&amp;rdquo; at $2^{-52}$ and calls $2^{-53}$ &amp;ldquo;unit roundoff.&amp;rdquo; Stoer and Bulirsch (2nd ed.) call $2^{-53}$ &amp;ldquo;machine precision.&amp;rdquo; Corless and Fillion seem to agree with Higham. Golub and Van Loan (3rd ed.) don&amp;rsquo;t use &amp;ldquo;machine epsilon&amp;rdquo; at all, and in the index one finds&lt;/p>
&lt;blockquote>
&lt;p>Machine precision. &lt;em>See&lt;/em> unit roundoff.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>Sigh.&lt;/em> The mathematical uses are, unsurprisingly, consistent. Frankly, I feel better about my personal inconsistencies at using those terms: at least I stood on the shoulders of giants.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 11: Least squares</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-11-least-squares/</link><pubDate>Wed, 28 Sep 2016 13:19:16 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-11-least-squares/</guid><description>&lt;p>This week&amp;rsquo;s notebooks (&lt;a href="https://gist.github.com/tobydriscoll/8d87997704e9fd400e96ea860d9f6a34#file-tb-lecture-11-ipynb" target="_blank" rel="noopener">MATLAB&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/3d9b29d953882738c51c9cabdcaf431b#file-tb-lecture-11-ipynb" target="_blank" rel="noopener">Julia&lt;/a>&amp;ndash;now all lectures are together for each language) are about least squares polynomial fitting.&lt;/p>
&lt;p>The computational parts are almost identical, except for how polynomials are represented. In MATLAB, a vector of coefficients is interpreted as a polynomial in the context of particular functions, such as &lt;code>polyval&lt;/code>. The major pain is that the convention is for the coefficients to be ordered from high degree to low, which is almost always the opposite of what you really want. Hence I&amp;rsquo;ve gotten used to writing code like&lt;/p>
&lt;pre>&lt;code>p = @(x) polyval( c(end: -1:1), x-1955 );
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s not a big deal, but it trips up some students every semester.&lt;/p>
&lt;p>Julia has a full-fledged polynomial type, if you care to add and load the package. And, it expects ordering from the constant term to the highest degree. So I came up with&lt;/p>
&lt;pre>&lt;code class="language-julia">p = Poly(c);
q = t -&amp;gt; p(t-1955);
&lt;/code>&lt;/pre>
&lt;p>Simple enough, but I find two disappointments. First, it&amp;rsquo;s a bare-bones class. For instance, the second object &lt;code>q&lt;/code> above is also a polynomial, but we&amp;rsquo;ll never know it formally, or be able to get its coefficients. A &lt;code>shiftvar&lt;/code> method or something similar would be nice. Second, in the effort to clone the MATLAB interface, a potential for serious confusion was introduced. The command &lt;code>p=poly(c)&lt;/code>
also works, but (like MATLAB&amp;rsquo;s counterpart) constructs a polynomial whose roots, not coefficients, are given. This is &lt;em>way&lt;/em> too easy a mistake to make.&lt;/p>
&lt;p>Another element this time was that I tried using the nascent &lt;a href="https://juliaplots.github.io/" target="_blank" rel="noopener">Plots package&lt;/a> for Julia. It&amp;rsquo;s an interesting attempt to graft a graceful interface onto the various graphics backends that already exist. I was motivated to try it because AFAIK, the &lt;code>PyPlots&lt;/code> package lacks a counterpart to &lt;code>fplot&lt;/code> from MATLAB. Perhaps in part because of my time with the &lt;a href="http://www.chebfun.org" target="_blank" rel="noopener">Chebfun project&lt;/a>, I have been putting more emphasis in my teaching on representing functions as such, rather than implicitly as vectors of numbers. It bothers me now, for example, that functions such as &lt;code>interp1&lt;/code> and &lt;code>ode45&lt;/code> return numbers or structures rather than callable functions, which is what their algorithms should be doing in the deep sense.&lt;/p>
&lt;p>Anyhow, I end up using &lt;code>fplot&lt;/code> a lot because of my emphasis on functions, and couldn&amp;rsquo;t find a counterpart in &lt;code>PyPlot&lt;/code>. In &lt;code>Plots&lt;/code>, however, the &lt;code>plot&lt;/code> command handles both numerical and functional arguments alike. Here&amp;rsquo;s a snippet from the notebook:&lt;/p>
&lt;pre>&lt;code class="language-julia">p = Poly(c);
plot( t-&amp;gt;p(t-1955), 1955,2000 )
plot!( year,anomaly, m=:o,l=nothing );
title!(&amp;quot;World temperature anomaly&amp;quot;);
xlabel!(&amp;quot;year&amp;quot;); ylabel!(&amp;quot;anomaly (deg C)&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Not bad! You can see a couple of quirks though. One is the use of &lt;a href="http://docs.julialang.org/en/release-0.5/manual/functions/#keyword-arguments" target="_blank" rel="noopener">keyword arguments&lt;/a> in line 3; the arguments &lt;code>m=:o&lt;/code> and &lt;code>l=nothing&lt;/code> respectively set the point markers to circles and the lines connecting points to be suppressed. This takes getting used to, but it&amp;rsquo;s memorable and compact enough.&lt;/p>
&lt;p>The other quirk that you see above is the use of the banged commands like &lt;code>plot!&lt;/code> and &lt;code>title!&lt;/code>. The bang in Julia is a convention meaning &amp;ldquo;operate in place&amp;rdquo; or &amp;ldquo;overwrite existing.&amp;rdquo; By default, the MATLAB-like commands replace the existing plot, so they have to be banged in order to build on top of it instead. This is a bit dubious in the case of titles and labels––why would I create a new plot by issuing a title?––but it is at least consistent, and, unlike the global state used in MATLAB by the &lt;code>hold&lt;/code> command, works the same regardless of context and history.&lt;/p>
&lt;p>One quirk––to me, a bug––that you don&amp;rsquo;t see is that the default in &lt;code>Plots&lt;/code> is that every plot creates or adds to a legend. I&amp;rsquo;m not a big fan of plot legends in most contexts, but you&amp;rsquo;re welcome to them if you like them. However, I don&amp;rsquo;t find it reasonable to have one forced on me for a graph with a single curve that I didn&amp;rsquo;t give a label to! I turned off this travesty by starting off with&lt;/p>
&lt;pre>&lt;code class="language-julia">using Plots; pyplot(legend=false);
&lt;/code>&lt;/pre>
&lt;p>which at least is straightforward, though entangled with my choice of backend.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 9: MATLAB</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-9-matlab/</link><pubDate>Tue, 20 Sep 2016 19:52:34 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-9-matlab/</guid><description>&lt;p>For &lt;a href="https://gist.github.com/tobydriscoll/3030cf16ae47dc58a4b51d9b47c8289b" target="_blank" rel="noopener">today&amp;rsquo;s notebooks&lt;/a> I got caught on a problem I anticipated in theory but failed to spot in practice for longer than I would like to admit.&lt;/p>
&lt;p>First let me mention how interesting this Lecture is to me personally. The title of the lecture is &amp;ldquo;MATLAB&amp;rdquo;, and it details three numerical experiments. The first of these uses QR factorization of a discretization of monomials in order to approximate the Legendre polynomials. I skipped this one here because I opted in class to show how Gram-Schmidt looks using &lt;a href="www.chebfun.org">Chebfun&lt;/a>. (It&amp;rsquo;s awesome.)&lt;/p>
&lt;p>The other two numerical experiments show different aspects of numerical instability of the Gram-Schmidt algorithm, classical and modified. The MATLAB version looks just like I would have written it 20 years ago:&lt;/p>
&lt;pre>&lt;code class="language-matlab">[U,S,V] = svd(randn(80));
s = 2.^(-1👎-80);
A = U*diag(s)*V';
semilogy(s,'.')
[Qc,Rc] = gs(A); &amp;amp;nbsp;% classical
hold on, semilogy(diag(Rc),'o')
[Qm,Rm] = mgs(A); % modified
semilogy(diag(Rm),'s')
&lt;/code>&lt;/pre>
&lt;p>The idea is that the diagonal elements of &lt;code>R&lt;/code> descend exponentially just like the singular values do. If you run this code (or peek at the link at the top), you see that MGS stops tracking them right around machine precision, whereas the less stable classical version wanders off at about half of the available digits.&lt;/p>
&lt;p>I did introduce my own wrinkle here. I can&amp;rsquo;t believe I haven&amp;rsquo;t thought of using this for teaching before, but by the conversion &lt;code>A=single(A)&lt;/code> I can simulate a different value of machine epsilon without changing anything else! It backs up the observations from the first graph.&lt;/p>
&lt;p>In Julia this gambit ran into a big snag. Here was my first code for MGS:&lt;/p>
&lt;pre>&lt;code class="language-julia">function mgs(A)
m,n = size(A);
Q = zeros(m,n); R = zeros(n,n);
for j = 1:n
R[j,j] = norm(A[:,j]);
Q[:,j] = A[:,j]/R[j,j];
R[j,j+1:n] = Q[:,j]'*A[:,j+1:n];
A[:,j+1:n] -= Q[:,j]*R[j,j+1:n];
end
return Q,R
end
&lt;/code>&lt;/pre>
&lt;p>Everything was fine in double precision. After a couple of missteps, I figured out how to make `A`` single precision:&lt;/p>
&lt;pre>&lt;code class="language-julia">A = convert(Array{Float32},A)
&lt;/code>&lt;/pre>
&lt;p>Not beautiful, but it works. However, while it had the desired effect on MGS, it did nothing to classical GS! It finally came down to a surprise:&lt;/p>
&lt;pre>&lt;code class="language-julia">julia&amp;gt; typeof( 1.0f0 + 1.0f0 )
Float32
julia&amp;gt; typeof( 1.0f0 + 1.0 )
Float64
&lt;/code>&lt;/pre>
&lt;p>A single plus a double is double. The rule in Julia is that the operands are converted to a type that can represent them both. MATLAB gives a different outcome, converting both numbers to single:&lt;/p>
&lt;pre>&lt;code class="language-matlab">&amp;gt;&amp;gt; class( single(1) + 1 )
ans =
single
&lt;/code>&lt;/pre>
&lt;p>I suppose the philosophy here is that there&amp;rsquo;s no point padding the numbers with meaningless digits&amp;mdash;the moment you introduce a single precision value, you&amp;rsquo;ve chosen that level of precision. I think that&amp;rsquo;s the more sensible choice for floating point; Julia is concerned with the consistency of its much more intricate and far-reaching type system. For Julia I changed the initialization of &lt;code>Q&lt;/code> and &lt;code>R&lt;/code> to&lt;/p>
&lt;pre>&lt;code class="language-julia">Q = zeros(A);
R = zeros(Q[1:n,1:n]);
&lt;/code>&lt;/pre>
&lt;p>That way they are initialized with the correct type in either case.&lt;/p>
&lt;p>Now for the bonehead move of the day. I seemed to get inconsistent and nonreproducible results in the single precision cases. I went away, did other things, came back into a fresh session, and&amp;hellip;no difference between double and single precision. I may have said a few things I now come to regret. Finally I remembered the key: Julia passes by reference, not value. MGS alters the input matrix, which has no effect outside the function in MATLAB but changes the &amp;lsquo;master copy&amp;rsquo; in Julia. A little switch to&lt;/p>
&lt;pre>&lt;code class="language-julia">function mgs(B)
A = copy(B);
&lt;/code>&lt;/pre>
&lt;p>and all was well. This is an example of how much MATLAB has shaped my thinking about programming. IIRC, MATLAB doesn&amp;rsquo;t always pass by value; if an input argument is not altered, it is not copied. But it&amp;rsquo;s handled by the compiler, not the programmer.&lt;/p>
&lt;p>If nothing else I&amp;rsquo;m getting ever more clarity on the ways MATLAB keeps things simple. Variables are bound to their values, period. Single precision is an irrevocable choice. Scalars and 1x1 matrices are the same thing. Don&amp;rsquo;t it always seem to go that you don&amp;rsquo;t know what you&amp;rsquo;ve got &amp;rsquo;til it&amp;rsquo;s gone?&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 8: Gram-Schmidt</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-8-gram-schmidt/</link><pubDate>Mon, 19 Sep 2016 19:44:42 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-8-gram-schmidt/</guid><description>&lt;p>This lecture is about the modified Gram-Schmidt method and flop counting. The &lt;a href="https://gist.github.com/tobydriscoll/bae2a5e864f490e571d79a0af541fb8c" target="_blank" rel="noopener">notebooks are here&lt;/a>.&lt;/p>
&lt;p>I&amp;rsquo;m lost.&lt;/p>
&lt;p>Almost as an afterthought I decided to add a demonstration of the timing of Gram-Schmidt compared to the asymptotic &lt;span class='MathJax_Preview'>&lt;img src='https://i2.wp.com/tobydriscoll.net/blog/wp-content/plugins/latex/cache/tex_9f84a66d88d24c3b1bc91df5b5346a13.gif?w=500' style='vertical-align: middle; border: none; ' class='tex' alt="O(n^2)" data-recalc-dims="1" /> flop count. Both MATLAB and Julia got very close to the trend as &lt;span class='MathJax_Preview'>&lt;img src='https://i0.wp.com/tobydriscoll.net/blog/wp-content/plugins/latex/cache/tex_7b8b965ad4bca0e41ab51de7b31363a1.gif?w=500' style='vertical-align: middle; border: none; padding-bottom:2px;' class='tex' alt="n" data-recalc-dims="1" /> got into the hundreds, using vectorized code:&lt;/p>
&lt;pre>&lt;code class="language-julia">n_ = collect(50:50:500);
time_ = zeros(size(n_));
for k = 1:length(n_)
n = n_[k];
A = rand(1200,n);
Q = zeros(1200,n); R = zeros(600,600);
tic();
R[1,1] = norm(A[:,1]);
Q[:,1] = A[:,1]/R[1,1];
for j = 2:n
R[1:j-1,j] = Q[:,1:j-1]'*A[:,j];
v = A[:,j] - Q[:,1:j-1]*R[1:j-1,j];
R[j,j] = norm(v);
Q[:,j] = v/R[j,j];
end
time_[k] = toc();
end
using PyPlot
loglog(n_,time_,&amp;quot;-o&amp;quot;,n_,(n_/500).^2,&amp;quot;--&amp;quot;)
xlabel(&amp;quot;n&amp;quot;), ylabel(&amp;quot;elapsed time&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>I noticed that while the timings were similar, Julia lagged MATLAB just a bit. I decided this would be a great chance for me to see Julia&amp;rsquo;s prowess with speedy loops firsthand.&lt;/p>
&lt;p>Compare the vectorized and unvectorized Julia versions here:&lt;/p>
&lt;script src="https://gist.github.com/tobydriscoll/c515e9f5bd4ab540b41db9852db53b72.js">&lt;/script>
&lt;p>Look at the last line&amp;ndash;it&amp;rsquo;s allocating 1.4GB of memory to make the nested loop version happen! I thought perhaps I should use &lt;code>copy&lt;/code> to create &lt;code>v&lt;/code> in each pass, but that change didn&amp;rsquo;t help. I even tried writing my own loop for computing the dot product, to no avail.&lt;/p>
&lt;p>It did help a little to replace the line in which &lt;code>v&lt;/code> is updated with&lt;/p>
&lt;pre>&lt;code class="language-julia">v = broadcast!(-,v,Q[:,i]*R[i,j])
&lt;/code>&lt;/pre>
&lt;p>The bang on the name of the function makes it operate in-place, overwriting the current storage. Apparently Julia will create &lt;a href="https://github.com/JuliaLang/julia/pull/17546" target="_blank" rel="noopener">some syntactic sugar for this maneuver in version 0.5&lt;/a>. Here it reduced the memory usage to 1.1 GB.&lt;/p>
&lt;p>Julia&amp;rsquo;s reputation is that it&amp;rsquo;s great with loops, especially compared to MATLAB and Python. As a Julia newbie I recognize that there may still be only a small change I need to make in order to see this for myself. But I feel as though having to use that &lt;code>broadcast!&lt;/code>, or even the more natural &lt;code>.=&lt;/code> that may be coming, is already too much to ask. I&amp;rsquo;m frustrated, confused, and disappointed.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lectures 6-7</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-6-7/</link><pubDate>Fri, 16 Sep 2016 20:27:58 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-6-7/</guid><description>&lt;p>Here are the Jupyter notebooks for &lt;a href="https://gist.github.com/tobydriscoll/fb438fc942a01e242cc08ee05385af17" target="_blank" rel="noopener">Lecture 6&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/4d1c8856da0c718e1d99d8171e1babec" target="_blank" rel="noopener">Lecture 7&lt;/a>. (I finally noticed that a Gist can hold more than one notebook&amp;hellip;duh.)&lt;/p>
&lt;p>Not much happened in Lecture 6, but I got gobsmacked in Lecture 7. It happened when I tried to convert this boring MATLAB code for backward substitution.&lt;/p>
&lt;pre>&lt;code class="language-matlab">A = magic(9); b = (1:9)';
[Q,R] = qr(A);
z = Q'*b;
x(9,1) = z(9)/R(9,9);
for i = 8👎1
x(i) = (z(i) - R(i,i+1:9)*x(i+1:9)) / R(i,i);
end
&lt;/code>&lt;/pre>
&lt;p>Here is what I first tried in Julia.&lt;/p>
&lt;pre>&lt;code class="language-julia">A = round(10*rand(9,9)); b = (1:9);
m = 9;
(Q,R) = qr(A);
z = Q'*b;
x = zeros(m);
x[m] = z[m]/R[m,m];
for i = m-1👎1
x[i] = (z[i] - R[i,i+1:m]*x[i+1:m]) / R[i,i];
end
&lt;/code>&lt;/pre>
&lt;p>Seems straightforward, but line 4 gives an error. I&amp;rsquo;m not going to copy the error message here, in case you&amp;rsquo;re using mobile data right now. What I mean is that it is verbose, not to mention obscure. You don&amp;rsquo;t appreciate simple, clear error messages until you get something else!&lt;/p>
&lt;p>Anyhow, I then remembered that in Julia, the colon construction &lt;code>(1:9)&lt;/code> produces a Range, not a Vector. As I understand it, Julia embraces a lazy design philosophy: it avoids evaluation of an expression until the last possible moment. Suppose the only use of that Range is to describe a loop iteration&amp;mdash;in that case, why have a vector?&lt;/p>
&lt;p>I&amp;rsquo;m all for lazy philosophy. (Haw haw!) It&amp;rsquo;s not clear to me why the context &lt;code>Q'*b&lt;/code>
does not automatically convert the Range into a Vector. It&amp;rsquo;s even less clear why they have deprecated the idiom &lt;code>[1:9]&lt;/code> to create a Vector; it works for now but gives a warning. Instead one should use &lt;code>collect&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-julia">A = round(10*rand(9,9)); b = collect(1:9);
m = 9;
(Q,R) = qr(A);
z = Q'*b;
x = zeros(m);
x[m] = z[m]/R[m,m];
for i = m-1👎1
x[i] = (z[i] - R[i,i+1:m]*x[i+1:m]) / R[i,i];
end
&lt;/code>&lt;/pre>
&lt;p>Feels very odd to me still, but okay.&lt;/p>
&lt;p>We are not out of the woods yet. This version still fails in the loop body, again vomiting opaque error messages. Remember how, &lt;a href="https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-2/">back in Lecture 2, I mentioned&lt;/a> that scalars and 1x1 matrices are different things? Inside the loop above, &lt;code>z[i]&lt;/code> is a scalar and the product is a length-1 vector. But the subtraction works anyway, as &lt;code>z[i]&lt;/code> is silently promoted to a 1-vector also. No, the problem comes with the assignment: you can&amp;rsquo;t assign a 1-vector to an element of an array of numbers.&lt;/p>
&lt;p>There&amp;rsquo;s a very long (space and time) &lt;a href="https://github.com/JuliaLang/julia/issues/4774#issuecomment-28430963" target="_blank" rel="noopener">discussion&lt;/a> about this and related issues in Julia. Suffice it to say that what mathematicians do with scalars, vectors, matrices, and tensors isn&amp;rsquo;t rigorously consistent&amp;mdash;or at least, there seem to be multiple, incompatible rigorous ways to use them.&lt;/p>
&lt;p>In this particular case I have found two unsatisfying workarounds. The idiom
&lt;code>x[i:i]&lt;/code> produces a Vector, not a scalar, so the assignment goes through. Or we can work on the other side of the assignment and pull out the scalar from the vector:&lt;/p>
&lt;pre>&lt;code class="language-julia">(z[i] - R[i,i+1:m]*x[i+1:m])[1] / R[i,i]
&lt;/code>&lt;/pre>
&lt;p>Now, it&amp;rsquo;s pleasing that this syntax does work, as there is no good MATLAB equivalent for indexing into a temporary expression. I just wish it was in the service of something less dismal.&lt;/p>
&lt;p>Again: Julia&amp;rsquo;s designers have solid reasons for doing things this way. I wouldn&amp;rsquo;t consider it a dealbreaker for research codes, but this episode is not something I would want to explain to undergrads who are just wrapping their heads around LU factorization. It pulls you right out of thinking about math and into thinking about strict-typing, pinhead-dancing angels. How unfortunate.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia Lecture 5: More on the SVD</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-5-more-on-the-svd/</link><pubDate>Mon, 12 Sep 2016 19:47:08 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-5-more-on-the-svd/</guid><description>&lt;p>Notebooks are viewable for &lt;a href="https://gist.github.com/tobydriscoll/934b88e10ac3b6ce6ac95c8c8f480aef" target="_blank" rel="noopener">matlab&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/40d7b7670558f58958e9136febaeec20" target="_blank" rel="noopener">julia&lt;/a>.&lt;/p>
&lt;p>This is one of my favorite demos. It illustrates low-rank approximation by the SVD to show patterns in voting behavior for the U.S. Congress. With no a priori models, project onto two singular vectors and pow&amp;ndash;&lt;a href="http://www.nytimes.com/2003/06/24/science/a-mathematician-crunches-the-supreme-court-s-numbers.html" target="_blank" rel="noopener">meaning and insight jump out&lt;/a>.&lt;/p>
&lt;p>I took one shortcut. I have a MATLAB script that reads the raw voting data from &lt;a href="http://voteview.com" target="_blank" rel="noopener">voteview.com&lt;/a> and converts it to a matrix. No doubt I would learn a lot about I/O in Julia if I translated it, but I got short on time and instead saved it locally from MATLAB. Then load it using the &lt;a href="https://github.com/simonster/MAT.jl" target="_blank" rel="noopener">MAT package&lt;/a> for Julia and Bob&amp;rsquo;s your uncle.&lt;/p>
&lt;p>I did stumble into a nasty gotcha, though. I decided to make histograms for the distributions of the &amp;ldquo;partisan&amp;rdquo; and &amp;ldquo;bipartisan&amp;rdquo; coordinate values. Unfortunately, there&amp;rsquo;s a name clash: MATLAB&amp;rsquo;s best known histogram plotter is called &lt;code>hist&lt;/code>, but Julia has a built-in function by that name that just bins the data. I knew there was also a &lt;code>hist()&lt;/code> in PyPlot, but to my bafflement the access for it was not &lt;code>PyPlot.hist()&lt;/code>, which does exist:&lt;/p>
&lt;pre>&lt;code class="language-julia">help?&amp;gt; PyPlot.hist
hist(v, e) -&amp;amp;gt; e, counts
Compute the histogram of v using a vector/range e as the edges...
hist(v[, n]) -&amp;amp;gt; e, counts
Compute the histogram of v, optionally using approximately...
&lt;/code>&lt;/pre>
&lt;p>This is Julia&amp;rsquo;s built-in function. The next thing I tried was typing in &lt;code>Pyplot.&lt;/code> and hitting tab for a list of completions. Most of the familiar MATLAB-style plotting functions are there, but no &lt;code>hist&lt;/code>, just &lt;code>hist2D&lt;/code>, which is not equivalent. I don&amp;rsquo;t remember now where I found it, but the way to call the function I want is the bizarre &lt;code>plt[:hist]&lt;/code>. Neither &lt;code>?plt&lt;/code> nor tab completion gives any whiff of this syntax or possibility. Obviously there&amp;rsquo;s some logic at work here, and no doubt my Julia and Python ignorance are showing, but this was the most frustrating Julia experience I&amp;rsquo;ve had yet.&lt;/p>
&lt;p>(Ironically, MATLAB has a newer plotting function called &lt;code>histogram&lt;/code>, which does not seem to conflict with any Julia names!)&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 4: SVD</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-4-svd/</link><pubDate>Mon, 12 Sep 2016 12:51:07 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-4-svd/</guid><description>&lt;p>The notebooks: &lt;a href="https://gist.github.com/tobydriscoll/08fc56bca086f957920f1088e0844c30" target="_blank" rel="noopener">matlab&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/324991720db46ff9c644cc43455bd23e" target="_blank" rel="noopener">julia&lt;/a>.&lt;/p>
&lt;p>Today is about some little conveniences/quirks in Julia. Starting here:&lt;/p>
&lt;pre>&lt;code class="language-julia">t = linspace(0,2*pi,300);
x1,x2 = (cos(t),sin(t));
&lt;/code>&lt;/pre>
&lt;p>The second line assigns to two variables simultaneously. It&amp;rsquo;s totally unnecessary here, but it helps to emphasize how the quantities are related.&lt;/p>
&lt;p>Next we have&lt;/p>
&lt;pre>&lt;code class="language-julia">U,σ,V = svd(A)
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m unreasonably happy about having Greek letters as variable names. Just type in &amp;lsquo;\sigma&amp;rsquo; and hit tab, and voila! It&amp;rsquo;s a reminder of how, in the U.S. at least, we&amp;rsquo;re so used to living within the limitations of ancient 128-character ASCII&amp;mdash;&lt;a href="https://en.wikipedia.org/wiki/ASCII" target="_blank" rel="noopener">telegraphs&lt;/a>, really&amp;mdash;that we can be surprised by expanded possibilities.&lt;/p>
&lt;p>Later on we have &lt;code>diagm(σ)&lt;/code>. In MATLAB, the &lt;code>diag&lt;/code> function has two roles: convert a vector to a diagonal matrix, and extract the diagonal elements of a matrix. This creates a curious edge case for MATLAB: for example, &lt;/p>
&lt;pre>&lt;code class="language-matlab">diag([1 2 3])
&lt;/code>&lt;/pre>
&lt;p>returns a 3-by-3 matrix, not the single element 1. This is almost always what you want, but I&amp;rsquo;ve run into gotchas wherein a program works perfectly until an input of the &amp;lsquo;wrong&amp;rsquo; size silently changes the behavior of a function. In Julia the two functionalities are separated into &lt;code>diag&lt;/code> and &lt;code>diagm&lt;/code>, which avoids the edge case ambiguity. I think it&amp;rsquo;s worth the clarity here to have the extra command.&lt;/p>
&lt;p>The one thing I missed having in the Julia version was MATLAB&amp;rsquo;s &lt;code>format&lt;/code> command, which lets you set the default display of numbers in all following output. In this notebook I just had numbers as placeholders and really wanted just to show shapes and sizes. Julia&amp;rsquo;s full-length output obfuscates the sizes quite a bit, and I&amp;rsquo;d like to tell it to calm down with all those digits for a little while (rather than saying so with each new output). If that capability is there, I overlooked it.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 3: Norms</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-3-norms/</link><pubDate>Wed, 07 Sep 2016 19:32:46 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-3-norms/</guid><description>&lt;p>Here are the &lt;a href="https://gist.github.com/tobydriscoll/b620d1b8beaa04cf87707a55928e3449" target="_blank" rel="noopener">MATLAB&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/2c486e89b12911b073f3c91e514db4f7" target="_blank" rel="noopener">julia&lt;/a> notebooks.&lt;/p>
&lt;p>The big issue this time around was graphics. This topic dramatically illustrates the advantages on both sides of the commercial/open source fence. On the MATLAB side, it&amp;rsquo;s perfectly clear what you should do. There are many options that have been well constructed, and it&amp;rsquo;s all under a relatively consistent umbrella. There are things to learn and options to choose, but it&amp;rsquo;s clear what functions you will be using to make, say, a scatter plot, and a lot of similarity across commands.&lt;/p>
&lt;p>Julia graphics are another story. At this writing, there are two options recommended on &lt;a href="http://julialang.org/downloads/plotting.html" target="_blank" rel="noopener">Julia&amp;rsquo;s official page about plotting packages&lt;/a>: &lt;a href="https://github.com/stevengj/PyPlot.jl" target="_blank" rel="noopener">PyPlot&lt;/a> and &lt;a href="https://github.com/dcjones/Gadfly.jl" target="_blank" rel="noopener">Gadfly&lt;/a>. It doesn&amp;rsquo;t take much exploration to decide that the former is favored by MATLAB veterans and the latter, by R devotees. Confusingly, the &lt;a href="http://julialang.org/downloads/" target="_blank" rel="noopener">general download page&lt;/a> for Julia mentions a third package called &lt;a href="https://github.com/tbreloff/Plots.jl" target="_blank" rel="noopener">Plots&lt;/a> that is supposed to integrate all of the backends. It&amp;rsquo;s still early days for Julia, and I&amp;rsquo;m sure much remains in flux.&lt;/p>
&lt;p>Moreover, because you can (quite easily) import and run Python code in Julia, in principle you have access to all Python plotting packages. One of the big players is &lt;a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib&lt;/a>, which is more or less what Julia&amp;rsquo;s PyPlot is supposed to provide. But there are also &lt;a href="http://bokeh.pydata.org/en/latest/" target="_blank" rel="noopener">Bokeh&lt;/a>, &lt;a href="https://plot.ly/" target="_blank" rel="noopener">plotly&lt;/a>, and &lt;a href="http://www.pyqtgraph.org/" target="_blank" rel="noopener">pyqtgraph&lt;/a>&amp;mdash;for all I know, many more besides. All of these can make gorgeous graphics, often highly interactive and even hosted in the cloud. The relative merits are not at all clear.&lt;/p>
&lt;p>Here we run into the &lt;a href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwjuv7-7j_zOAhXG2xoKHdrgAvYQtwIIHjAA&amp;amp;url=http%3A%2F%2Fwww.ted.com%2Ftalks%2Fbarry_schwartz_on_the_paradox_of_choice%3Flanguage%3Den&amp;amp;usg=AFQjCNHkeD4jDrbOc7TgI5YOQfU1IQ7xOQ" target="_blank" rel="noopener">paradox of choice&lt;/a>: having many options, even good ones, can provoke anxiety rather than satisfaction. Which package do I invest time in learning? MATLAB limits choice but provides a sort of editorial, almost paternal, reassurance.&lt;/p>
&lt;p>My personal goal is to learn Julia from the standpoint of a MATLAB user, so PyPlot it is. All in all, the transition isn&amp;rsquo;t bad, though there are some twists.&lt;/p>
&lt;p>In the last few years I&amp;rsquo;ve been more often turning to automatic function plotting in MATLAB, using &lt;code>fplot&lt;/code>, &lt;code>ezsurf&lt;/code>, and &lt;code>ezcontour&lt;/code>. If PyPlot supports those, I have yet to find out about them. So it&amp;rsquo;s back to the world of evaluating functions on tensor product grids.  A MATLAB veteran turns to meshgrid, but Julia supports broadcasting across singleton dimensions. For example:&lt;/p>
&lt;pre>&lt;code class="language-julia">using PyPlot
x = linspace(-1,1,90);
y = x';
contour(x[:],y[:],sqrt(x.^2 .+ y.^2))&amp;lt;/pre&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Because &lt;code>x&lt;/code> has a column shape while &lt;code>y&lt;/code> has a row shape, the &lt;code>.+&lt;/code>
operator broadcasts each along the &amp;ldquo;missing&amp;rdquo; dimension. It&amp;rsquo;s a clever shortcut once you know it. It works just as well for contours of the vector 1-norm, but for the max norm I had to broadcast manually:&lt;/p>
&lt;pre>&lt;code class="language-julia">contour(x[:],y[:],broadcast(max,abs(x),abs(y)))
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s not clear to me why that broadcast should not happen automatically, given that
&lt;code>max&lt;/code> is a dedicated elementwise operator.&lt;/p>
&lt;p>There&amp;rsquo;s more Julia subtlety hiding in this notebook, but those issues will wait for another time.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 2</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-2/</link><pubDate>Fri, 02 Sep 2016 19:12:54 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-2/</guid><description>&lt;p>Here are the &lt;a href="http://nbviewer.jupyter.org/gist/tobydriscoll/8aa30fdad0346f1c5656ff4a468b1b05" target="_blank" rel="noopener">matlab&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/7f404b36fd47d2878f90dd76a1d7a9b9" target="_blank" rel="noopener">julia&lt;/a> notebooks.&lt;/p>
&lt;p>Two things stood out this time. First, consider the following snippet.&lt;/p>
&lt;pre>&lt;code class="language-julia">u = [ 4; -1; 2+2im ]
v = [ -1; 1im; 1 ]
println(&amp;quot;dot(u,v) gives &amp;quot;, dot(u,v))
println(&amp;quot;u'*v gives &amp;quot;,u'*v)
&lt;/code>&lt;/pre>
&lt;p>The result is&lt;/p>
&lt;pre>&lt;code>dot(u,v) gives -2 - 3im
u'*v gives Complex{Int64}[-2 - 3im]
&lt;/code>&lt;/pre>
&lt;p>Unlike in MATLAB, a scalar is not the same thing as a 1-by-1 matrix. This has consequences. The code &lt;code>(u'*v)*eye(3)&lt;/code> throws a dimension mismatch error, while the equivalent with &lt;code>dot&lt;/code> is fine. In the strict sense this is correct, and I suppose Julia made a decision to be strict in contrast to MATLAB&amp;rsquo;s typical laxity. The price is that little bump introduced into a transition that is normally seamless in the minds of users and programmers 99% of the time.&lt;/p>
&lt;p>The other difference is in style more than anything else. Compare MATLAB&amp;rsquo;s&lt;/p>
&lt;pre>&lt;code class="language-matlab">[Q,~] = qr(A);
&lt;/code>&lt;/pre>
&lt;p>to Julia&amp;rsquo;s&lt;/p>
&lt;pre>&lt;code class="language-julia">Q = qr(A)[1]
&lt;/code>&lt;/pre>
&lt;p>Julia&amp;rsquo;s version would be easier if you wanted to extract the $n$th output, where $n$ is a variable, though you could manage it in MATLAB with cells. I&amp;rsquo;m not sure how common that situation is. Also, it could be a surprise in MATLAB that&lt;/p>
&lt;pre>&lt;code class="language-matlab">Q=qr(A)
&lt;/code>&lt;/pre>
&lt;p>does &lt;em>not&lt;/em> do the same thing, because the content and meaning of the outputs depend on the number of outputs.&lt;/p>
&lt;p>A distinction for QR factorization in particular in the two languages is that MATLAB returns the full version by default, while Julia defaults to the skinny form. The latter is nice because an unsuspecting student (or professor) who calls &lt;code>qr(A)&lt;/code> in MATLAB for a really tall matrix might as well kill the process and restart MATLAB Julia makes you do something extra to get the memory-dangerous version.&lt;/p></description></item><item><title>Trefethen &amp; Bau, Lecture 1</title><link>https://tobydriscoll.net/blog/trefethen-bau-lecture-1/</link><pubDate>Thu, 01 Sep 2016 19:56:08 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-lecture-1/</guid><description>&lt;p>Have a look at the &lt;a href="http://nbviewer.jupyter.org/gist/tobydriscoll/2c2a21256da6efab7433bf42371a43f2" target="_blank" rel="noopener">MATLAB&lt;/a> and &lt;a href="http://nbviewer.jupyter.org/gist/tobydriscoll/32708bacb1d569fc589c51571890028b" target="_blank" rel="noopener">Julia&lt;/a> versions of the notebooks for this lecture.&lt;/p>
&lt;p>The first disappointment in Julia comes right at the start: no &lt;code>magic&lt;/code> command in Julia! Why not? I love demonstrating with magic square matrices:&lt;/p>
&lt;ul>
&lt;li>They are instantly familiar or at least understandable to any level of mathematician.&lt;/li>
&lt;li>They have integer entries and thus display compactly.&lt;/li>
&lt;li>You can demonstrate &lt;code>sum&lt;/code>, transpose, and &lt;code>diag&lt;/code> naturally. And getting the &amp;ldquo;antidiagonal&amp;rdquo; sum is a nice exercise.&lt;/li>
&lt;li>The even-sized ones are singular.&lt;/li>
&lt;li>They have the memorable eigenvector $[1,;1,; \cdots; 1]$.&lt;/li>
&lt;/ul>
&lt;p>What a shame. I substitute random matrices, which sacrifices some reproducibility. At least the same code would work in MATLAB.&lt;/p>
&lt;p>Another gotcha comes in line 2: if you create a vector with all integer entries, they are stored as integers. Famously, in MATLAB every number is a complex float unless you specifically declare it otherwise. Julia&amp;rsquo;s approach is standard in computer science, and there are excellent practical reasons for using it. Nor is it difficult to force Julia to use floats. Nevertheless the issue illustrates one of the subtle ways that MATLAB caters to those who are immersed in scientific computing, where pure integer results are rare.&lt;/p>
&lt;p>Next up is a simple but significant change in the syntax: square brackets &lt;code>[ ] &lt;/code>for indexing of matrices and vectors. In MATLAB parentheses &lt;code>()&lt;/code> serve both this purpose and to make function calls. Julia makes sense to me here. It makes this expression unambiguous:&lt;/p>
&lt;pre>&lt;code class="language-matlab">F( [1 2 3] )
&lt;/code>&lt;/pre>
&lt;p>In MATLAB this could be an access to the first three elements of an array F, or (the Julia meaning) a call to a function F with a vector passed as the first argument. I can&amp;rsquo;t think of a reason to have those different actions be expressed identically. I imagine the clash would complicate parsing code, but that&amp;rsquo;s an area I know nothing about.&lt;/p>
&lt;p>From here things settle down. Julia wants me to say &lt;code>1im&lt;/code> rather than &lt;code>1i&lt;/code> or &lt;code>1j&lt;/code> for the imaginary unit; fine. And I must remember to use spaces to separate columns in a matrix construction or concatenation. I often use commas for this in MATLAB. I&amp;rsquo;m a little confused because commas are used to create tuples in Julia, so I would have expected&lt;/p>
&lt;pre>&lt;code class="language-julia">x = [ 1, 2, 3 ]
&lt;/code>&lt;/pre>
&lt;p>to create (if anything) an array whose single element is the tuple 1,2,3. Instead I get a column vector with entries 1, 2, and 3, which, while a lot more useful, was a small surprise to this newbie.&lt;/p>
&lt;p>Enumerating the differences like this makes the experience sound far more frustrating than I found it to be. It&amp;rsquo;s more like driving a car with a different-feeling clutch than going from an automatic to a manual transmission.&lt;/p>
&lt;p>And I really appreciate the Jupyter notebooks! More on them versus the MATLAB Publisher and new Live Editor another time.&lt;/p></description></item><item><title>Trefethen &amp; Bau, via MATLAB and Julia</title><link>https://tobydriscoll.net/blog/trefethen-bau-via-matlab-and-julia/</link><pubDate>Thu, 01 Sep 2016 18:57:05 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-via-matlab-and-julia/</guid><description>&lt;p>This semester I&amp;rsquo;m teaching &lt;a href="https://udel.instructure.com/courses/1335769/assignments/syllabus" target="_blank" rel="noopener">MATH 612&lt;/a>, which is numerical linear and nonlinear algebra for grad students. Linear algebra dominates the course, and for that I&amp;rsquo;m following the now classic textbook by &lt;a href="http://bookstore.siam.org/ot50/" target="_blank" rel="noopener">Trefethen &amp;amp; Bau&lt;/a>. This book has real meaning to me because I learned the subject from &lt;a href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwi-qPSJl-7OAhXJ2B4KHTxqBrEQFgghMAA&amp;amp;url=https%3A%2F%2Fpeople.maths.ox.ac.uk%2Ftrefethen%2F&amp;amp;usg=AFQjCNEas_P8d7AHd2BC-vUoVo7V74ONJw" target="_blank" rel="noopener">Nick Trefethen&lt;/a> at Cornell, just a year or two before the book was written. It&amp;rsquo;s when numerical analysis became an appealing subject to me.&lt;/p>
&lt;p>That course is also when I started to learn &lt;a href="http://www.matlab.com" target="_blank" rel="noopener">MATLAB&lt;/a>. I&amp;rsquo;ve been using MATLAB for over 20 years and I&amp;rsquo;m damn good at it. I&amp;rsquo;ve written &lt;a href="http://dx.doi.org/10.1137/1.9780898717662" target="_blank" rel="noopener">a book that teaches it&lt;/a>, and &lt;a href="https://books.google.com/books/about/Schwarz_Christoffel_mapping.html?id=k5KU6clCKssC&amp;amp;hl=en" target="_blank" rel="noopener">another book&lt;/a> largely based on a&lt;a href="http://tobydriscoll.net/SC/index.html" target="_blank" rel="noopener"> software package&lt;/a> I wrote for conformal mapping, and I was an early and key contributor to the &lt;a href="http://www.chebfun.org" target="_blank" rel="noopener">Chebfun&lt;/a> project. I even dominated a game of MATLAB Jeopardy as a grad student at the &lt;a href="http://www.colostate.edu/dept/Mathematics/matlab/vol3num3" target="_blank" rel="noopener">1995 MATLAB Conference&lt;/a> (when version 4.2 of MATLAB ruled the Earth).&lt;/p>
&lt;p>(It isn&amp;rsquo;t quite contemporary, but the &lt;a href="https://web.archive.org/web/19961213182828/http://cam.cornell.edu/" target="_blank" rel="noopener">1996 home page for the Cornell Center for Applied Mathematics&lt;/a> has a banner graphic created in MATLAB&amp;mdash;by yours truly.)&lt;/p>
&lt;p>The tl;dr is that MATLAB has dominated my professional life since that course. It&amp;rsquo;s still a great tool to use for that course, too&amp;mdash;in my mind, learning the theory and learning the numerics are inextricable. In the context of computing, it&amp;rsquo;s incredible to have a 25-year winning streak!&lt;/p>
&lt;p>But while the pedagogical value remains as high as ever, MATLAB is a smaller part of the &amp;ldquo;desktop scientific computing&amp;rdquo; landscape than it was. It&amp;rsquo;s still a behemoth, but there are more good options than ever.  For some time I have felt neglectful toward options that are similar but different, namely &lt;a href="http://scipy.org/" target="_blank" rel="noopener">SciPy&lt;/a> and &lt;a href="http://julialang.org" target="_blank" rel="noopener">Julia&lt;/a>. I&amp;rsquo;ve picked up bits and pieces of them, but not enough to do any serious work.&lt;/p>
&lt;p>Thus I&amp;rsquo;ve decided to learn Julia the same way I did MATLAB: by using it as we cover elementary numerical linear algebra. The students will still get MATLAB, but I&amp;rsquo;ll be doing Julia in parallel. For each lecture (chapter) of Trefethen &amp;amp; Bau, I&amp;rsquo;ll make two &lt;a href="http://jupyter.org/" target="_blank" rel="noopener">Jupyter&lt;/a> notebooks with identical text and two versions of the codes. I&amp;rsquo;m not rewriting T&amp;amp;B, just trying to illustrate some of the concrete ideas and conclusions in each lecture. I&amp;rsquo;m sure my early Julia efforts will be cringeworthy to the cognoscenti, but just as with learning a human language, you have to risk sounding stupid for a while in order to start sounding less stupid. If I can keep up the pace, I&amp;rsquo;ll blog about what I learn about porting to Julia with each new notebook.&lt;/p></description></item><item><title>Why not Zoidberg?</title><link>https://tobydriscoll.net/blog/why-not-zoidberg/</link><pubDate>Fri, 31 Jul 2015 16:11:40 +0000</pubDate><guid>https://tobydriscoll.net/blog/why-not-zoidberg/</guid><description>&lt;p>Something fun for Friday?&lt;/p>
&lt;p>My older son binge-watched &lt;a href="http://www.cc.com/shows/futurama" target="_blank" rel="noopener">Futurama&lt;/a> on Netflix a few months ago. This was one of the funniest shows of at least recent TV history. Especially if you like nerdy, cultural-reference, rapid-fire style humor like a real &lt;a href="http://www.pewresearch.org/fact-tank/2014/06/05/generation-x-americas-neglected-middle-child/" target="_blank" rel="noopener">Gen-Xer&lt;/a>.&lt;/p>
&lt;p>It&amp;rsquo;s also probably the first and only time in television history that &lt;a href="http://theinfosphere.org/Futurama_theorem" target="_blank" rel="noopener">a new mathematical theorem was proved for and first presented in a series episode&lt;/a>. The whole run of the series had &lt;a href="http://theinfosphere.org/List_of_mathematics_references" target="_blank" rel="noopener">numerous mathematical references.&lt;/a> This may have something to do with the fact that co-creator and writer &lt;a href="https://en.wikipedia.org/wiki/Ken_Keeler" target="_blank" rel="noopener">Ken Keeler&lt;/a> has a PhD in applied math from Harvard.&lt;/p></description></item><item><title>showall</title><link>https://tobydriscoll.net/blog/showall/</link><pubDate>Thu, 30 Jul 2015 21:14:51 +0000</pubDate><guid>https://tobydriscoll.net/blog/showall/</guid><description>&lt;p>I just scratched a &lt;a href="http://matlab.com" target="_blank" rel="noopener">MATLAB&lt;/a> itch. So many times I&amp;rsquo;ve seen&amp;mdash;and experienced myself&amp;mdash;people popping open figure windows in MATLAB, then trying to juggle them and move them around the screen so that you can see all of them at once. If you know what you&amp;rsquo;re doing, you can dock them into the desktop and lay them out there, but it&amp;rsquo;s still a lot of clicks.&lt;/p>
&lt;p>So, I present &lt;a href="http://www.mathworks.com/matlabcentral/fileexchange/52344-showall" target="_blank" rel="noopener">showall.m&lt;/a>, a little function that will automatically bring forward all the figures (or the ones you specify) and lay them out in a grid. Now you can master your figures!&lt;/p></description></item><item><title>Data science? Data science!</title><link>https://tobydriscoll.net/blog/data-science-data-science/</link><pubDate>Thu, 30 Jul 2015 13:36:33 +0000</pubDate><guid>https://tobydriscoll.net/blog/data-science-data-science/</guid><description>&lt;p>I just received a copy of &lt;a href="http://sinews.siam.org/" target="_blank" rel="noopener">SIAM News&lt;/a> on a dead tree. It features &lt;a href="http://sinews.siam.org/DetailsPage/tabid/607/ArticleID/565/Data-Science.aspx" target="_blank" rel="noopener">a piece&lt;/a> by &lt;a href="https://www.cs.utah.edu/~crj/" target="_blank" rel="noopener">Chris Johnson&lt;/a> and &lt;a href="http://math.uwaterloo.ca/amath-numerical-analysis-and-scientific-computing-group/hans-de-stercks-homepage" target="_blank" rel="noopener">Hans de Sterck&lt;/a> about &amp;ldquo;Data Science: What Is It and How Is It Taught?&amp;rdquo; As usual in these articles, I find the specifics more interesting than the generalities of a panel discussion. I really liked this bit about the new program in &lt;a href="http://www.science.vt.edu/ais/cmda/" target="_blank" rel="noopener">Computational Modeling and Data Analytics at Virginia Tech&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>In a sense, creating such a program offers the opportunity to rethink curricula on classical topics like calculus that have at many institutions not seen substantial change throughout most of the past century.&lt;/p>
&lt;/blockquote>
&lt;p>This! Well outside the context of data science, too.&lt;/p>
&lt;p>I&amp;rsquo;m so sick of teaching calculus as though it were still 1960. Not that calculus has changed, of course, but what we need from it has been utterly transformed. In the age of computing, knowledge of calculus is more useful for posing the right questions&amp;mdash;as opposed to getting answers to mindless exercises that can be done in seconds on &lt;a href="http://wolframalpha.com" target="_blank" rel="noopener">Wolfram Alpha&lt;/a>. Don&amp;rsquo;t even get me started on teaching series convergence tests to engineering freshmen.&lt;/p>
&lt;p>As far as how to teach data science&amp;hellip;let me figure out how to learn it, first. I&amp;rsquo;m intrigued by &lt;a href="https://github.com/okulbilisim/awesome-datascience" target="_blank" rel="noopener">this repository&lt;/a> as a start.&lt;/p>
&lt;figure id="figure-thanks-to-kzawadzhttptwittercomkzawadz-for-the-infographic-covered-by-creative-commons-ancsa-licensehttpcreativecommonsorglicensesby-nc-sa40">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://camo.githubusercontent.com/9dca9506dbabc0ea73aedb6d2981808152ae6e90/687474703a2f2f692e696d6775722e636f6d2f57344e524964552e706e67" alt="Thanks to [@kzawadz](http://twitter.com/kzawadz) for the infographic. Covered by [Creative Commons A/NC/SA license](http://creativecommons.org/licenses/by-nc-sa/4.0/)." loading="lazy" data-zoomable width="500" />&lt;/div>
&lt;/div>&lt;figcaption>
Thanks to &lt;a href="http://twitter.com/kzawadz" target="_blank" rel="noopener">@kzawadz&lt;/a> for the infographic. Covered by &lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">Creative Commons A/NC/SA license&lt;/a>.
&lt;/figcaption>&lt;/figure></description></item><item><title>Length of papers</title><link>https://tobydriscoll.net/blog/length-of-papers/</link><pubDate>Wed, 29 Jul 2015 14:44:07 +0000</pubDate><guid>https://tobydriscoll.net/blog/length-of-papers/</guid><description>&lt;p>&lt;a href="http://people.maths.ox.ac.uk/trefethen/" target="_blank" rel="noopener">Nick Trefethen&lt;/a> has &lt;a href="http://trefethen.net/2015/06/13/journal-articles-are-getting-longer-2/comment-page-1/#comment-732" target="_blank" rel="noopener">posted&lt;/a> a wonderful graph showing how the average length of papers published in several &lt;a href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CB4QFjAAahUKEwi4ktrlyYDHAhWCHR4KHXRgBPQ&amp;amp;url=https%3A%2F%2Fwww.siam.org%2Fjournals%2F&amp;amp;ei=mua4VbjdDIK7ePTAkaAP&amp;amp;usg=AFQjCNEAU5ysKpu0RcHiazJR64ftWy1drA&amp;amp;sig2=slnnzBVvgEk_Tt5flE7Jtw&amp;amp;bvm=bv.98717601,d.dmo" target="_blank" rel="noopener">SIAM journals&lt;/a> has doubled over the last 40 years.&lt;/p></description></item><item><title>Quantum weirdness</title><link>https://tobydriscoll.net/blog/quantum-weirdness/</link><pubDate>Fri, 24 Jul 2015 01:06:45 +0000</pubDate><guid>https://tobydriscoll.net/blog/quantum-weirdness/</guid><description>&lt;p>I&amp;rsquo;m a little late getting this news, but I&amp;rsquo;m fascinated by &lt;a href="http://physicsworld.com/cws/article/news/2015/may/26/do-atoms-going-through-a-double-slit-know-if-they-are-being-observed" target="_blank" rel="noopener">an experiment at Australian National University&lt;/a> showing (once again) in a vivid way &lt;a href="http://www.physics.org/toplistdetail.asp?id=17" target="_blank" rel="noopener">how strange the quantum mechanical world is&lt;/a>.&lt;/p>
&lt;p>The experiment was a variation on the celebrated &lt;a href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=10&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CEsQtwIwCWoVChMIwtfB6cbyxgIVipQNCh14iwhY&amp;amp;url=http%3A%2F%2Fvideo.mit.edu%2Fwatch%2Fthomas-youngs-double-slit-experiment-8432%2F&amp;amp;ei=cIyxVcLfIYqpNviWosAF&amp;amp;usg=AFQjCNEnwjM9Ezlw12Hu4Ptbd88sJgWseg&amp;amp;sig2=5Yq58W4UySke647DGWG08Q&amp;amp;bvm=bv.98476267,d.eXY" target="_blank" rel="noopener">double slit experiment&lt;/a> that shows how photons are both particles &lt;em>and&lt;/em> waves, at least in some interpretations of the universe. That&amp;rsquo;s freakish in a not-news kind of way, as is the fact that the same is true of good old atoms, which might seem more as though they should stay particular all the time.&lt;/p>
&lt;p>One amusing view of the ANU experiment is that an atom can &amp;ldquo;decide&amp;rdquo; to be either particle-like or wave-like &lt;em>based on information from the future.&lt;/em> Just like &lt;a href="http://hitchhikers.wikia.com/wiki/Milliways" target="_blank" rel="noopener">the restaurant at the end of the universe&lt;/a>, that is, of course, impossible.&lt;/p>
&lt;p>If I were a better physicist I could explain this to you, but in all honesty it was just the lack of intuitiveness about high energy physics (and maybe the contemporaneous demise of the &lt;a href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CCgQFjABahUKEwi22sOdyfLGAhVM04AKHa4-CHQ&amp;amp;url=http%3A%2F%2Fwww.scientificamerican.com%2Farticle%2Fthe-supercollider-that-never-was%2F&amp;amp;ei=9o6xVbaHH8ymgwSu_aCgBw&amp;amp;usg=AFQjCNF6MtpZrD2FsH7s0Y8Hlp-t-QbspQ&amp;amp;sig2=NfTuJoyEzlpEgDq5RPY74g&amp;amp;bvm=bv.98476267,d.eXY" target="_blank" rel="noopener">Superconducting Super Collider&lt;/a>) that turned me off to the subject as an undergraduate. Still, what a universe to live in, eh Horatio?&lt;/p></description></item><item><title>A retrospective look at college math</title><link>https://tobydriscoll.net/blog/a-retrospective-look-at-college-math/</link><pubDate>Wed, 22 Jul 2015 14:28:44 +0000</pubDate><guid>https://tobydriscoll.net/blog/a-retrospective-look-at-college-math/</guid><description>&lt;p>I recommend the post &lt;a href="http://blogs.ams.org/matheducation/2015/07/20/what-i-wish-i-had-learned-more-about-in-college-mathematics/#sthash.6e9g5byf.dpuf" target="_blank" rel="noopener">What I Wish I Had Learned More About in College Mathematics&lt;/a>, written by Sabrina Schmidt, a former math undergrad at Vassar who now works as a data manager. My favorite quote:&lt;/p>
&lt;blockquote>
&lt;p>I wish that I had been introduced earlier and more often to applications, as they would have provided me with a better idea of potential areas of specialization after graduation.&lt;/p>
&lt;/blockquote>
&lt;p>She goes on to mention &lt;a href="http://projecteuclid.org/euclid.im/1109190965" target="_blank" rel="noopener">PageRank&lt;/a> (which I usually cover in my numerical computation courses) as an application of linear algebra, and e-commerce as an application of number theory. She also has other STEM courses, statistics, and computer science on her wish list for her former self.&lt;/p>
&lt;p>Good read.&lt;/p></description></item><item><title>Flipping experiences</title><link>https://tobydriscoll.net/blog/flipping-experiences/</link><pubDate>Mon, 20 Jul 2015 13:46:41 +0000</pubDate><guid>https://tobydriscoll.net/blog/flipping-experiences/</guid><description>&lt;p>In June I attended a &lt;a href="http://mathworks.com" target="_blank" rel="noopener">MathWorks&lt;/a> faculty research summit in Boston. The idea was to bring together academics and industry reps. As one of the very few non-engineers, it didn&amp;rsquo;t give me much fodder for research. But there was a parallel session for educators with a couple of crossover sessions. I &lt;a href="https://tobydriscoll.net/talk/2015-flipping-experiences/">spoke&lt;/a> in one of those about what I have learned from flipping the classroom in my numerical computation course. You can &lt;a href="https://www.icloud.com/keynote/0vEAS1_9IWdUXSTHO-98-ECzg#Flipping-classroom" target="_blank" rel="noopener">view the slides online&lt;/a>.&lt;/p>
&lt;p>The executive summary: Connecting with students in person is the main thing separating me from a &lt;a href="https://www.coursera.org/course/scicomp" target="_blank" rel="noopener">MOOC&lt;/a>. Major challenges in this particular course are the wide variety of backgrounds of the students, and material that spans advanced mathematics as well as some skill with computer coding. My goal is to teach how to bridge the two, to become fluent enough in both types of thinking to at least know when to go the experts and what to ask. Flipping lets students have time to fill in soft spots in their knowledge while absorbing new material, and to get help from me and their peers while they wrestle with putting new ideas into practice. I have no data on whether they do better in this style of class. (They believe they do a bit better, though like it a bit less.) But I know that &lt;em>I&lt;/em> am more engaged, and so I&amp;rsquo;m giving them the best that I have to offer.&lt;/p></description></item><item><title>Promotion system</title><link>https://tobydriscoll.net/blog/promotion-system/</link><pubDate>Fri, 17 Jul 2015 13:39:03 +0000</pubDate><guid>https://tobydriscoll.net/blog/promotion-system/</guid><description>&lt;p>In keeping with my post on &lt;a href="https://tobydriscoll.net/blog/grades-and-motivation/">how grades in a course affect student motivation&lt;/a>, I&amp;rsquo;ve been pondering alternatives to the classic mean-them-and-mean-it model.&lt;/p>
&lt;p>All of my family members have spent time studying karate. (I&amp;rsquo;m a brown belt, FYI, which is like an A.B.D.) One thing I&amp;rsquo;ve always liked about the dojos I&amp;rsquo;ve known is how the belt promotion system works. It&amp;rsquo;s what I would now call a &lt;a href="http://www.knewton.com/blog/ed-tech-101/edtech-101-mastery-based-learning/" target="_blank" rel="noopener">mastery based learning&lt;/a> concept. Students are tested to advance to the next belt when they are ready, regardless of time spent in the system (of course there are practical limitations on the speed of progression). The tests themselves are rigorous but the results are typically foregone conclusions, by design.&lt;/p>
&lt;p>Truly self-paced mastery learning is difficult to fit into the college grading model. With a technology assist it&amp;rsquo;s possible in topics like pre-calculus and at least some calculus, and probably a few other introductory courses I&amp;rsquo;m not familiar with. I don&amp;rsquo;t see how I could do it in my advanced course this fall.&lt;/p>
&lt;p>I could also think of a more corporate model, which is where most of the students will end up. So the first few weeks would be like an interview to determine the initial job rank (i.e., final grade). Based on performance I would give personal feedback and update their ranks accordingly throughout the term. This goes hand in hand with &lt;a href="https://tobydriscoll.net/blog/making-continuous-assessment-work/">continuous assessment&lt;/a>, which I plan to do anyway.&lt;/p>
&lt;p>Because the later material in part builds on earlier concepts, I could argue that progress later on could make up for early struggles. In any case students would be free to fight for grade promotions to the very end of the course. Unlike the karate model, demotions are possible, so they couldn&amp;rsquo;t reach an acceptable level and just lay back.&lt;/p>
&lt;p>A radical realization of this concept would include doing away with the numerical grades on each assignment! I admit, that excites me&amp;mdash;I can&amp;rsquo;t stand the arbitrariness of deciding how many &amp;ldquo;points&amp;rdquo; each mistake is worth. I see no reason why a &lt;a href="https://www.cmu.edu/teaching/designteach/teach/rubrics.html" target="_blank" rel="noopener">grading rubric&lt;/a> can&amp;rsquo;t be precise without being applied quantitively.&lt;/p>
&lt;p>This would be a huge culture shift for me and for the students. It&amp;rsquo;s risky. I&amp;rsquo;d love to hear opinions and experiences trying to do this sort of thing in math.&lt;/p></description></item><item><title>Grades and motivation</title><link>https://tobydriscoll.net/blog/grades-and-motivation/</link><pubDate>Tue, 14 Jul 2015 14:23:05 +0000</pubDate><guid>https://tobydriscoll.net/blog/grades-and-motivation/</guid><description>&lt;p>Grading is weird in so many ways. In the U.S. system, we report a &amp;ldquo;letter&amp;rdquo; grade that is basically an integer from 0 to 10 or so. This value appears on the student&amp;rsquo;s transcript without comment or context, which is an inherently meaningless way to present any data.&lt;/p>
&lt;p>But the raw value itself isn&amp;rsquo;t well defined anyway. When I give a student a C+ in calculus, does it mean that she mastered about 75% of the major topics in the course? Or does it mean that she understands about 3/4 of what is going on in any particular topic? Which of these is preferable? Would a C+ in bicycle riding be enough of a prerequisite to learn how to ride a motorcycle?&lt;/p>
&lt;p>The closest analog to grades in the real world that I can think of is the annual or quarterly performance review. There are doubts being expressed about these too. In &lt;a href="http://www.bloomberg.com/bw/articles/2013-11-07/the-annual-performance-review-worthless-corporate-ritual" target="_blank" rel="noopener">a piece on Bloomberg Business&lt;/a>, long-time management consultant Aubrey Daniels says, &amp;ldquo;It’s a sadistic process for what purpose I don’t know&amp;hellip;.Think of a sports team: A coach doesn’t wait until the end of a season to give his players feedback.&amp;rdquo; So, we&amp;rsquo;re coming back around to &lt;a href="http://tobydriscoll.net/blog/blog/2015/07/13/making-continuous-assessment-work/" target="_blank" rel="noopener">continuous assessment&lt;/a>.&lt;/p>
&lt;p>Yet the &lt;em>form&lt;/em> of the assessment needs to change too.&lt;/p>
&lt;p>What motivates people in the workplace? For one thing, being recognized for their successes. In math we tend to view perfection as the standard, and everything that falls short on homework or exams earns deductions. This is a notably dismal and discouraging viewpoint for learners. It emphasizes the negativity of errors both large and small. When you compare the (hopefully!) flawless and polished solutions on the answer key with your own stumbling attempts, how could you feel anything but foolish? Where is the upside?&lt;/p>
&lt;p>Another thing that motivates us in the real world is a chance to fix our failures. If you&amp;rsquo;ve scored badly on two midterms in a calculus course, you&amp;rsquo;re probably wise to invest your effort elsewhere. The chances of pulling yourself out of the muck are small, in part because averages are heartless and have perfect memory. I always have disdained grading methods that forgive early bad scores or give &amp;ldquo;extra&amp;rdquo; credit chances, but I have to admit that a system that makes recovery from a bad start seem impossible is no way to maintain motivation.&lt;/p>
&lt;p>I don&amp;rsquo;t have answers yet, but I&amp;rsquo;m thinking about some things. More later.&lt;/p></description></item><item><title>Bio breakfast</title><link>https://tobydriscoll.net/blog/bio-breakfast/</link><pubDate>Tue, 14 Jul 2015 12:00:03 +0000</pubDate><guid>https://tobydriscoll.net/blog/bio-breakfast/</guid><description>&lt;p>So here I am at the Delaware &lt;a href="http://www.delawarebio.org/biobreakfast-series" target="_blank" rel="noopener">Bio Breakfast&lt;/a>. Nobody is more surprised than I! Making a career out of math seems like an odd path to trying to improve human health, but here I sit.&lt;/p></description></item><item><title>Making continuous assessment work</title><link>https://tobydriscoll.net/blog/making-continuous-assessment-work/</link><pubDate>Mon, 13 Jul 2015 15:25:46 +0000</pubDate><guid>https://tobydriscoll.net/blog/making-continuous-assessment-work/</guid><description>&lt;p>I&amp;rsquo;ve come to think that in math at least, continuous learning and assessment may be more important even than [http://www.crlt.umich.edu/tstrategies/tsal](active learning). The traditional model of chunking assessments into weekly or monthly batches encourages the cram-and-dump style of &amp;ldquo;learning.&amp;rdquo; Since students are allowed to delay work on assignments that are crucial to their understanding of incoming material, it&amp;rsquo;s impossible for them to build that understanding in real time. Instead they copy and hope to parse later, when assessment is demanded.&lt;/p>
&lt;p>It&amp;rsquo;s tempting to say that students should suck it up and organize their time better. This attitude ignores human nature, especially the nature of people in late adolescence and early adulthood. Even a large part of my own work is deadline-driven rather than proactive. And &lt;em>I&lt;/em> love math!&lt;/p>
&lt;p>Any big change in expectations encounters resistance. Fortunately, breaking through that resistance sometimes spills over into breaking resistance to the tough job of learning itself.  The trick is doing so in a way that feels fair to the students and manageable to the instructor. It&amp;rsquo;s hard to overthrow everything at once.&lt;/p>
&lt;p>Here&amp;rsquo;s what I&amp;rsquo;m thinking for my fall course on numerical computing. Each class meeting (3 times a week) has a cycle associated with it:&lt;/p>
&lt;p>&lt;em>Before class:&lt;/em>&lt;/p>
&lt;ol>
&lt;li>(them) Read/watch and reflect.&lt;/li>
&lt;li>(them) Take an online quiz on the new material.&lt;/li>
&lt;/ol>
&lt;p>&lt;em>In class:&lt;/em>
3. (mostly me) Review problem spots. Fill in some of the details.
4. (us) Work to produce one graph or one table relevant to the new material.
5. (them) Turn in a description of what is still not clear.&lt;/p>
&lt;p>&lt;em>After class:&lt;/em>
6. (me) While everything is fresh, I take one last try at explaining material that is still confusing.
7. (them) Do a couple of homework problems. Before the next meeting, for full credit; before the following meeting, for partial credit.&lt;/p>
&lt;p>As you can tell, this is a lot of work for everyone, and&amp;ndash;by design&amp;ndash;it&amp;rsquo;s not flexible. To compensate, I won&amp;rsquo;t give exams. There will be some group projects for summative assessments instead.&lt;/p></description></item><item><title>Addiction</title><link>https://tobydriscoll.net/blog/addiction/</link><pubDate>Thu, 09 Jul 2015 22:17:20 +0000</pubDate><guid>https://tobydriscoll.net/blog/addiction/</guid><description>&lt;p>Last week my 14-year-old son asked rhetorically, &amp;ldquo;How is it that more people aren&amp;rsquo;t addicted to math?&amp;rdquo;&lt;/p>
&lt;p>I know son, I know.&lt;/p></description></item></channel></rss>