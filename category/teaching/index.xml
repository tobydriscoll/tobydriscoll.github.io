<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>teaching | Toby Driscoll</title><link>https://tobydriscoll.net/category/teaching/</link><atom:link href="https://tobydriscoll.net/category/teaching/index.xml" rel="self" type="application/rss+xml"/><description>teaching</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2023 by Tobin A. Driscoll</copyright><lastBuildDate>Fri, 02 Feb 2018 20:54:56 +0000</lastBuildDate><image><url>https://tobydriscoll.net/media/logo_hueac33246f157bcd904a645b7aca24b63_24178_300x300_fit_lanczos_3.png</url><title>teaching</title><link>https://tobydriscoll.net/category/teaching/</link></image><item><title>Jekyll for clicker questions</title><link>https://tobydriscoll.net/blog/jekyll-for-clicker-questions/</link><pubDate>Fri, 02 Feb 2018 20:54:56 +0000</pubDate><guid>https://tobydriscoll.net/blog/jekyll-for-clicker-questions/</guid><description>&lt;p>For a few years, I&amp;rsquo;ve been a fan of clickers (aka personal response systems) for large lecture sections. Clickers are a simple&amp;ndash;and scalable&amp;ndash;way to incorporate a little widespread active learning in the classroom. They can&amp;rsquo;t work miracles, but they do allow me to reward attendance, rouse the students once in a while, and give good feedback to all of us about how well the latest concepts are sinking in. I like the accountability: If you got the question wrong when 80% of the class got it right, that&amp;rsquo;s on you, but if 20% of the class got it right, that&amp;rsquo;s on me.&lt;/p>
&lt;p>UD is an &lt;a href="http://www.iclicker.com" target="_blank" rel="noopener">iclicker&lt;/a> shop. When I want to poll the class, I click a &amp;ldquo;go&amp;rdquo; button on a small toolbar that overlays any other application. When I&amp;rsquo;m done, I click &amp;ldquo;stop.&amp;rdquo; I can show the results and designate the correct answer on the spot, or I can go back later and pick the right answer while looking at a screenshot from when the question started.&lt;/p>
&lt;p>In the past I&amp;rsquo;ve used clickers with handwritten questions projected using a document camera. I don&amp;rsquo;t get the screenshot this way, but it works fine. However, in the best case I&amp;rsquo;m left to manage 50-100 sheets of paper for a course. That&amp;rsquo;s something I&amp;rsquo;m increasingly cranky about doing in my life overall, and I&amp;rsquo;m likely to fail at it during the heat of a lecture, especially when (as I like to do) I start replaying questions from past weeks or months. Plus, if I later decide to tweak a question or the answer choices, I&amp;rsquo;ve got to scrap a page and rewrite it.&lt;/p>
&lt;p>Enter &lt;a href="http://jekyllrb.com" target="_blank" rel="noopener">Jekyll&lt;/a>. This is a brilliant software t0ol that converts lightly marked data files into a website. It&amp;rsquo;s blog-centric, but it can be used for other kinds of data as well, and I&amp;rsquo;ve customized it for collecting clicker questions. You can get it for yourself from &lt;a href="https://github.com/tobydriscoll/clicker-quiz" target="_blank" rel="noopener">this Github repo&lt;/a>. It requires being comfortable with a command line, but it&amp;rsquo;s not otherwise technically challenging.&lt;/p>
&lt;p>For instance, in one file I have&lt;/p>
&lt;pre>---
layout: question
chapter: Introduction
title: Derivative
---
{::comment}
The \dd macro is defined in /_includes/texmacros.md.
{:/comment}
What is $\dd{}{x}\left(e^x\right)$?
1. *$e^x$*{: #correct}
1. $x$
1. $1$
1. $\ln(x)$
1. $\tan(x)$&lt;/pre>
&lt;p>The page that results can be viewed &lt;a href="https://tobydriscoll.github.io/clicker-quiz/questions/introduction/q02.html" target="_blank" rel="noopener">here&lt;/a>.  It&amp;rsquo;s pretty easy to see how the output arises from the input. All I do is make one file per question, putting them into subdirectories if I want. They&amp;rsquo;re collected and indexed by the &amp;ldquo;chapter&amp;rdquo; property at the top of the file.&lt;/p>
&lt;p>Having maintained more than one &lt;a href="http://tobydriscoll.net" target="_blank" rel="noopener">website&lt;/a> of HTML files by hand, I found Jekyll to be a revelation. Headers and footers can be included automatically on all pages set to a certain style. (I use this to define MathJax macros in one file that get copied into all the output question pages.)  Content, such as an index or table of contents, can be generated programmatically based on properties of the data. There&amp;rsquo;s a nice &lt;a href="https://www.chronicle.com/blogs/profhacker/jekyll1" target="_blank" rel="noopener">step-by-step series for getting started with Jekyll&lt;/a> on the ProfHacker blog.&lt;/p>
&lt;p>Jekyll versus raw HTML is like using a power drill/driver versus the Craftsman screwdriver with the hard plastic handle that digs divots into your palm when there&amp;rsquo;s a job of any decent size. I&amp;rsquo;ll probably move my personal site this blog over to Jekyll at some point.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lectures 24-29: Eigenvalue stuff</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-24-29-eigenvalue-stuff/</link><pubDate>Thu, 27 Oct 2016 14:16:39 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-24-29-eigenvalue-stuff/</guid><description>&lt;p>Part V of T&amp;amp;B is on dense methods for eigenvalue and singular value problems. For my course, this is the part of the text that I condense most severely. In part that&amp;rsquo;s due to the need to cover unconstrained nonlinear solving and optimization stuff later on. But I also find that this is the least compelling part of the text for my purposes.&lt;/p>
&lt;p>It&amp;rsquo;s heavily weighted toward the hermitian case. That&amp;rsquo;s the cleanest situation, so I see the rationale. But it&amp;rsquo;s pretty surprising that the lead author of &lt;a href="http://press.princeton.edu/titles/8113.html" target="_blank" rel="noopener">&lt;em>Spectra and Pseudospectra&lt;/em>&lt;/a> mentions eigenvalue conditioning and sensitivity only in a single exercise! (The exercises not in the lecture named &amp;ldquo;Eigenvalue problems,&amp;rdquo; nor the one named &amp;ldquo;Overview of eigenvalue algorithms.&amp;rdquo; It&amp;rsquo;s under &amp;ldquo;Reduction to Hessenberg or tridiagonal form.&amp;rdquo;) In contrast with the tone of earlier parts of the book, one could study the methods of these sections thoroughly and yet not appreciate when the answers are inaccurate, or possibly irrelevant. Because I took this course from Trefethen at a crucial time in the development of his thinking on the subject, my perception of the issues behind computing eigenvalues is quite different from what the text itself conveys.&lt;/p>
&lt;p>(EDIT: If I had but read a few sections more before writing the above, I would have recalled that there is discussion about this in Lecture 34, under &amp;ldquo;A Note of Caution: Nonnormality.&amp;rdquo; It&amp;rsquo;s all laid out in clear language, so mea culpa. The ordering still feels a little awkward. I&amp;rsquo;ll probably have a half or full class period just on nonnormality.)&lt;/p>
&lt;p>So. In my class I touched on 24-29, and you can find my related &lt;a href="https://www.dropbox.com/sh/kxyc1on3k4f3sh0/AACnyHY2FmXgUpHmJvSYV6Qaa?dl=0" target="_blank" rel="noopener">MATLAB notebooks&lt;/a> and &lt;a href="https://www.dropbox.com/sh/gq3a0nr1gm4p87a/AABlOcb33OAjO40PFG6tkYSva?dl=0" target="_blank" rel="noopener">Julia notebooks&lt;/a> on them. (I&amp;rsquo;ve given up on using Gists for these. The web interface can&amp;rsquo;t seem to handle having a lot of notebooks in one Gist, the rendering is slow, and I see no advantage for me beyond static HTML.) They&amp;rsquo;re a little rough in places, as it&amp;rsquo;s been challenging to keep up the pace.&lt;/p>
&lt;p>There aren&amp;rsquo;t big MATLAB/Julia issues to report. If anything, I think Julia has cleaned up and rationalized some of the quirkiness of the MATLAB versions. In MATLAB, one uses &lt;code>eig&lt;/code> for everything. The results depend on the number of output arguments.&lt;/p>
&lt;pre>&lt;code class="language-matlab">&amp;gt;&amp;gt; A = hilb(3);
&amp;gt;&amp;gt; lambda = eig(A)
lambda =
0.0027
0.1223
1.4083
&amp;gt;&amp;gt; [X,D] = eig(A)
X =
-0.1277 0.5474 0.8270
0.7137 -0.5283 0.4599
-0.6887 -0.6490 0.3233
D =
0.0027 0 0
0 0.1223 0
0 0 1.4083
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s a bit awkward that the position of the eigenvalue output changes, and that it&amp;rsquo;s a vector in one case and a matrix in the other. And the difference goes beyond cosmetics: the calculation can be significantly faster if eigenvectors are not required. Julia gives you three variants, so you can retrieve exactly what you want.&lt;/p>
&lt;pre>&lt;code class="language-julia">julia&amp;gt; A = [1/(i+j) for i=1:3, j=1:3];
julia&amp;gt; (λ,X) = eig(A)
([0.000646659,0.0409049,0.875115],
[0.19925 -0.638787 -0.743136; ... -0.411255])
julia&amp;gt; λ = eigvals(A)
3-element Array{Float64,1}:
0.000646659
0.0409049
0.875115
julia&amp;gt; D = eigvecs(A)
3×3 Array{Float64,2}:
0.19925 -0.638787 -0.743136
-0.761278 0.376612 -0.527843
0.617053 0.670906 -0.411255
&lt;/code>&lt;/pre>
&lt;p>You even have &lt;code>eigmax&lt;/code> and &lt;code>eigmin&lt;/code> when the spectrum is real. One thing neither language gives you is an easy way to specify a sort order for the results. In MATLAB, for instance, one ends up doing things like:&lt;/p>
&lt;pre>&lt;code class="language-matlab">&amp;gt;&amp;gt; [X,D] = eig(A);
&amp;gt;&amp;gt; lambda = diag(D);
&amp;gt;&amp;gt; [~,idx] = sort(real(lambda));
&amp;gt;&amp;gt; X = X(:,idx); lambda = lambda(idx)
lambda =
-2.1898 + 1.4354i
-2.1898 - 1.4354i
0.0301 + 0.6095i
0.0301 - 0.6095i
1.2276 + 2.2020i
1.2276 - 2.2020i
1.8278 + 0.0000i
&lt;/code>&lt;/pre>
&lt;p>Meh. It&amp;rsquo;s not a lot better in Julia, as far as I can tell.&lt;/p>
&lt;pre>&lt;code class="language-julia">julia&amp;gt; A = randn(7,7);
julia&amp;gt; (λ,X) = eig(A);
julia&amp;gt; idx = sortperm(real(λ));
julia&amp;gt; X = X[:,idx]; λ = λ[idx]
7-element Array{Complex{Float64},1}:
-3.38359+0.0im
-2.33084+0.233909im
-2.33084-0.233909im
0.415007+0.0im
1.03098+0.0im
1.11426+2.34596im
1.11426-2.34596im
&lt;/code>&lt;/pre>
&lt;p>Altogether, Julia is feeling less like a foreign country and more like a province. Sometimes I even remember to use square brackets on the first try.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia: Lectures 20, 21, 23: Solving square systems</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-20-21-23-solving-square-systems/</link><pubDate>Sat, 22 Oct 2016 16:20:15 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-20-21-23-solving-square-systems/</guid><description>&lt;p>Three in one this time: &lt;a href="https://gist.github.com/tobydriscoll/e2f44e7756b864050b52c6773aa83b70" target="_blank" rel="noopener">Lecture 20&lt;/a>, which is on Gaussian elimination / LU factorization, &lt;a href="https://gist.github.com/tobydriscoll/2b74da569406da55c599a61afe99cf56" target="_blank" rel="noopener">Lecture 21&lt;/a>, on row pivoting, and &lt;a href="https://gist.github.com/tobydriscoll/17ce89eb2c827e0b870877219ec64fe8" target="_blank" rel="noopener">Lecture 23&lt;/a>, on Cholesky factorization. I mainly skipped over Lecture 22, about the curious case of the stability of pivoted LU, but the main example is dropped into the end of my coverage of pivoting.&lt;/p>
&lt;p>The Julia surprises are, not surprisingly, coming less frequently. In Lecture 20 I had some fun with rational representations. I like using MATLAB&amp;rsquo;s &lt;code>format rat&lt;/code> when presenting Gaussian elimination, as it allows me to recall the way the process looks when learned by hand. It&amp;rsquo;s a fun trick, but of course the underlying values are still all double precision, and the rational approximations to them are found ex post facto. By contrast, Julia offers true rational numbers, constructed and shown using the &lt;code>//&lt;/code> operation.&lt;/p>
&lt;p>Compare the MATLAB&lt;/p>
&lt;pre>&lt;code class="language-matlab">format rat
I = eye(4);
L21 = I + (-5/17)*I(:,2)*I(:,1)';
L31 = I + (-9/17)*I(:,3)*I(:,1)';
L41 = I + (-4/17)*I(:,4)*I(:,1)';
&lt;/code>&lt;/pre>
&lt;p>to the Julia&lt;/p>
&lt;pre>&lt;code class="language-julia">I = eye(Rational,4);
L21 = copy(I); L21[2,1] = -5//17;
L31 = copy(I); L31[3,1] = -9//17;
L41 = copy(I); L41[4,1] = -4//17;
&lt;/code>&lt;/pre>
&lt;p>The MATLAB code requires only the &lt;code>format&lt;/code> call, because it&amp;rsquo;s only the display of results that is affected. The Julia code is doing something deeper and needs more changes as a result.&lt;/p>
&lt;p>Julia could use something like a &lt;code>format&lt;/code> command. I almost always find MATLAB&amp;rsquo;s terminal output more readable, or at least easier to manipulate into a good form. Here&amp;rsquo;s one example using the rational output. First, MATLAB:&lt;/p>
&lt;pre>&lt;code class="language-matlab">17 2 3 13 93
0 194/17 155/17 71/17 963/17
0 101/17 92/17 87/17 574/17
0 230/17 243/17 -18/17 1158/17
&lt;/code>&lt;/pre>
&lt;p>And the Julia:&lt;/p>
&lt;pre>&lt;code class="language-julia">4x5 Array{Rational{T&amp;amp;lt;:Integer},2}:
17//1 2//1 3//1 13//1 93//2
0//1 194//17 155//17 71//17 963//34
0//1 101//17 92//17 87//17 287//17
0//1 230//17 243//17 -18//17 579//17
&lt;/code>&lt;/pre>
&lt;p>I almost never need that header line that Julia gives. The numbers are already showing themselves to be Rational, and the shape of the array is self-evident. (Though I now see that MATLAB 2016b is adding such headers to non-float output.) The zero structure also jumps out more clearly in the MATLAB case, though it&amp;rsquo;s profligate with whitespace.&lt;/p>
&lt;p>Another comparison, of MATLAB (using the default format):&lt;/p>
&lt;pre>&lt;code class="language-matlab">1 0 0 0 0 1
0 1 0 0 0 2
0 0 1 0 0 4
0 0 0 1 0 8
0 0 0 0 1 16
0 0 0 0 0 32
&lt;/code>&lt;/pre>
&lt;p>versus Julia:&lt;/p>
&lt;pre>&lt;code class="language-julia">6×6 Array{Float64,2}:
1.0 0.0 0.0 0.0 0.0 1.0
0.0 1.0 0.0 0.0 0.0 2.0
0.0 0.0 1.0 0.0 0.0 4.0
0.0 0.0 0.0 1.0 0.0 8.0
0.0 0.0 0.0 0.0 1.0 16.0
0.0 0.0 0.0 0.0 0.0 32.0
&lt;/code>&lt;/pre>
&lt;p>There&amp;rsquo;s nothing wrong per se about Julia&amp;rsquo;s. But which version would you write down, or expect to see in print? One last case, of a matrix that is supposed to be triangular but for a little roundoff. First, Julia:&lt;/p>
&lt;pre>&lt;code class="language-julia">4×4 Array{Float64,2}:
17.0 2.0 3.0 13.0
0.0 13.5294 14.2941 -1.05882
0.0 0.0 -2.93913 5.06957
5.55112e-16 0.0 -4.44089e-16 4.09024
&lt;/code>&lt;/pre>
&lt;p>And MATLAB, using the default format:&lt;/p>
&lt;pre>&lt;code class="language-matlab">17.0000 2.0000 3.0000 13.0000
0 13.5294 14.2941 -1.0588
0 0 -2.9391 5.0696
0.0000 0 -0.0000 4.0902
&lt;/code>&lt;/pre>
&lt;p>Julia has chosen to align on the decimal point. It&amp;rsquo;s also suppressing trailing zeros, except for the first, giving an odd and false impression of values that have a precise number of significant digits. MATLAB&amp;rsquo;s choice of right alignment is visually superior, and only exact zero gets a special display. True, you might want that exponential notation for the tiny values; you can get it by changing the format.&lt;/p>
&lt;pre>&lt;code class="language-matlab">&amp;gt;&amp;gt; format short e
&amp;gt;&amp;gt; U
U =
1.7000e+01 2.0000e+00 3.0000e+00 1.3000e+01
0 1.3529e+01 1.4294e+01 -1.0588e+00
0 0 -2.9391e+00 5.0696e+00
5.5511e-16 0 -4.4409e-16 4.0902e+00
&amp;gt;&amp;gt; format short g
&amp;gt;&amp;gt; U
U =
17 2 3 13
0 13.529 14.294 -1.0588
0 0 -2.9391 5.0696
5.5511e-16 0 -4.4409e-16 4.0902
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s nice to have options.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia: Lecture 19, Stability of least squares</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-19-stability-of-least-squares/</link><pubDate>Tue, 11 Oct 2016 21:16:35 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-19-stability-of-least-squares/</guid><description>&lt;p>Here are &lt;a href="https://gist.github.com/tobydriscoll/dfb794e2c6891944790e628f68058ba4" target="_blank" rel="noopener">the notebooks&lt;/a> in MATLAB and Julia.&lt;/p>
&lt;p>The new wrinkle in these codes is extended precision. In MATLAB you need to have the Symbolic Math toolbox to do this in the form of &lt;code>vpa&lt;/code>. In Julia, you have to use version 0.5 or (presumably) later, which had a surprising side effect I&amp;rsquo;ll get to below.&lt;/p>
&lt;p>The reason for extended precision is that this lecture presents experiments on the accuracy of different algorithms for linear least squares problems. In order to demonstrate this on a fairly ill conditioned problem, the answer is supposed to be computed in extended precision, yielding a normalization constant that sets the desired quantity to be 1 for at least 16 significant digits.&lt;/p>
&lt;p>The least squares problem comes from fitting exp(sin(4t)) to a polynomial of degree 14. I see two ways to define how extended precision is to be used. Option (1) is to form the matrix $A$ and the vector $b$ in double precision, then solve the least squares problem with them, but in extended precision. Option (2) is to build in extended precision from the beginning of the problem, creating $A$ and $b$ that differ in the extended digits. I was first attracted to option (1), but option (2) has the clear advantage that the result should be independent of machine and language, whereas in the other case the data could be rounded or computed differently to double precision.&lt;/p>
&lt;p>Here&amp;rsquo;s how this looks in MATLAB.&lt;/p>
&lt;pre>&lt;code class="language-matlab">t = vpa(0:m-1,64)'/vpa(m-1,64); % 64 sig. digits!
A = t.^0;
for j = 1:14, A=[A,t.*A(:,j)]; end
b = exp(sin(4*t));
[Q,R] = qr(A,0); % Householder QR
x1 = R\ (Q'*b);
[Q,R] = mgs([A b]); % Gram-Schmidt QR
x2 = R(1:15,1:15) \ R(1:15,16);
&lt;/code>&lt;/pre>
&lt;p>Here are the outputs for the last element of x in the four methods:&lt;/p>
&lt;pre>&lt;code>2006.7874531048518338761038143559
2006.7874531048518338761038143553
2006.7874531048518338766907539159
2006.7874531048518338761038143555
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s not a problem that the third result disagrees in the last 10 or so digits, since that&amp;rsquo;s an unstable method.&lt;/p>
&lt;p>Here&amp;rsquo;s how it went in Julia.&lt;/p>
&lt;pre>&lt;code class="language-julia">setprecision(BigFloat,128); # use 128-bit floats
t = convert(Array{BigFloat},collect(0:m-1))/convert(BigFloat,m-1);
A = [t[i].^j for i=1:m, j=0:n-1];
b = exp(sin(4*t));
(Q,R) = qr(A);
x1 = R\ (Q'*b);
(Q,R) = mgs([A b]);
x2 = R[1:15,1:15] \ R[1:15,16];
x3 = (A'*A)$$A'*b);
x4 = A\b;
&lt;/code>&lt;/pre>
&lt;p>That first line isn&amp;rsquo;t pretty, but after that it&amp;rsquo;s quite natural. I found Juila&amp;rsquo;s extended precision to be fast compared to MATLAB&amp;rsquo;s. The results:&lt;/p>
&lt;pre>&lt;code>2.006787453104851833876103814338068195207e+03
2.006787453104851833876103814355358077263e+03
2.006787453104851834342923924263804001505e+03
2.006787453104851833876103814376793404332e+03
&lt;/code>&lt;/pre>
&lt;p>These are the same up to the last couple of digits of MATLAB&amp;rsquo;s answer. Unfortunately, my values don&amp;rsquo;t agree with what&amp;rsquo;s in T&amp;amp;B, which is 2006.787453080206. The text doesn&amp;rsquo;t say much about how this was done, so it&amp;rsquo;s impossible for me to say why.&lt;/p>
&lt;p>I probably don&amp;rsquo;t pay enough attention to extended precision. I know some people in the radial basis function community who use it to overcome the very poor conditioning of those bases. They seem quite happy with it. It&amp;rsquo;s always felt like cheating to me, but that&amp;rsquo;s hardly a rational argument.&lt;/p>
&lt;p>Above I said that there was an unexpected side effect related to my using extended precision in Julia. I discovered that (a) it became available in base Julia in version 0.5 and (b) the homebrew Julia I had installed was version 0.4.3, even though 0.5 had apparently been out for a while. Upon upgrading, I found that my MGS routine throwing an error! The offending line was&lt;/p>
&lt;pre>&lt;code class="language-julia">A[:,j+1:n] -= Q[:,j]*R[j,j+1:n];
&lt;/code>&lt;/pre>
&lt;p>The issue is that now both of the references on the right-hand side are vectors, which have only one dimension. Therefore the implied outer product is considered undefined. I had to switch to&lt;/p>
&lt;pre>&lt;code class="language-julia">A[:,j+1:n] -= Q[:,j:j]*R[j:j,j+1:n];
&lt;/code>&lt;/pre>
&lt;p>Because &lt;code>j:j&lt;/code> is a range, not a scalar, the submatrix references are two-dimensional matrices with appropriate singleton dimensions, so the outer product proceeds.&lt;/p>
&lt;p>I&amp;rsquo;m not sure how to feel about this. It&amp;rsquo;s disturbing to extract a row of a matrix and get an object without a row shape. In fact you can even say it&amp;rsquo;s got a column shape, because you are allowed to transpose it into a 1-by-n matrix! On the other hand, there are consistent rules governing the indexing, and 0D, 1D, and 2D extractions are all possible. I&amp;rsquo;m starting to think that the true problem is that I learned and conceptualize linear algebra in a way that works up to dimension 2 but contains some implied hacks that break multilinear algebra. I wish I knew this stuff better.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lectures 12-13: Conditioning and floating point</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-12-13-conditioning-and-floating-point/</link><pubDate>Tue, 04 Oct 2016 17:57:57 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-12-13-conditioning-and-floating-point/</guid><description>&lt;p>I&amp;rsquo;ve run into trouble managing gists with lots of files in them, so I&amp;rsquo;m back to doing one per lecture. Here are &lt;a href="https://gist.github.com/tobydriscoll/df92e76d369617e2f5e56cf2fdab8117" target="_blank" rel="noopener">Lecture 12&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/7449b8d30704663835214af91d2b4d90" target="_blank" rel="noopener">Lecture 13&lt;/a>.&lt;/p>
&lt;p>We&amp;rsquo;ve entered Part 3 of the book, which is on conditioning and stability matters. The lectures in this part are heavily theoretical and often abstract, so I find a little occasional computer time helps to clear the cobwebs.&lt;/p>
&lt;p>Right off the top, in reproducing Figure 12.1, I ran right into the trap I worried about in &lt;a href="https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-11-least-squares/">my last post&lt;/a> regarding polynomials in Julia. In MATLAB, the polynomial coefficients are just a plain vector. That makes perturbing them trivial:&lt;/p>
&lt;pre>&lt;code class="language-matlab">p = poly([1,1,1,0.4,2.2]); % polynomial with these roots
q = p + 1e-9*randn(size(p)); % perturb its coefficients
&lt;/code>&lt;/pre>
&lt;p>In Julia, you can use the &lt;code>Polynomials&lt;/code> package and get polynomial objects. Behold:&lt;/p>
&lt;pre>&lt;code class="language-julia">using Polynomials
p = poly([1,1,1,0.4,2.2]); # polynomial with these roots
q = p + Poly(1e-9*randn(6)); # perturb coefficients
&lt;/code>&lt;/pre>
&lt;p>Note that &lt;code>poly&lt;/code> constructs a polynomial from a vector of &lt;em>roots&lt;/em>, while &lt;code>Poly&lt;/code> constructs one from a vector of &lt;em>coefficients&lt;/em>. Sure enough, I used &lt;code>poly&lt;/code> in both lines the first time around. It&amp;rsquo;s a pernicious mistake, because it produces no error&amp;mdash;the polynomials can be added no matter what. The mistake was mine, but I think this is an unfortunate design.&lt;/p>
&lt;p>The only other notable usage in Lecture 12 is my first use of a &lt;a href="http://docs.julialang.org/en/release-0.5/manual/arrays/?highlight=matrix%20comprehension#comprehensions" target="_blank" rel="noopener">comprehension&lt;/a>:&lt;/p>
&lt;pre>&lt;code class="language-julia">hilb(n) = [ 1.0/(i+j) for i=1:n, j=1:n ];
&lt;/code>&lt;/pre>
&lt;p>This is a pretty handy way to create a matrix.&lt;/p>
&lt;p>In Lecture 13 I had some fun dissecting floating point numbers in both systems. There was only one area in which Julia didn&amp;rsquo;t go as smoothly as I would hope. MATLAB offers &lt;code>realmin&lt;/code> and &lt;code>realmax&lt;/code> , which give the smallest and largest normalized floating point numbers. While Julia has similar-sounding commands, they are interpreted differently:&lt;/p>
&lt;pre>&lt;code class="language-julia">julia&amp;gt; typemin(Float64), typemax(Float64)
(-Inf,Inf)
&lt;/code>&lt;/pre>
&lt;p>Eh, not so much. There is even one more layer of subtlety. Consider&lt;/p>
&lt;pre>&lt;code class="language-julia">julia&amp;gt; (prevfloat(Inf),nextfloat(0.0))
(1.7976931348623157e308,5.0e-324)
&lt;/code>&lt;/pre>
&lt;p>The first of these values is exactly the same as &lt;code>realmax&lt;/code>, but the second is not &lt;code>realmin&lt;/code>. &lt;a href="https://en.wikipedia.org/wiki/IEEE_floating_point" target="_blank" rel="noopener">IEEE 754 double precision&lt;/a> has &amp;ldquo;denormalized&amp;rdquo; numbers that let you trade away bits of precision to get closer to zero in magnitude. Julia is reporting the smallest denormalized number, not the smallest full-precision number. Julia&amp;rsquo;s not wrong, but access to the extreme finite double precision values isn&amp;rsquo;t as straightforward as it could be.&lt;/p>
&lt;p>One last observation. Trefethen &amp;amp; Bau refer to the value $2^{-53}$ as &amp;ldquo;machine epsilon.&amp;rdquo; This isn&amp;rsquo;t what MATLAB and Julia use, which is $2^{-52}$. Nick Higham&amp;rsquo;s &lt;em>Accuracy and Stability of Numerical Algorithms&lt;/em> also has &amp;ldquo;machine epsilon&amp;rdquo; at $2^{-52}$ and calls $2^{-53}$ &amp;ldquo;unit roundoff.&amp;rdquo; Stoer and Bulirsch (2nd ed.) call $2^{-53}$ &amp;ldquo;machine precision.&amp;rdquo; Corless and Fillion seem to agree with Higham. Golub and Van Loan (3rd ed.) don&amp;rsquo;t use &amp;ldquo;machine epsilon&amp;rdquo; at all, and in the index one finds&lt;/p>
&lt;blockquote>
&lt;p>Machine precision. &lt;em>See&lt;/em> unit roundoff.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>Sigh.&lt;/em> The mathematical uses are, unsurprisingly, consistent. Frankly, I feel better about my personal inconsistencies at using those terms: at least I stood on the shoulders of giants.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 11: Least squares</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-11-least-squares/</link><pubDate>Wed, 28 Sep 2016 13:19:16 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-11-least-squares/</guid><description>&lt;p>This week&amp;rsquo;s notebooks (&lt;a href="https://gist.github.com/tobydriscoll/8d87997704e9fd400e96ea860d9f6a34#file-tb-lecture-11-ipynb" target="_blank" rel="noopener">MATLAB&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/3d9b29d953882738c51c9cabdcaf431b#file-tb-lecture-11-ipynb" target="_blank" rel="noopener">Julia&lt;/a>&amp;ndash;now all lectures are together for each language) are about least squares polynomial fitting.&lt;/p>
&lt;p>The computational parts are almost identical, except for how polynomials are represented. In MATLAB, a vector of coefficients is interpreted as a polynomial in the context of particular functions, such as &lt;code>polyval&lt;/code>. The major pain is that the convention is for the coefficients to be ordered from high degree to low, which is almost always the opposite of what you really want. Hence I&amp;rsquo;ve gotten used to writing code like&lt;/p>
&lt;pre>&lt;code>p = @(x) polyval( c(end: -1:1), x-1955 );
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s not a big deal, but it trips up some students every semester.&lt;/p>
&lt;p>Julia has a full-fledged polynomial type, if you care to add and load the package. And, it expects ordering from the constant term to the highest degree. So I came up with&lt;/p>
&lt;pre>&lt;code class="language-julia">p = Poly(c);
q = t -&amp;gt; p(t-1955);
&lt;/code>&lt;/pre>
&lt;p>Simple enough, but I find two disappointments. First, it&amp;rsquo;s a bare-bones class. For instance, the second object &lt;code>q&lt;/code> above is also a polynomial, but we&amp;rsquo;ll never know it formally, or be able to get its coefficients. A &lt;code>shiftvar&lt;/code> method or something similar would be nice. Second, in the effort to clone the MATLAB interface, a potential for serious confusion was introduced. The command &lt;code>p=poly(c)&lt;/code>
also works, but (like MATLAB&amp;rsquo;s counterpart) constructs a polynomial whose roots, not coefficients, are given. This is &lt;em>way&lt;/em> too easy a mistake to make.&lt;/p>
&lt;p>Another element this time was that I tried using the nascent &lt;a href="https://juliaplots.github.io/" target="_blank" rel="noopener">Plots package&lt;/a> for Julia. It&amp;rsquo;s an interesting attempt to graft a graceful interface onto the various graphics backends that already exist. I was motivated to try it because AFAIK, the &lt;code>PyPlots&lt;/code> package lacks a counterpart to &lt;code>fplot&lt;/code> from MATLAB. Perhaps in part because of my time with the &lt;a href="http://www.chebfun.org" target="_blank" rel="noopener">Chebfun project&lt;/a>, I have been putting more emphasis in my teaching on representing functions as such, rather than implicitly as vectors of numbers. It bothers me now, for example, that functions such as &lt;code>interp1&lt;/code> and &lt;code>ode45&lt;/code> return numbers or structures rather than callable functions, which is what their algorithms should be doing in the deep sense.&lt;/p>
&lt;p>Anyhow, I end up using &lt;code>fplot&lt;/code> a lot because of my emphasis on functions, and couldn&amp;rsquo;t find a counterpart in &lt;code>PyPlot&lt;/code>. In &lt;code>Plots&lt;/code>, however, the &lt;code>plot&lt;/code> command handles both numerical and functional arguments alike. Here&amp;rsquo;s a snippet from the notebook:&lt;/p>
&lt;pre>&lt;code class="language-julia">p = Poly(c);
plot( t-&amp;gt;p(t-1955), 1955,2000 )
plot!( year,anomaly, m=:o,l=nothing );
title!(&amp;quot;World temperature anomaly&amp;quot;);
xlabel!(&amp;quot;year&amp;quot;); ylabel!(&amp;quot;anomaly (deg C)&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Not bad! You can see a couple of quirks though. One is the use of &lt;a href="http://docs.julialang.org/en/release-0.5/manual/functions/#keyword-arguments" target="_blank" rel="noopener">keyword arguments&lt;/a> in line 3; the arguments &lt;code>m=:o&lt;/code> and &lt;code>l=nothing&lt;/code> respectively set the point markers to circles and the lines connecting points to be suppressed. This takes getting used to, but it&amp;rsquo;s memorable and compact enough.&lt;/p>
&lt;p>The other quirk that you see above is the use of the banged commands like &lt;code>plot!&lt;/code> and &lt;code>title!&lt;/code>. The bang in Julia is a convention meaning &amp;ldquo;operate in place&amp;rdquo; or &amp;ldquo;overwrite existing.&amp;rdquo; By default, the MATLAB-like commands replace the existing plot, so they have to be banged in order to build on top of it instead. This is a bit dubious in the case of titles and labels––why would I create a new plot by issuing a title?––but it is at least consistent, and, unlike the global state used in MATLAB by the &lt;code>hold&lt;/code> command, works the same regardless of context and history.&lt;/p>
&lt;p>One quirk––to me, a bug––that you don&amp;rsquo;t see is that the default in &lt;code>Plots&lt;/code> is that every plot creates or adds to a legend. I&amp;rsquo;m not a big fan of plot legends in most contexts, but you&amp;rsquo;re welcome to them if you like them. However, I don&amp;rsquo;t find it reasonable to have one forced on me for a graph with a single curve that I didn&amp;rsquo;t give a label to! I turned off this travesty by starting off with&lt;/p>
&lt;pre>&lt;code class="language-julia">using Plots; pyplot(legend=false);
&lt;/code>&lt;/pre>
&lt;p>which at least is straightforward, though entangled with my choice of backend.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 8: Gram-Schmidt</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-8-gram-schmidt/</link><pubDate>Mon, 19 Sep 2016 19:44:42 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-8-gram-schmidt/</guid><description>&lt;p>This lecture is about the modified Gram-Schmidt method and flop counting. The &lt;a href="https://gist.github.com/tobydriscoll/bae2a5e864f490e571d79a0af541fb8c" target="_blank" rel="noopener">notebooks are here&lt;/a>.&lt;/p>
&lt;p>I&amp;rsquo;m lost.&lt;/p>
&lt;p>Almost as an afterthought I decided to add a demonstration of the timing of Gram-Schmidt compared to the asymptotic &lt;span class='MathJax_Preview'>&lt;img src='https://i2.wp.com/tobydriscoll.net/blog/wp-content/plugins/latex/cache/tex_9f84a66d88d24c3b1bc91df5b5346a13.gif?w=500' style='vertical-align: middle; border: none; ' class='tex' alt="O(n^2)" data-recalc-dims="1" /> flop count. Both MATLAB and Julia got very close to the trend as &lt;span class='MathJax_Preview'>&lt;img src='https://i0.wp.com/tobydriscoll.net/blog/wp-content/plugins/latex/cache/tex_7b8b965ad4bca0e41ab51de7b31363a1.gif?w=500' style='vertical-align: middle; border: none; padding-bottom:2px;' class='tex' alt="n" data-recalc-dims="1" /> got into the hundreds, using vectorized code:&lt;/p>
&lt;pre>&lt;code class="language-julia">n_ = collect(50:50:500);
time_ = zeros(size(n_));
for k = 1:length(n_)
n = n_[k];
A = rand(1200,n);
Q = zeros(1200,n); R = zeros(600,600);
tic();
R[1,1] = norm(A[:,1]);
Q[:,1] = A[:,1]/R[1,1];
for j = 2:n
R[1:j-1,j] = Q[:,1:j-1]'*A[:,j];
v = A[:,j] - Q[:,1:j-1]*R[1:j-1,j];
R[j,j] = norm(v);
Q[:,j] = v/R[j,j];
end
time_[k] = toc();
end
using PyPlot
loglog(n_,time_,&amp;quot;-o&amp;quot;,n_,(n_/500).^2,&amp;quot;--&amp;quot;)
xlabel(&amp;quot;n&amp;quot;), ylabel(&amp;quot;elapsed time&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>I noticed that while the timings were similar, Julia lagged MATLAB just a bit. I decided this would be a great chance for me to see Julia&amp;rsquo;s prowess with speedy loops firsthand.&lt;/p>
&lt;p>Compare the vectorized and unvectorized Julia versions here:&lt;/p>
&lt;script src="https://gist.github.com/tobydriscoll/c515e9f5bd4ab540b41db9852db53b72.js">&lt;/script>
&lt;p>Look at the last line&amp;ndash;it&amp;rsquo;s allocating 1.4GB of memory to make the nested loop version happen! I thought perhaps I should use &lt;code>copy&lt;/code> to create &lt;code>v&lt;/code> in each pass, but that change didn&amp;rsquo;t help. I even tried writing my own loop for computing the dot product, to no avail.&lt;/p>
&lt;p>It did help a little to replace the line in which &lt;code>v&lt;/code> is updated with&lt;/p>
&lt;pre>&lt;code class="language-julia">v = broadcast!(-,v,Q[:,i]*R[i,j])
&lt;/code>&lt;/pre>
&lt;p>The bang on the name of the function makes it operate in-place, overwriting the current storage. Apparently Julia will create &lt;a href="https://github.com/JuliaLang/julia/pull/17546" target="_blank" rel="noopener">some syntactic sugar for this maneuver in version 0.5&lt;/a>. Here it reduced the memory usage to 1.1 GB.&lt;/p>
&lt;p>Julia&amp;rsquo;s reputation is that it&amp;rsquo;s great with loops, especially compared to MATLAB and Python. As a Julia newbie I recognize that there may still be only a small change I need to make in order to see this for myself. But I feel as though having to use that &lt;code>broadcast!&lt;/code>, or even the more natural &lt;code>.=&lt;/code> that may be coming, is already too much to ask. I&amp;rsquo;m frustrated, confused, and disappointed.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lectures 6-7</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-6-7/</link><pubDate>Fri, 16 Sep 2016 20:27:58 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lectures-6-7/</guid><description>&lt;p>Here are the Jupyter notebooks for &lt;a href="https://gist.github.com/tobydriscoll/fb438fc942a01e242cc08ee05385af17" target="_blank" rel="noopener">Lecture 6&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/4d1c8856da0c718e1d99d8171e1babec" target="_blank" rel="noopener">Lecture 7&lt;/a>. (I finally noticed that a Gist can hold more than one notebook&amp;hellip;duh.)&lt;/p>
&lt;p>Not much happened in Lecture 6, but I got gobsmacked in Lecture 7. It happened when I tried to convert this boring MATLAB code for backward substitution.&lt;/p>
&lt;pre>&lt;code class="language-matlab">A = magic(9); b = (1:9)';
[Q,R] = qr(A);
z = Q'*b;
x(9,1) = z(9)/R(9,9);
for i = 8👎1
x(i) = (z(i) - R(i,i+1:9)*x(i+1:9)) / R(i,i);
end
&lt;/code>&lt;/pre>
&lt;p>Here is what I first tried in Julia.&lt;/p>
&lt;pre>&lt;code class="language-julia">A = round(10*rand(9,9)); b = (1:9);
m = 9;
(Q,R) = qr(A);
z = Q'*b;
x = zeros(m);
x[m] = z[m]/R[m,m];
for i = m-1👎1
x[i] = (z[i] - R[i,i+1:m]*x[i+1:m]) / R[i,i];
end
&lt;/code>&lt;/pre>
&lt;p>Seems straightforward, but line 4 gives an error. I&amp;rsquo;m not going to copy the error message here, in case you&amp;rsquo;re using mobile data right now. What I mean is that it is verbose, not to mention obscure. You don&amp;rsquo;t appreciate simple, clear error messages until you get something else!&lt;/p>
&lt;p>Anyhow, I then remembered that in Julia, the colon construction &lt;code>(1:9)&lt;/code> produces a Range, not a Vector. As I understand it, Julia embraces a lazy design philosophy: it avoids evaluation of an expression until the last possible moment. Suppose the only use of that Range is to describe a loop iteration&amp;mdash;in that case, why have a vector?&lt;/p>
&lt;p>I&amp;rsquo;m all for lazy philosophy. (Haw haw!) It&amp;rsquo;s not clear to me why the context &lt;code>Q'*b&lt;/code>
does not automatically convert the Range into a Vector. It&amp;rsquo;s even less clear why they have deprecated the idiom &lt;code>[1:9]&lt;/code> to create a Vector; it works for now but gives a warning. Instead one should use &lt;code>collect&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-julia">A = round(10*rand(9,9)); b = collect(1:9);
m = 9;
(Q,R) = qr(A);
z = Q'*b;
x = zeros(m);
x[m] = z[m]/R[m,m];
for i = m-1👎1
x[i] = (z[i] - R[i,i+1:m]*x[i+1:m]) / R[i,i];
end
&lt;/code>&lt;/pre>
&lt;p>Feels very odd to me still, but okay.&lt;/p>
&lt;p>We are not out of the woods yet. This version still fails in the loop body, again vomiting opaque error messages. Remember how, &lt;a href="https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-2/">back in Lecture 2, I mentioned&lt;/a> that scalars and 1x1 matrices are different things? Inside the loop above, &lt;code>z[i]&lt;/code> is a scalar and the product is a length-1 vector. But the subtraction works anyway, as &lt;code>z[i]&lt;/code> is silently promoted to a 1-vector also. No, the problem comes with the assignment: you can&amp;rsquo;t assign a 1-vector to an element of an array of numbers.&lt;/p>
&lt;p>There&amp;rsquo;s a very long (space and time) &lt;a href="https://github.com/JuliaLang/julia/issues/4774#issuecomment-28430963" target="_blank" rel="noopener">discussion&lt;/a> about this and related issues in Julia. Suffice it to say that what mathematicians do with scalars, vectors, matrices, and tensors isn&amp;rsquo;t rigorously consistent&amp;mdash;or at least, there seem to be multiple, incompatible rigorous ways to use them.&lt;/p>
&lt;p>In this particular case I have found two unsatisfying workarounds. The idiom
&lt;code>x[i:i]&lt;/code> produces a Vector, not a scalar, so the assignment goes through. Or we can work on the other side of the assignment and pull out the scalar from the vector:&lt;/p>
&lt;pre>&lt;code class="language-julia">(z[i] - R[i,i+1:m]*x[i+1:m])[1] / R[i,i]
&lt;/code>&lt;/pre>
&lt;p>Now, it&amp;rsquo;s pleasing that this syntax does work, as there is no good MATLAB equivalent for indexing into a temporary expression. I just wish it was in the service of something less dismal.&lt;/p>
&lt;p>Again: Julia&amp;rsquo;s designers have solid reasons for doing things this way. I wouldn&amp;rsquo;t consider it a dealbreaker for research codes, but this episode is not something I would want to explain to undergrads who are just wrapping their heads around LU factorization. It pulls you right out of thinking about math and into thinking about strict-typing, pinhead-dancing angels. How unfortunate.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia Lecture 5: More on the SVD</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-5-more-on-the-svd/</link><pubDate>Mon, 12 Sep 2016 19:47:08 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-5-more-on-the-svd/</guid><description>&lt;p>Notebooks are viewable for &lt;a href="https://gist.github.com/tobydriscoll/934b88e10ac3b6ce6ac95c8c8f480aef" target="_blank" rel="noopener">matlab&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/40d7b7670558f58958e9136febaeec20" target="_blank" rel="noopener">julia&lt;/a>.&lt;/p>
&lt;p>This is one of my favorite demos. It illustrates low-rank approximation by the SVD to show patterns in voting behavior for the U.S. Congress. With no a priori models, project onto two singular vectors and pow&amp;ndash;&lt;a href="http://www.nytimes.com/2003/06/24/science/a-mathematician-crunches-the-supreme-court-s-numbers.html" target="_blank" rel="noopener">meaning and insight jump out&lt;/a>.&lt;/p>
&lt;p>I took one shortcut. I have a MATLAB script that reads the raw voting data from &lt;a href="http://voteview.com" target="_blank" rel="noopener">voteview.com&lt;/a> and converts it to a matrix. No doubt I would learn a lot about I/O in Julia if I translated it, but I got short on time and instead saved it locally from MATLAB. Then load it using the &lt;a href="https://github.com/simonster/MAT.jl" target="_blank" rel="noopener">MAT package&lt;/a> for Julia and Bob&amp;rsquo;s your uncle.&lt;/p>
&lt;p>I did stumble into a nasty gotcha, though. I decided to make histograms for the distributions of the &amp;ldquo;partisan&amp;rdquo; and &amp;ldquo;bipartisan&amp;rdquo; coordinate values. Unfortunately, there&amp;rsquo;s a name clash: MATLAB&amp;rsquo;s best known histogram plotter is called &lt;code>hist&lt;/code>, but Julia has a built-in function by that name that just bins the data. I knew there was also a &lt;code>hist()&lt;/code> in PyPlot, but to my bafflement the access for it was not &lt;code>PyPlot.hist()&lt;/code>, which does exist:&lt;/p>
&lt;pre>&lt;code class="language-julia">help?&amp;gt; PyPlot.hist
hist(v, e) -&amp;amp;gt; e, counts
Compute the histogram of v using a vector/range e as the edges...
hist(v[, n]) -&amp;amp;gt; e, counts
Compute the histogram of v, optionally using approximately...
&lt;/code>&lt;/pre>
&lt;p>This is Julia&amp;rsquo;s built-in function. The next thing I tried was typing in &lt;code>Pyplot.&lt;/code> and hitting tab for a list of completions. Most of the familiar MATLAB-style plotting functions are there, but no &lt;code>hist&lt;/code>, just &lt;code>hist2D&lt;/code>, which is not equivalent. I don&amp;rsquo;t remember now where I found it, but the way to call the function I want is the bizarre &lt;code>plt[:hist]&lt;/code>. Neither &lt;code>?plt&lt;/code> nor tab completion gives any whiff of this syntax or possibility. Obviously there&amp;rsquo;s some logic at work here, and no doubt my Julia and Python ignorance are showing, but this was the most frustrating Julia experience I&amp;rsquo;ve had yet.&lt;/p>
&lt;p>(Ironically, MATLAB has a newer plotting function called &lt;code>histogram&lt;/code>, which does not seem to conflict with any Julia names!)&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 4: SVD</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-4-svd/</link><pubDate>Mon, 12 Sep 2016 12:51:07 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-4-svd/</guid><description>&lt;p>The notebooks: &lt;a href="https://gist.github.com/tobydriscoll/08fc56bca086f957920f1088e0844c30" target="_blank" rel="noopener">matlab&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/324991720db46ff9c644cc43455bd23e" target="_blank" rel="noopener">julia&lt;/a>.&lt;/p>
&lt;p>Today is about some little conveniences/quirks in Julia. Starting here:&lt;/p>
&lt;pre>&lt;code class="language-julia">t = linspace(0,2*pi,300);
x1,x2 = (cos(t),sin(t));
&lt;/code>&lt;/pre>
&lt;p>The second line assigns to two variables simultaneously. It&amp;rsquo;s totally unnecessary here, but it helps to emphasize how the quantities are related.&lt;/p>
&lt;p>Next we have&lt;/p>
&lt;pre>&lt;code class="language-julia">U,σ,V = svd(A)
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;m unreasonably happy about having Greek letters as variable names. Just type in &amp;lsquo;\sigma&amp;rsquo; and hit tab, and voila! It&amp;rsquo;s a reminder of how, in the U.S. at least, we&amp;rsquo;re so used to living within the limitations of ancient 128-character ASCII&amp;mdash;&lt;a href="https://en.wikipedia.org/wiki/ASCII" target="_blank" rel="noopener">telegraphs&lt;/a>, really&amp;mdash;that we can be surprised by expanded possibilities.&lt;/p>
&lt;p>Later on we have &lt;code>diagm(σ)&lt;/code>. In MATLAB, the &lt;code>diag&lt;/code> function has two roles: convert a vector to a diagonal matrix, and extract the diagonal elements of a matrix. This creates a curious edge case for MATLAB: for example, &lt;/p>
&lt;pre>&lt;code class="language-matlab">diag([1 2 3])
&lt;/code>&lt;/pre>
&lt;p>returns a 3-by-3 matrix, not the single element 1. This is almost always what you want, but I&amp;rsquo;ve run into gotchas wherein a program works perfectly until an input of the &amp;lsquo;wrong&amp;rsquo; size silently changes the behavior of a function. In Julia the two functionalities are separated into &lt;code>diag&lt;/code> and &lt;code>diagm&lt;/code>, which avoids the edge case ambiguity. I think it&amp;rsquo;s worth the clarity here to have the extra command.&lt;/p>
&lt;p>The one thing I missed having in the Julia version was MATLAB&amp;rsquo;s &lt;code>format&lt;/code> command, which lets you set the default display of numbers in all following output. In this notebook I just had numbers as placeholders and really wanted just to show shapes and sizes. Julia&amp;rsquo;s full-length output obfuscates the sizes quite a bit, and I&amp;rsquo;d like to tell it to calm down with all those digits for a little while (rather than saying so with each new output). If that capability is there, I overlooked it.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 3: Norms</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-3-norms/</link><pubDate>Wed, 07 Sep 2016 19:32:46 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-3-norms/</guid><description>&lt;p>Here are the &lt;a href="https://gist.github.com/tobydriscoll/b620d1b8beaa04cf87707a55928e3449" target="_blank" rel="noopener">MATLAB&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/2c486e89b12911b073f3c91e514db4f7" target="_blank" rel="noopener">julia&lt;/a> notebooks.&lt;/p>
&lt;p>The big issue this time around was graphics. This topic dramatically illustrates the advantages on both sides of the commercial/open source fence. On the MATLAB side, it&amp;rsquo;s perfectly clear what you should do. There are many options that have been well constructed, and it&amp;rsquo;s all under a relatively consistent umbrella. There are things to learn and options to choose, but it&amp;rsquo;s clear what functions you will be using to make, say, a scatter plot, and a lot of similarity across commands.&lt;/p>
&lt;p>Julia graphics are another story. At this writing, there are two options recommended on &lt;a href="http://julialang.org/downloads/plotting.html" target="_blank" rel="noopener">Julia&amp;rsquo;s official page about plotting packages&lt;/a>: &lt;a href="https://github.com/stevengj/PyPlot.jl" target="_blank" rel="noopener">PyPlot&lt;/a> and &lt;a href="https://github.com/dcjones/Gadfly.jl" target="_blank" rel="noopener">Gadfly&lt;/a>. It doesn&amp;rsquo;t take much exploration to decide that the former is favored by MATLAB veterans and the latter, by R devotees. Confusingly, the &lt;a href="http://julialang.org/downloads/" target="_blank" rel="noopener">general download page&lt;/a> for Julia mentions a third package called &lt;a href="https://github.com/tbreloff/Plots.jl" target="_blank" rel="noopener">Plots&lt;/a> that is supposed to integrate all of the backends. It&amp;rsquo;s still early days for Julia, and I&amp;rsquo;m sure much remains in flux.&lt;/p>
&lt;p>Moreover, because you can (quite easily) import and run Python code in Julia, in principle you have access to all Python plotting packages. One of the big players is &lt;a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib&lt;/a>, which is more or less what Julia&amp;rsquo;s PyPlot is supposed to provide. But there are also &lt;a href="http://bokeh.pydata.org/en/latest/" target="_blank" rel="noopener">Bokeh&lt;/a>, &lt;a href="https://plot.ly/" target="_blank" rel="noopener">plotly&lt;/a>, and &lt;a href="http://www.pyqtgraph.org/" target="_blank" rel="noopener">pyqtgraph&lt;/a>&amp;mdash;for all I know, many more besides. All of these can make gorgeous graphics, often highly interactive and even hosted in the cloud. The relative merits are not at all clear.&lt;/p>
&lt;p>Here we run into the &lt;a href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwjuv7-7j_zOAhXG2xoKHdrgAvYQtwIIHjAA&amp;amp;url=http%3A%2F%2Fwww.ted.com%2Ftalks%2Fbarry_schwartz_on_the_paradox_of_choice%3Flanguage%3Den&amp;amp;usg=AFQjCNHkeD4jDrbOc7TgI5YOQfU1IQ7xOQ" target="_blank" rel="noopener">paradox of choice&lt;/a>: having many options, even good ones, can provoke anxiety rather than satisfaction. Which package do I invest time in learning? MATLAB limits choice but provides a sort of editorial, almost paternal, reassurance.&lt;/p>
&lt;p>My personal goal is to learn Julia from the standpoint of a MATLAB user, so PyPlot it is. All in all, the transition isn&amp;rsquo;t bad, though there are some twists.&lt;/p>
&lt;p>In the last few years I&amp;rsquo;ve been more often turning to automatic function plotting in MATLAB, using &lt;code>fplot&lt;/code>, &lt;code>ezsurf&lt;/code>, and &lt;code>ezcontour&lt;/code>. If PyPlot supports those, I have yet to find out about them. So it&amp;rsquo;s back to the world of evaluating functions on tensor product grids.  A MATLAB veteran turns to meshgrid, but Julia supports broadcasting across singleton dimensions. For example:&lt;/p>
&lt;pre>&lt;code class="language-julia">using PyPlot
x = linspace(-1,1,90);
y = x';
contour(x[:],y[:],sqrt(x.^2 .+ y.^2))&amp;lt;/pre&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Because &lt;code>x&lt;/code> has a column shape while &lt;code>y&lt;/code> has a row shape, the &lt;code>.+&lt;/code>
operator broadcasts each along the &amp;ldquo;missing&amp;rdquo; dimension. It&amp;rsquo;s a clever shortcut once you know it. It works just as well for contours of the vector 1-norm, but for the max norm I had to broadcast manually:&lt;/p>
&lt;pre>&lt;code class="language-julia">contour(x[:],y[:],broadcast(max,abs(x),abs(y)))
&lt;/code>&lt;/pre>
&lt;p>It&amp;rsquo;s not clear to me why that broadcast should not happen automatically, given that
&lt;code>max&lt;/code> is a dedicated elementwise operator.&lt;/p>
&lt;p>There&amp;rsquo;s more Julia subtlety hiding in this notebook, but those issues will wait for another time.&lt;/p></description></item><item><title>Trefethen &amp; Bau &amp; MATLAB &amp; Julia, Lecture 2</title><link>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-2/</link><pubDate>Fri, 02 Sep 2016 19:12:54 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-matlab-julia-lecture-2/</guid><description>&lt;p>Here are the &lt;a href="http://nbviewer.jupyter.org/gist/tobydriscoll/8aa30fdad0346f1c5656ff4a468b1b05" target="_blank" rel="noopener">matlab&lt;/a> and &lt;a href="https://gist.github.com/tobydriscoll/7f404b36fd47d2878f90dd76a1d7a9b9" target="_blank" rel="noopener">julia&lt;/a> notebooks.&lt;/p>
&lt;p>Two things stood out this time. First, consider the following snippet.&lt;/p>
&lt;pre>&lt;code class="language-julia">u = [ 4; -1; 2+2im ]
v = [ -1; 1im; 1 ]
println(&amp;quot;dot(u,v) gives &amp;quot;, dot(u,v))
println(&amp;quot;u'*v gives &amp;quot;,u'*v)
&lt;/code>&lt;/pre>
&lt;p>The result is&lt;/p>
&lt;pre>&lt;code>dot(u,v) gives -2 - 3im
u'*v gives Complex{Int64}[-2 - 3im]
&lt;/code>&lt;/pre>
&lt;p>Unlike in MATLAB, a scalar is not the same thing as a 1-by-1 matrix. This has consequences. The code &lt;code>(u'*v)*eye(3)&lt;/code> throws a dimension mismatch error, while the equivalent with &lt;code>dot&lt;/code> is fine. In the strict sense this is correct, and I suppose Julia made a decision to be strict in contrast to MATLAB&amp;rsquo;s typical laxity. The price is that little bump introduced into a transition that is normally seamless in the minds of users and programmers 99% of the time.&lt;/p>
&lt;p>The other difference is in style more than anything else. Compare MATLAB&amp;rsquo;s&lt;/p>
&lt;pre>&lt;code class="language-matlab">[Q,~] = qr(A);
&lt;/code>&lt;/pre>
&lt;p>to Julia&amp;rsquo;s&lt;/p>
&lt;pre>&lt;code class="language-julia">Q = qr(A)[1]
&lt;/code>&lt;/pre>
&lt;p>Julia&amp;rsquo;s version would be easier if you wanted to extract the $n$th output, where $n$ is a variable, though you could manage it in MATLAB with cells. I&amp;rsquo;m not sure how common that situation is. Also, it could be a surprise in MATLAB that&lt;/p>
&lt;pre>&lt;code class="language-matlab">Q=qr(A)
&lt;/code>&lt;/pre>
&lt;p>does &lt;em>not&lt;/em> do the same thing, because the content and meaning of the outputs depend on the number of outputs.&lt;/p>
&lt;p>A distinction for QR factorization in particular in the two languages is that MATLAB returns the full version by default, while Julia defaults to the skinny form. The latter is nice because an unsuspecting student (or professor) who calls &lt;code>qr(A)&lt;/code> in MATLAB for a really tall matrix might as well kill the process and restart MATLAB Julia makes you do something extra to get the memory-dangerous version.&lt;/p></description></item><item><title>Trefethen &amp; Bau, Lecture 1</title><link>https://tobydriscoll.net/blog/trefethen-bau-lecture-1/</link><pubDate>Thu, 01 Sep 2016 19:56:08 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-lecture-1/</guid><description>&lt;p>Have a look at the &lt;a href="http://nbviewer.jupyter.org/gist/tobydriscoll/2c2a21256da6efab7433bf42371a43f2" target="_blank" rel="noopener">MATLAB&lt;/a> and &lt;a href="http://nbviewer.jupyter.org/gist/tobydriscoll/32708bacb1d569fc589c51571890028b" target="_blank" rel="noopener">Julia&lt;/a> versions of the notebooks for this lecture.&lt;/p>
&lt;p>The first disappointment in Julia comes right at the start: no &lt;code>magic&lt;/code> command in Julia! Why not? I love demonstrating with magic square matrices:&lt;/p>
&lt;ul>
&lt;li>They are instantly familiar or at least understandable to any level of mathematician.&lt;/li>
&lt;li>They have integer entries and thus display compactly.&lt;/li>
&lt;li>You can demonstrate &lt;code>sum&lt;/code>, transpose, and &lt;code>diag&lt;/code> naturally. And getting the &amp;ldquo;antidiagonal&amp;rdquo; sum is a nice exercise.&lt;/li>
&lt;li>The even-sized ones are singular.&lt;/li>
&lt;li>They have the memorable eigenvector $[1,;1,; \cdots; 1]$.&lt;/li>
&lt;/ul>
&lt;p>What a shame. I substitute random matrices, which sacrifices some reproducibility. At least the same code would work in MATLAB.&lt;/p>
&lt;p>Another gotcha comes in line 2: if you create a vector with all integer entries, they are stored as integers. Famously, in MATLAB every number is a complex float unless you specifically declare it otherwise. Julia&amp;rsquo;s approach is standard in computer science, and there are excellent practical reasons for using it. Nor is it difficult to force Julia to use floats. Nevertheless the issue illustrates one of the subtle ways that MATLAB caters to those who are immersed in scientific computing, where pure integer results are rare.&lt;/p>
&lt;p>Next up is a simple but significant change in the syntax: square brackets &lt;code>[ ] &lt;/code>for indexing of matrices and vectors. In MATLAB parentheses &lt;code>()&lt;/code> serve both this purpose and to make function calls. Julia makes sense to me here. It makes this expression unambiguous:&lt;/p>
&lt;pre>&lt;code class="language-matlab">F( [1 2 3] )
&lt;/code>&lt;/pre>
&lt;p>In MATLAB this could be an access to the first three elements of an array F, or (the Julia meaning) a call to a function F with a vector passed as the first argument. I can&amp;rsquo;t think of a reason to have those different actions be expressed identically. I imagine the clash would complicate parsing code, but that&amp;rsquo;s an area I know nothing about.&lt;/p>
&lt;p>From here things settle down. Julia wants me to say &lt;code>1im&lt;/code> rather than &lt;code>1i&lt;/code> or &lt;code>1j&lt;/code> for the imaginary unit; fine. And I must remember to use spaces to separate columns in a matrix construction or concatenation. I often use commas for this in MATLAB. I&amp;rsquo;m a little confused because commas are used to create tuples in Julia, so I would have expected&lt;/p>
&lt;pre>&lt;code class="language-julia">x = [ 1, 2, 3 ]
&lt;/code>&lt;/pre>
&lt;p>to create (if anything) an array whose single element is the tuple 1,2,3. Instead I get a column vector with entries 1, 2, and 3, which, while a lot more useful, was a small surprise to this newbie.&lt;/p>
&lt;p>Enumerating the differences like this makes the experience sound far more frustrating than I found it to be. It&amp;rsquo;s more like driving a car with a different-feeling clutch than going from an automatic to a manual transmission.&lt;/p>
&lt;p>And I really appreciate the Jupyter notebooks! More on them versus the MATLAB Publisher and new Live Editor another time.&lt;/p></description></item><item><title>Trefethen &amp; Bau, via MATLAB and Julia</title><link>https://tobydriscoll.net/blog/trefethen-bau-via-matlab-and-julia/</link><pubDate>Thu, 01 Sep 2016 18:57:05 +0000</pubDate><guid>https://tobydriscoll.net/blog/trefethen-bau-via-matlab-and-julia/</guid><description>&lt;p>This semester I&amp;rsquo;m teaching &lt;a href="https://udel.instructure.com/courses/1335769/assignments/syllabus" target="_blank" rel="noopener">MATH 612&lt;/a>, which is numerical linear and nonlinear algebra for grad students. Linear algebra dominates the course, and for that I&amp;rsquo;m following the now classic textbook by &lt;a href="http://bookstore.siam.org/ot50/" target="_blank" rel="noopener">Trefethen &amp;amp; Bau&lt;/a>. This book has real meaning to me because I learned the subject from &lt;a href="https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwi-qPSJl-7OAhXJ2B4KHTxqBrEQFgghMAA&amp;amp;url=https%3A%2F%2Fpeople.maths.ox.ac.uk%2Ftrefethen%2F&amp;amp;usg=AFQjCNEas_P8d7AHd2BC-vUoVo7V74ONJw" target="_blank" rel="noopener">Nick Trefethen&lt;/a> at Cornell, just a year or two before the book was written. It&amp;rsquo;s when numerical analysis became an appealing subject to me.&lt;/p>
&lt;p>That course is also when I started to learn &lt;a href="http://www.matlab.com" target="_blank" rel="noopener">MATLAB&lt;/a>. I&amp;rsquo;ve been using MATLAB for over 20 years and I&amp;rsquo;m damn good at it. I&amp;rsquo;ve written &lt;a href="http://dx.doi.org/10.1137/1.9780898717662" target="_blank" rel="noopener">a book that teaches it&lt;/a>, and &lt;a href="https://books.google.com/books/about/Schwarz_Christoffel_mapping.html?id=k5KU6clCKssC&amp;amp;hl=en" target="_blank" rel="noopener">another book&lt;/a> largely based on a&lt;a href="http://tobydriscoll.net/SC/index.html" target="_blank" rel="noopener"> software package&lt;/a> I wrote for conformal mapping, and I was an early and key contributor to the &lt;a href="http://www.chebfun.org" target="_blank" rel="noopener">Chebfun&lt;/a> project. I even dominated a game of MATLAB Jeopardy as a grad student at the &lt;a href="http://www.colostate.edu/dept/Mathematics/matlab/vol3num3" target="_blank" rel="noopener">1995 MATLAB Conference&lt;/a> (when version 4.2 of MATLAB ruled the Earth).&lt;/p>
&lt;p>(It isn&amp;rsquo;t quite contemporary, but the &lt;a href="https://web.archive.org/web/19961213182828/http://cam.cornell.edu/" target="_blank" rel="noopener">1996 home page for the Cornell Center for Applied Mathematics&lt;/a> has a banner graphic created in MATLAB&amp;mdash;by yours truly.)&lt;/p>
&lt;p>The tl;dr is that MATLAB has dominated my professional life since that course. It&amp;rsquo;s still a great tool to use for that course, too&amp;mdash;in my mind, learning the theory and learning the numerics are inextricable. In the context of computing, it&amp;rsquo;s incredible to have a 25-year winning streak!&lt;/p>
&lt;p>But while the pedagogical value remains as high as ever, MATLAB is a smaller part of the &amp;ldquo;desktop scientific computing&amp;rdquo; landscape than it was. It&amp;rsquo;s still a behemoth, but there are more good options than ever.  For some time I have felt neglectful toward options that are similar but different, namely &lt;a href="http://scipy.org/" target="_blank" rel="noopener">SciPy&lt;/a> and &lt;a href="http://julialang.org" target="_blank" rel="noopener">Julia&lt;/a>. I&amp;rsquo;ve picked up bits and pieces of them, but not enough to do any serious work.&lt;/p>
&lt;p>Thus I&amp;rsquo;ve decided to learn Julia the same way I did MATLAB: by using it as we cover elementary numerical linear algebra. The students will still get MATLAB, but I&amp;rsquo;ll be doing Julia in parallel. For each lecture (chapter) of Trefethen &amp;amp; Bau, I&amp;rsquo;ll make two &lt;a href="http://jupyter.org/" target="_blank" rel="noopener">Jupyter&lt;/a> notebooks with identical text and two versions of the codes. I&amp;rsquo;m not rewriting T&amp;amp;B, just trying to illustrate some of the concrete ideas and conclusions in each lecture. I&amp;rsquo;m sure my early Julia efforts will be cringeworthy to the cognoscenti, but just as with learning a human language, you have to risk sounding stupid for a while in order to start sounding less stupid. If I can keep up the pace, I&amp;rsquo;ll blog about what I learn about porting to Julia with each new notebook.&lt;/p></description></item><item><title>Data science? Data science!</title><link>https://tobydriscoll.net/blog/data-science-data-science/</link><pubDate>Thu, 30 Jul 2015 13:36:33 +0000</pubDate><guid>https://tobydriscoll.net/blog/data-science-data-science/</guid><description>&lt;p>I just received a copy of &lt;a href="http://sinews.siam.org/" target="_blank" rel="noopener">SIAM News&lt;/a> on a dead tree. It features &lt;a href="http://sinews.siam.org/DetailsPage/tabid/607/ArticleID/565/Data-Science.aspx" target="_blank" rel="noopener">a piece&lt;/a> by &lt;a href="https://www.cs.utah.edu/~crj/" target="_blank" rel="noopener">Chris Johnson&lt;/a> and &lt;a href="http://math.uwaterloo.ca/amath-numerical-analysis-and-scientific-computing-group/hans-de-stercks-homepage" target="_blank" rel="noopener">Hans de Sterck&lt;/a> about &amp;ldquo;Data Science: What Is It and How Is It Taught?&amp;rdquo; As usual in these articles, I find the specifics more interesting than the generalities of a panel discussion. I really liked this bit about the new program in &lt;a href="http://www.science.vt.edu/ais/cmda/" target="_blank" rel="noopener">Computational Modeling and Data Analytics at Virginia Tech&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>In a sense, creating such a program offers the opportunity to rethink curricula on classical topics like calculus that have at many institutions not seen substantial change throughout most of the past century.&lt;/p>
&lt;/blockquote>
&lt;p>This! Well outside the context of data science, too.&lt;/p>
&lt;p>I&amp;rsquo;m so sick of teaching calculus as though it were still 1960. Not that calculus has changed, of course, but what we need from it has been utterly transformed. In the age of computing, knowledge of calculus is more useful for posing the right questions&amp;mdash;as opposed to getting answers to mindless exercises that can be done in seconds on &lt;a href="http://wolframalpha.com" target="_blank" rel="noopener">Wolfram Alpha&lt;/a>. Don&amp;rsquo;t even get me started on teaching series convergence tests to engineering freshmen.&lt;/p>
&lt;p>As far as how to teach data science&amp;hellip;let me figure out how to learn it, first. I&amp;rsquo;m intrigued by &lt;a href="https://github.com/okulbilisim/awesome-datascience" target="_blank" rel="noopener">this repository&lt;/a> as a start.&lt;/p>
&lt;figure id="figure-thanks-to-kzawadzhttptwittercomkzawadz-for-the-infographic-covered-by-creative-commons-ancsa-licensehttpcreativecommonsorglicensesby-nc-sa40">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://camo.githubusercontent.com/9dca9506dbabc0ea73aedb6d2981808152ae6e90/687474703a2f2f692e696d6775722e636f6d2f57344e524964552e706e67" alt="Thanks to [@kzawadz](http://twitter.com/kzawadz) for the infographic. Covered by [Creative Commons A/NC/SA license](http://creativecommons.org/licenses/by-nc-sa/4.0/)." loading="lazy" data-zoomable width="500" />&lt;/div>
&lt;/div>&lt;figcaption>
Thanks to &lt;a href="http://twitter.com/kzawadz" target="_blank" rel="noopener">@kzawadz&lt;/a> for the infographic. Covered by &lt;a href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">Creative Commons A/NC/SA license&lt;/a>.
&lt;/figcaption>&lt;/figure></description></item><item><title>A retrospective look at college math</title><link>https://tobydriscoll.net/blog/a-retrospective-look-at-college-math/</link><pubDate>Wed, 22 Jul 2015 14:28:44 +0000</pubDate><guid>https://tobydriscoll.net/blog/a-retrospective-look-at-college-math/</guid><description>&lt;p>I recommend the post &lt;a href="http://blogs.ams.org/matheducation/2015/07/20/what-i-wish-i-had-learned-more-about-in-college-mathematics/#sthash.6e9g5byf.dpuf" target="_blank" rel="noopener">What I Wish I Had Learned More About in College Mathematics&lt;/a>, written by Sabrina Schmidt, a former math undergrad at Vassar who now works as a data manager. My favorite quote:&lt;/p>
&lt;blockquote>
&lt;p>I wish that I had been introduced earlier and more often to applications, as they would have provided me with a better idea of potential areas of specialization after graduation.&lt;/p>
&lt;/blockquote>
&lt;p>She goes on to mention &lt;a href="http://projecteuclid.org/euclid.im/1109190965" target="_blank" rel="noopener">PageRank&lt;/a> (which I usually cover in my numerical computation courses) as an application of linear algebra, and e-commerce as an application of number theory. She also has other STEM courses, statistics, and computer science on her wish list for her former self.&lt;/p>
&lt;p>Good read.&lt;/p></description></item><item><title>Flipping experiences</title><link>https://tobydriscoll.net/blog/flipping-experiences/</link><pubDate>Mon, 20 Jul 2015 13:46:41 +0000</pubDate><guid>https://tobydriscoll.net/blog/flipping-experiences/</guid><description>&lt;p>In June I attended a &lt;a href="http://mathworks.com" target="_blank" rel="noopener">MathWorks&lt;/a> faculty research summit in Boston. The idea was to bring together academics and industry reps. As one of the very few non-engineers, it didn&amp;rsquo;t give me much fodder for research. But there was a parallel session for educators with a couple of crossover sessions. I &lt;a href="https://tobydriscoll.net/talk/2015-flipping-experiences/">spoke&lt;/a> in one of those about what I have learned from flipping the classroom in my numerical computation course. You can &lt;a href="https://www.icloud.com/keynote/0vEAS1_9IWdUXSTHO-98-ECzg#Flipping-classroom" target="_blank" rel="noopener">view the slides online&lt;/a>.&lt;/p>
&lt;p>The executive summary: Connecting with students in person is the main thing separating me from a &lt;a href="https://www.coursera.org/course/scicomp" target="_blank" rel="noopener">MOOC&lt;/a>. Major challenges in this particular course are the wide variety of backgrounds of the students, and material that spans advanced mathematics as well as some skill with computer coding. My goal is to teach how to bridge the two, to become fluent enough in both types of thinking to at least know when to go the experts and what to ask. Flipping lets students have time to fill in soft spots in their knowledge while absorbing new material, and to get help from me and their peers while they wrestle with putting new ideas into practice. I have no data on whether they do better in this style of class. (They believe they do a bit better, though like it a bit less.) But I know that &lt;em>I&lt;/em> am more engaged, and so I&amp;rsquo;m giving them the best that I have to offer.&lt;/p></description></item><item><title>Promotion system</title><link>https://tobydriscoll.net/blog/promotion-system/</link><pubDate>Fri, 17 Jul 2015 13:39:03 +0000</pubDate><guid>https://tobydriscoll.net/blog/promotion-system/</guid><description>&lt;p>In keeping with my post on &lt;a href="https://tobydriscoll.net/blog/grades-and-motivation/">how grades in a course affect student motivation&lt;/a>, I&amp;rsquo;ve been pondering alternatives to the classic mean-them-and-mean-it model.&lt;/p>
&lt;p>All of my family members have spent time studying karate. (I&amp;rsquo;m a brown belt, FYI, which is like an A.B.D.) One thing I&amp;rsquo;ve always liked about the dojos I&amp;rsquo;ve known is how the belt promotion system works. It&amp;rsquo;s what I would now call a &lt;a href="http://www.knewton.com/blog/ed-tech-101/edtech-101-mastery-based-learning/" target="_blank" rel="noopener">mastery based learning&lt;/a> concept. Students are tested to advance to the next belt when they are ready, regardless of time spent in the system (of course there are practical limitations on the speed of progression). The tests themselves are rigorous but the results are typically foregone conclusions, by design.&lt;/p>
&lt;p>Truly self-paced mastery learning is difficult to fit into the college grading model. With a technology assist it&amp;rsquo;s possible in topics like pre-calculus and at least some calculus, and probably a few other introductory courses I&amp;rsquo;m not familiar with. I don&amp;rsquo;t see how I could do it in my advanced course this fall.&lt;/p>
&lt;p>I could also think of a more corporate model, which is where most of the students will end up. So the first few weeks would be like an interview to determine the initial job rank (i.e., final grade). Based on performance I would give personal feedback and update their ranks accordingly throughout the term. This goes hand in hand with &lt;a href="https://tobydriscoll.net/blog/making-continuous-assessment-work/">continuous assessment&lt;/a>, which I plan to do anyway.&lt;/p>
&lt;p>Because the later material in part builds on earlier concepts, I could argue that progress later on could make up for early struggles. In any case students would be free to fight for grade promotions to the very end of the course. Unlike the karate model, demotions are possible, so they couldn&amp;rsquo;t reach an acceptable level and just lay back.&lt;/p>
&lt;p>A radical realization of this concept would include doing away with the numerical grades on each assignment! I admit, that excites me&amp;mdash;I can&amp;rsquo;t stand the arbitrariness of deciding how many &amp;ldquo;points&amp;rdquo; each mistake is worth. I see no reason why a &lt;a href="https://www.cmu.edu/teaching/designteach/teach/rubrics.html" target="_blank" rel="noopener">grading rubric&lt;/a> can&amp;rsquo;t be precise without being applied quantitively.&lt;/p>
&lt;p>This would be a huge culture shift for me and for the students. It&amp;rsquo;s risky. I&amp;rsquo;d love to hear opinions and experiences trying to do this sort of thing in math.&lt;/p></description></item><item><title>Grades and motivation</title><link>https://tobydriscoll.net/blog/grades-and-motivation/</link><pubDate>Tue, 14 Jul 2015 14:23:05 +0000</pubDate><guid>https://tobydriscoll.net/blog/grades-and-motivation/</guid><description>&lt;p>Grading is weird in so many ways. In the U.S. system, we report a &amp;ldquo;letter&amp;rdquo; grade that is basically an integer from 0 to 10 or so. This value appears on the student&amp;rsquo;s transcript without comment or context, which is an inherently meaningless way to present any data.&lt;/p>
&lt;p>But the raw value itself isn&amp;rsquo;t well defined anyway. When I give a student a C+ in calculus, does it mean that she mastered about 75% of the major topics in the course? Or does it mean that she understands about 3/4 of what is going on in any particular topic? Which of these is preferable? Would a C+ in bicycle riding be enough of a prerequisite to learn how to ride a motorcycle?&lt;/p>
&lt;p>The closest analog to grades in the real world that I can think of is the annual or quarterly performance review. There are doubts being expressed about these too. In &lt;a href="http://www.bloomberg.com/bw/articles/2013-11-07/the-annual-performance-review-worthless-corporate-ritual" target="_blank" rel="noopener">a piece on Bloomberg Business&lt;/a>, long-time management consultant Aubrey Daniels says, &amp;ldquo;It’s a sadistic process for what purpose I don’t know&amp;hellip;.Think of a sports team: A coach doesn’t wait until the end of a season to give his players feedback.&amp;rdquo; So, we&amp;rsquo;re coming back around to &lt;a href="http://tobydriscoll.net/blog/blog/2015/07/13/making-continuous-assessment-work/" target="_blank" rel="noopener">continuous assessment&lt;/a>.&lt;/p>
&lt;p>Yet the &lt;em>form&lt;/em> of the assessment needs to change too.&lt;/p>
&lt;p>What motivates people in the workplace? For one thing, being recognized for their successes. In math we tend to view perfection as the standard, and everything that falls short on homework or exams earns deductions. This is a notably dismal and discouraging viewpoint for learners. It emphasizes the negativity of errors both large and small. When you compare the (hopefully!) flawless and polished solutions on the answer key with your own stumbling attempts, how could you feel anything but foolish? Where is the upside?&lt;/p>
&lt;p>Another thing that motivates us in the real world is a chance to fix our failures. If you&amp;rsquo;ve scored badly on two midterms in a calculus course, you&amp;rsquo;re probably wise to invest your effort elsewhere. The chances of pulling yourself out of the muck are small, in part because averages are heartless and have perfect memory. I always have disdained grading methods that forgive early bad scores or give &amp;ldquo;extra&amp;rdquo; credit chances, but I have to admit that a system that makes recovery from a bad start seem impossible is no way to maintain motivation.&lt;/p>
&lt;p>I don&amp;rsquo;t have answers yet, but I&amp;rsquo;m thinking about some things. More later.&lt;/p></description></item><item><title>Making continuous assessment work</title><link>https://tobydriscoll.net/blog/making-continuous-assessment-work/</link><pubDate>Mon, 13 Jul 2015 15:25:46 +0000</pubDate><guid>https://tobydriscoll.net/blog/making-continuous-assessment-work/</guid><description>&lt;p>I&amp;rsquo;ve come to think that in math at least, continuous learning and assessment may be more important even than [http://www.crlt.umich.edu/tstrategies/tsal](active learning). The traditional model of chunking assessments into weekly or monthly batches encourages the cram-and-dump style of &amp;ldquo;learning.&amp;rdquo; Since students are allowed to delay work on assignments that are crucial to their understanding of incoming material, it&amp;rsquo;s impossible for them to build that understanding in real time. Instead they copy and hope to parse later, when assessment is demanded.&lt;/p>
&lt;p>It&amp;rsquo;s tempting to say that students should suck it up and organize their time better. This attitude ignores human nature, especially the nature of people in late adolescence and early adulthood. Even a large part of my own work is deadline-driven rather than proactive. And &lt;em>I&lt;/em> love math!&lt;/p>
&lt;p>Any big change in expectations encounters resistance. Fortunately, breaking through that resistance sometimes spills over into breaking resistance to the tough job of learning itself.  The trick is doing so in a way that feels fair to the students and manageable to the instructor. It&amp;rsquo;s hard to overthrow everything at once.&lt;/p>
&lt;p>Here&amp;rsquo;s what I&amp;rsquo;m thinking for my fall course on numerical computing. Each class meeting (3 times a week) has a cycle associated with it:&lt;/p>
&lt;p>&lt;em>Before class:&lt;/em>&lt;/p>
&lt;ol>
&lt;li>(them) Read/watch and reflect.&lt;/li>
&lt;li>(them) Take an online quiz on the new material.&lt;/li>
&lt;/ol>
&lt;p>&lt;em>In class:&lt;/em>
3. (mostly me) Review problem spots. Fill in some of the details.
4. (us) Work to produce one graph or one table relevant to the new material.
5. (them) Turn in a description of what is still not clear.&lt;/p>
&lt;p>&lt;em>After class:&lt;/em>
6. (me) While everything is fresh, I take one last try at explaining material that is still confusing.
7. (them) Do a couple of homework problems. Before the next meeting, for full credit; before the following meeting, for partial credit.&lt;/p>
&lt;p>As you can tell, this is a lot of work for everyone, and&amp;ndash;by design&amp;ndash;it&amp;rsquo;s not flexible. To compensate, I won&amp;rsquo;t give exams. There will be some group projects for summative assessments instead.&lt;/p></description></item></channel></rss>