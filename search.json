[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "The sorry state of teaching ODEs\n\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nJul 16, 2019\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nMatlab vs. Julia vs. Python\n\n\n\n\n\n\n\ncomputing\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2019\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nNew look, new tech\n\n\n\n\n\n\n\nmath\n\n\nresearch\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2019\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nJekyll for clicker questions\n\n\n\n\n\n\n\nacademia\n\n\ncomputing\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2018\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia: Iterative methods\n\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2017\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nIt already has happened here\n\n\n\n\n\n\n\nsociety and culture\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia, Lectures 24-29: Eigenvalue stuff\n\n\n\n\n\n\n\ncomputing\n\n\nmath\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia: Lectures 20, 21, 23: Solving square systems\n\n\n\n\n\n\n\ncomputing\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia: Lecture 19, Stability of least squares\n\n\n\n\n\n\n\ncomputing\n\n\nmath\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia, Lectures 12-13: Conditioning and floating point\n\n\n\n\n\n\n\ncomputing\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia, Lecture 11: Least squares\n\n\n\n\n\n\n\ncomputing\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia, Lecture 9: MATLAB\n\n\n\n\n\n\n\nmath\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia, Lecture 8: Gram-Schmidt\n\n\n\n\n\n\n\ncomputing\n\n\nmath\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia, Lectures 6-7\n\n\n\n\n\n\n\nteaching\n\n\ncomputing\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia Lecture 5: More on the SVD\n\n\n\n\n\n\n\ncomputing\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia, Lecture 4: SVD\n\n\n\n\n\n\n\ncomputing\n\n\nmath\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia, Lecture 3: Norms\n\n\n\n\n\n\n\ncomputing\n\n\nmath\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau & MATLAB & Julia, Lecture 2\n\n\n\n\n\n\n\ncomputing\n\n\nmath\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau, Lecture 1\n\n\n\n\n\n\n\ncomputing\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nTrefethen & Bau, via MATLAB and Julia\n\n\n\n\n\n\n\nmath\n\n\ncomputing\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2016\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nWhy not Zoidberg?\n\n\n\n\n\n\n\nmath\n\n\nsociety and culture\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nshowall\n\n\n\n\n\n\n\ncomputing\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nData science? Data science!\n\n\n\n\n\n\n\nacademia\n\n\nmath\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nLength of papers\n\n\n\n\n\n\n\nacademia\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nJul 29, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nQuantum weirdness\n\n\n\n\n\n\n\nscience\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nA retrospective look at college math\n\n\n\n\n\n\n\nmath\n\n\nteaching\n\n\n\n\nA former math undergrad reflects on what she wishes she had learned more about in college.\n\n\n\n\n\n\nJul 22, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nFlipping experiences\n\n\n\n\n\n\n\nteaching\n\n\n\n\nExperiences with flipping the classroom in a numerical computation course.\n\n\n\n\n\n\nJul 20, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nPromotion system\n\n\n\n\n\n\n\nteaching\n\n\n\n\nCan mastery-based learning work in a college math course?\n\n\n\n\n\n\nJul 17, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nBio Breakfast\n\n\n\n\n\n\n\nmath\n\n\nresearch\n\n\n\n\nHow to do better at teaching without driving everyone crazy. Maybe.\n\n\n\n\n\n\nJul 14, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nGrades and motivation\n\n\n\n\n\n\n\nteaching\n\n\n\n\nWhat even is a grade, anyway?\n\n\n\n\n\n\nJul 14, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nMaking continuous assessment work\n\n\n\n\n\n\n\nmath\n\n\nteaching\n\n\n\n\nHow to do better at teaching without driving everyone crazy. Maybe.\n\n\n\n\n\n\nJul 13, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\n  \n\n\n\n\nAddiction\n\n\n\n\n\n\n\nmath\n\n\n\n\nKids say the darndest things.\n\n\n\n\n\n\nJul 9, 2015\n\n\nToby Driscoll\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "book/data-science-1/index.html",
    "href": "book/data-science-1/index.html",
    "title": "Data Science 1",
    "section": "",
    "text": "For my Math 219 class, I was determined to create an introduction to data science for students who have finished a calculus course and plan to study more math. It’s intended to cover the subject at about the same level as calculus ABC, not expecting a lot of mathematical sophistication but also not shying away from the math that’s necessary. It introduces vectors, matrices, and related simple linear algebra in a gradual and concrete way that will motivate and smooth the transition to a more rigorous linear algebra course later on.\nThe course also provides programming foundations in Pandas, Seaborn, and Scikit-Learn, assuming only basic Python skill as a prereuqisite.\nBecause I couldn’t find a textbook that I liked, I created my own. It’s got over 150 examples and demos, most of which are accompanied by short explanatory videos. It also shows students how to set up VS Code to run Jupyter notebooks and employ Copilot AI assistance."
  },
  {
    "objectID": "book/fnc/index.html",
    "href": "book/fnc/index.html",
    "title": "Fundamentals of Numerical Computation",
    "section": "",
    "text": "Buy in print at the SIAM bookstore. Members of SIAM, including student members, get a 30% discount.\nBuy an e-book at the Google Play store. A rental option is also available.\n\n\n\n\n\n\n\nThis textbook is designed to introduce undergraduates in math, computer science, engineering, and related fields to the principles and practice of numerical computation. Our approach emphasizes linear algebra and approximation. The text presents mathematical underpinnings and analysis, complemented with 45 functions and over 160 examples coded in MATLAB, all available for download. Previous experience in MATLAB is not required. The functions and examples have also been implemented in Julia and Python.\nThe text is organized to be useful for either a one-semester introduction or two-semester sequence, with the most advanced techniques and concepts held back for the second half of the book.\nPlease note the known errata.\n\nResources\nThe book’s functions and example scripts can be downloaded as a MATLAB toolbox and installed by double-clicking the file. Alternatively, you can visit the Github page that has the needed files, as well as suggested in-class labs and projects, slides for instructors, and links to some (slightly outdated) videos linked to the text.\nThere are also Julia versions and Python versions of the function and example files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContents\n\nNumbers, problems, and algorithms\n\nFloating point numbers\nProblems and conditioning\nStability of algorithms\n\nSquare linear systems\n\nPolynomial interpolation\nComputing with matrices\nLinear systems\nLU factorization\nEfficiency of matrix computations\nRow pivoting\nVector and matrix norms\nConditioning of linear systems\nExploiting matrix structure\n\nOverdetermined linear systems\n\nFitting functions to data\nThe normal equations\nThe QR factorization\nComputing QR factorizations\n\nRoots of nonlinear equations\n\nThe rootfinding problem\nFixed point iteration\nNewton’s method in one variable\nInterpolation-based methods\nNewton for nonlinear systems\nQuasi-Newton methods\nNonlinear least squares\n\nPiecewise interpolation and calculus\n\nThe interpolation problem\nPiecewise linear interpolation\nCubic splines\nFinite differences\nConvergence of finite differences\nNumerical integration\nAdaptive integration\n\nInitial-value problems for ODEs\n\nBasics of IVPs\nEuler’s method\nSystems of differential equations\nRunge-Kutta methods\nAdaptive Runge-Kutta\nMultistep methods\nImplementation of multistep methods\nZero-stability of multistep methods\n\nMatrix analysis\n\nFrom matrix to insight\nEigenvalue decomposition\nSingular value decomposition\nSymmetry and definiteness\nDimension reduction\n\nKrylov methods in linear algebra\n\nSparsity and structure\nPower iteration\nInverse iteration\nKrylov subspaces\nGMRES\nMINRES and conjugate gradients\nMatrix-free iterations\nPreconditioning\n\nGlobal function approximation\n\nPolynomial interpolation\nThe barycentric formula\nStability of polynomial interpolation\nOrthogonal polynomials\nTrigonometric interpolation\nSpectrally accurate integration\nImproper integrals\n\nBoundary-value problems\n\nShooting\nDifferentiation matrices\nCollocation for linear problems\nNonlinearity and boundary conditions\nThe Galerkin method\n\nDiffusion equations\n\nBlack-Scholes equation\nThe method of lines\nAbsolute stability\nStiffness\nMethod of lines for parabolic PDEs\n\nAdvection equations\n\nTraffic flow\nUpwinding and stability\nAbsolute stability for advection\nThe wave equation\n\nTwo-dimensional problems\n\nTensor-product discretizations\nTwo-dimensional diffusion and advection\nLaplace and Poisson equations\nNonlinear elliptic PDEs"
  },
  {
    "objectID": "project/chebfun/index.html",
    "href": "project/chebfun/index.html",
    "title": "Chebfun",
    "section": "",
    "text": "Chebfun is a free MATLAB package and open source software project. Its aim is to provide numerical computation with functions. My interest in and contribution to the system is most visibly in the solution of ordinary and (1+1 dimensional) partial differential equations. Using familiar MATLAB syntax such as \\ and eigs, solutions to boundary-value or eigenvalue problems can be obtained with full numerical precision automatically.\n\nGallery"
  },
  {
    "objectID": "project/matlabcentral/index.html",
    "href": "project/matlabcentral/index.html",
    "title": "MATLAB software",
    "section": "",
    "text": "In over 25 years of MATLAB programming, I’ve contributed a few utilities and MATLAB gadgets that I’ve found helpful. For example:\n\nunplot Delete the most recently plotted object(s). Used it so, so many times.\nlatex Convert a matrix into a LaTeX matrix or table. Maybe the table data type in MATLAB has obsoleted this, but it’s still handy for writing papers from time to time.\ngslope Click twice on a plot and get the slope between the points. Useful for determining convergence rates.\nsparse Toeplitz matrix construction Some practical sparse matrices are Toeplitz. This takes the pain out of constructing them.\npiecewise-defined function builder Creates a single callable function from a collection of piecewise definitions."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "You can also visit my Zotero profile, my Google Scholar profile, my arXiv author page, or download a bibliography file.\n\n\n\n\n \n\nDriscoll, T. A., Nakatsukasa, Y., & Trefethen, L. N. (2024). AAA Rational Approximation on a Continuum. SIAM Journal on Scientific Computing, 46(2), A929–A952. https://doi.org/10.1137/23M1570508\n\n\nDriscoll, T. A., Braun, R. J., Luke, R. A., Sinopoli, D., Phatak, A., Dorsch, J., Begley, C. G., & Awisi-Gyau, D. (2023). Fitting ODE models of tear film breakup. Modeling and Artificial Intelligence in Ophthalmology, 5(1), 1–36. 10.35119/maio.v5i1.128\n\n\nTaranchuk, M. J., Cummings, L. J., Driscoll, T. A., & Braun, R. J. (2023). Extensional flow of a free film of nematic liquid crystal with moderate elasticity. Physics of Fluids, 35(6), 062113. 10.1063/5.0151809\n\n\nDriscoll, T., Braun, R. J., & Begley, C. G. (2021). Automatic detection of the cornea location in video captures of fluorescence. Modeling and Artificial Intelligence in Ophthalmology, 3(1), Article 1. 10.35119/maio.v3i1.113\n\n\nLuke, R. A., Braun, R. J., Driscoll, T. A., Awisi-Gyau, D., & Begley, C. G. (2021). Parameter Estimation for Mixed-Mechanism Tear Film Thinning. Bulletin of Mathematical Biology, 83(5), 56. 10.1007/s11538-021-00871-x\n\n\nBraun, R. J., Luke, R. A., Driscoll, T. A., & Begley, C. G. (2021). Dynamics and mechanisms for tear breakup (TBU) on the ocular surface. Mathematical Biosciences and Engineering, 18(5), Article mbe-18-05-262. 10.3934/mbe.2021262\n\n\nLuke, R. A., Braun, R. J., Driscoll, T. A., Begley, C. G., & Awisi-Gyau, D. (2020). Parameter Estimation for Evaporation-Driven Tear Film Thinning. Bulletin of Mathematical Biology, 82(6), 71. 10.1007/s11538-020-00745-8\n\n\nAiton, K. W., & Driscoll, T. A. (2020). Preconditioned Nonlinear Iterations for Overlapping Chebyshev Discretizations with Independent Grids. SIAM Journal on Scientific Computing, 42(4), A2360–A2370. 10.1137/19m1242483\n\n\nDriscoll, T. (2019). ComplexRegions.jl: A Julia package for regions in the complex plane. Journal of Open Source Software, 4(44), 1811. 10.21105/joss.01811\n\n\nAiton, K. W., & Driscoll, T. A. (2019). An Adaptive Partition of Unity Method for Multivariate Chebyshev Polynomial Approximations. SIAM Journal on Scientific Computing, 41(5), A3230–A3245. 10.1137/18m1184904\n\n\nMaki, K. L., Henshaw, W. D., McManus, A., Braun, R. J., Chapp, D. M., & Driscoll, T. A. (2019). A model for tear film dynamics during a realistic blink. Journal for Modeling in Ophthalmology, 3, 21–27. 10.35119/maio.v2i3.91\n\n\nBraun, R. J., Driscoll, T. A., Begley, C. G., King-Smith, P. E., & Siddique, J. I. (2018). On tear film breakup (TBU): Dynamics and imaging. Mathematical Medicine and Biology, 35(2), 145–180. 10.1093/imammb/dqw023\n\n\nAiton, K. W., & Driscoll, T. A. (2018). An adaptive partition of unity method for Chebyshev polynomial interpolation. SIAM Journal on Scientific Computing, 40(1), A251–A265. 10.1137/17m112052x\n\n\nDriscoll, T. A., Braun, R. J., & Brosch, J. K. (2018). Simulation of parabolic flow on an eye-shaped domain with moving boundary. Journal of Engineering Mathematics, 111(1), 111–126. 10.1007/s10665-018-9957-7\n\n\nBrosch, J. K., Wu, Z., Begley, C. G., Driscoll, T. A., & Braun, R. J. (2017). Blink characterization using curve fitting and clustering algorithms. Journal for Modeling in Ophthalmology, 1(3), 60–81. 10.35119/maio.v1i3.38\n\n\nLi, L., Braun, R. J., Driscoll, T. A., Henshaw, W. D., Banks, J. W., & King-Smith, P. E. (2016). Computed tear film and osmolarity dynamics on an eye-shaped domain. Mathematical Medicine and Biology, 33(2), 123–157. 10.1093/imammb/dqv013\n\n\nDriscoll, T. A., Süli, E., & Townsend, A. (2016). New directions in numerical computation. Notices of the American Mathematical Society, 63(4), 398–400. 10.1090/noti1363\n\n\nDriscoll, T. A., & Hale, N. (2015). Rectangular spectral collocation. IMA Journal of Numerical Analysis, 36(1), 108–132. 10.1093/imanum/dru062\n\n\nDeng, Q., Braun, R., & Driscoll, T. A. (2014). Heat transfer and tear film dynamics over multiple blink cycles. Physics of Fluids, 26(7), 071901. 10.1063/1.4887341\n\n\nDriscoll, T. A., & Weideman, J. (2014). Optimal domain splitting for interpolation by Chebyshev polynomials. SIAM Journal on Numerical Analysis, 52(4), 1913–1927. 10.1137/130919428\n\n\nDeng, Q., Braun, R. J., Driscoll, T. A., & King-Smith, P. E. (2013). A model for the tear film and ocular surface temperature for partial blinks. Interfacial Phenomena and Heat Transfer, 1(4), 357–381. 10.1615/interfacphenomheattransfer.v1.i4.40\n\n\nBirkisson, A., & Driscoll, T. A. (2012). Automatic Fréchet Differentiation for the Numerical Solution of Boundary-Value Problems. ACM Transactions on Mathematical Software, 38(4), 1–29. 10.1145/2331130.2331134\n\n\nDeng, Q., & Driscoll, T. A. (2012). A Fast Treecode for Multiquadric Interpolation with Varying Shape Parameters. SIAM Journal on Scientific Computing, 34(2), A1126–A1140. 10.1137/110836225\n\n\nBraun, R., Usha, R., McFadden, G., Driscoll, T. A., Cook, L., & King-Smith, P. E. (2012). Thin film dynamics on a prolate spheroid with application to the cornea. Journal of Engineering Mathematics, 73(1), 121–138. 10.1007/s10665-011-9482-4\n\n\nReid, W. M., Driscoll, T. A., & Doty, M. F. (2012). Forming delocalized intermediate states with realistic quantum dots. Journal of Applied Physics, 111, 056102. 10.1063/1.3691113\n\n\nNeves, A. M. A., Driscoll, T. A., Heryudono, A. R. H., Ferreira, A. J. M., Soares, C. M. M., & Jorge, R. M. N. (2011). Adaptive Methods for Analysis of Composite Plates with Radial Basis Functions. Mechanics of Advanced Materials and Structures, 18(6), 420–430. 10.1080/15376494.2010.528155\n\n\nDriscoll, T. A. (2010). Automatic spectral collocation for integral, integro-differential, and integrally reformulated differential equations. Journal of Computational Physics, 229(17), 5980–5998. 10.1016/j.jcp.2010.04.029\n\n\nHeryudono, A. R. H., & Driscoll, T. A. (2010). Radial Basis Function Interpolation on Irregular Domain through Conformal Transplantation. Journal of Scientific Computing, 44(3), 286–300. 10.1007/s10915-010-9380-3\n\n\nUsher, D. C., Driscoll, T. A., Dhurjati, P., Pelesko, J. A., Rossi, L. F., Schleiniger, G., Pusecker, K., & White, H. B. (2010). A transformative model for undergraduate quantitative biology education. CBE Life Sciences Education, 9(3), 181–188. 10.1187/cbe.10-03-0029\n\n\nDeLillo, T. K., Driscoll, T. A., Elcrat, A. R., & Pfaltzgraff, J. A. (2008). Radial and circular slit maps of unbounded multiply connected circle domains. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 464(2095), 1719–1737. 10.1098/rspa.2008.0006\n\n\nDriscoll, T. A., Bornemann, F., & Trefethen, L. N. (2008). The chebop system for automatic solution of differential equations. BIT Numerical Mathematics, 48(4), 701–723. 10.1007/s10543-008-0198-4\n\n\nMaki, K. L., Braun, R. J., Driscoll, T. A., & King-Smith, P. E. (2008). An overset grid method for the study of reflex tearing. Math. Med. Biol., 25, 187–214. 10.1093/imammb/dqn013\n\n\nDriscoll, T. A., & Heryudono, A. R. H. (2007). Adaptive residual subsampling methods for radial basis function interpolation and collocation problems. Computers & Mathematics with Applications, 53(6), 927–939. 10.1016/j.camwa.2006.06.005\n\n\nDriscoll, T. A., & Maki, K. L. (2007). Searching for Rare Growth Factors Using Multicanonical Monte Carlo Methods. SIAM Review, 49(4), 673–692. 10.1137/050637662\n\n\nHeryudono, A., Braun, R. J., Driscoll, T. A., Maki, K. L., Cook, Lp., & PE, K.-S. (2007). Single-equation models for the tear film in a blink cycle: Realistic lid motion. Mathematical Medicine and Biology-a Journal of The Ima, 24(4), 347–377. 10.1093/imammb/dqm004\n\n\nDeLillo, T. K., Driscoll, T. A., Elcrat, A. R., & Pfaltzgraff, J. A. (2006). Computation of Multiply Connected Schwarz-Christoffel Maps for Exterior Domains. Computational Methods and Function Theory, 6(2), 301–315. 10.1007/BF03321616\n\n\nPelesko, J. A., & Driscoll, T. A. (2006). The effect of the small-aspect-ratio approximation on canonical electrostatic MEMS models. Journal of Engineering Mathematics, 53(3–4), 239–252. 10.1007/s10665-005-9013-2\n\n\nPlatte, R. B., & Driscoll, T. A. (2006). Eigenvalue stability of radial basis function discretizations for time-dependent problems. Computers & Mathematics with Applications, 51(8), 1251–1268. 10.1016/j.camwa.2006.04.007\n\n\nDriscoll, T. A. (2005). Algorithm 843: Improvements to the Schwarz-Christoffel toolbox for MATLAB. ACM Transactions on Mathematical Software (TOMS), 31(2), 239–251. 10.1145/1067967.1067971\n\n\nPlatte, R. B., & Driscoll, T. A. (2005). Polynomials and Potential Theory for Gaussian Radial Basis Function Interpolation. SIAM Journal on Numerical Analysis, 43(2), 750–766. 10.1137/040610143\n\n\nCollins, C. R., Driscoll, T. A., & Stephenson, K. (2004). Curvature flow in conformal mapping. Computational Methods and Function Theory, 3(1), 325–347. 10.1007/bf03321041\n\n\nDriscoll, T. (2004). Book Review: A Practical Guide to Boundary Element Methods With the Software Library BEMLIB. By C. P OZRIKIDIS. CRC Press, 2002. Journal of Fluid Mechanics, 505, 378–379. 10.1017/s0022112004008201\n\n\nPlatte, R. B., & Driscoll, T. A. (2004). Computing eigenmodes of elliptic operators using radial basis functions. Computers & Mathematics with Applications, 48(3–4), 561–576. 10.1016/j.camwa.2003.08.007\n\n\nDriscoll, T. A., & Gottlieb, H. (2003). Isospectral shapes with Neumann and alternating boundary conditions. Physical Review E, 68(1), 016702. 10.1103/physreve.68.016702\n\n\nDriscoll, T. A. (2002). A Composite Runge–Kutta Method for the Spectral Solution of Semilinear PDEs. Journal of Computational Physics, 182(2), 357–367. 10.1006/jcph.2002.7127\n\n\nDriscoll, T. A., & Fornberg, B. (2002). Interpolation in the limit of increasingly flat radial basis functions. Computers & Mathematics with Applications, 43(3–5), 413–422. 10.1016/s0898-1221(01)00295-4\n\n\nFornberg, B., Driscoll, T. A., Wright, G., & Charles, R. (2002). Observations on the behavior of radial basis function approximations near boundaries. Computers & Mathematics with Applications, 43(3–5), 473–490. 10.1016/s0898-1221(01)00299-1\n\n\nDriscoll, T. A., & Fornberg, B. (2001). A Padé-based algorithm for overcoming the Gibbs phenomenon. Numerical Algorithms, 26(1), 77–92. 10.1023/A:1016648530648\n\n\nGoano, M., Bertazzi, F., Caravelli, P., Ghione, G., & Driscoll, T. A. (2001). A general conformal-mapping approach to the optimum electrode design of coplanar waveguides with arbitrary cross section. IEEE Transactions on Microwave Theory and Techniques, 49(9), 1573–1580. 10.1109/22.942569\n\n\nDriscoll, T. A., & Fornberg, B. (2000). Note on nonsymmetric finite differences for Maxwell’s equations. Journal of Computational Physics, 161(2), 723–727. 10.1006/jcph.2000.6524\n\n\nGhrist, M., Fornberg, B., & Driscoll, T. A. (2000). Staggered time integrators for wave equations. SIAM Journal on Numerical Analysis, 38(3), 718–741. 10.1137/s0036142999351777\n\n\nDriscoll, T. A. (1999). A Nonoverlapping Domain Decomposition Method for Symm’s Equation for Conformal Mapping. SIAM Journal on Numerical Analysis, 36(3), 922–934. 10.1137/S0036142997324162\n\n\nDriscoll, T. A. (1999). Computational Conformal Mapping (book review). SIAM Review, 41(4), 832–834.\n\n\nDriscoll, T. A., & Fornberg, B. (1999). Block pseudospectral methods for Maxwell’s equations II: Two-dimensional, discontinuous-coefficient case. SIAM Journal on Scientific Computing, 21(3), 1146–1167. 10.1137/s106482759833320x\n\n\nFornberg, B., & Driscoll, T. A. (1999). A fast spectral algorithm for nonlinear wave equations with linear dispersion. Journal of Computational Physics, 155(2), 456–467. 10.1006/jcph.1999.6351\n\n\nDriscoll, T. A., Toh, K.-C., & Trefethen, L. N. (1998). From Potential Theory to Matrix Iterations in Six Steps. SIAM Review, 40(3), 547–578. 10.1137/S0036144596305582\n\n\nDriscoll, T. A., & Fornberg, B. (1998). A Block Pseudospectral Method for Maxwell’s Equations: I. One-Dimensional Case. J. Comput. Phys., 140, 1–19. 10.1006/jcph.1998.5883\n\n\nDriscoll, T. A., & Vavasis, S. A. (1998). Numerical conformal mapping using cross-ratios and Delaunay triangulation. SIAM Journal on Scientific Computing, 19(6), 1783–1803. 10.1137/s1064827596298580\n\n\nDriscoll, T. A. (1997). Eigenmodes of Isospectral Drums. SIAM Review, 39(1), 1–17. 10.1137/S0036144595285069\n\n\nDriscoll, T. A., & Trefethen, L. N. (1996). Pseudospectra for the wave equation with an absorbing boundary. Journal of Computational and Applied Mathematics, 69(1), 125–142. 10.1016/0377-0427(95)00021-6\n\n\nDriscoll, T. A. (1996). Algorithm 756: A MATLAB toolbox for Schwarz-Christoffel mapping. ACM Transactions on Mathematical Software (TOMS), 22(2), 168–186. 10.1145/229473.229475\n\n\nBaggett, J. S., Driscoll, T. A., & Trefethen, L. N. (1995). A mostly linear model of transition to turbulence. Physics of Fluids, 7(4), 833–838. 10.1063/1.868606\n\n\nDzielski, J. E., & Driscoll, T. A. (1993). Error bound on the solution of a linear differential equation in Chebyshev series. International Journal of Systems Science, 24(7), 1317–1327. 10.1080/00207729308949562\n\n\nTrefethen, L. N., Trefethen, A. E., Reddy, S. C., & Driscoll, T. A. (1993). Hydrodynamic stability without eigenvalues. Science, 261(5121), 578–584. 10.1126/science.261.5121.578\n\n\n\n\n\n\n\n\n \n\nDriscoll, T. A., & Braun, R. J. (2018). Fundamentals of Numerical Computation. Society for Industrial and Applied Mathematics.\n\n\nTrefethen, L. N., Birkisson, Á., & Driscoll, T. A. (2017). Exploring ODEs. SIAM. 10.1137/1.9781611975161\n\n\nDriscoll, T. A., Hale, N., & Trefethen, L. N. (2014). Chebfun guide. Pafnuty Publications, Oxford.\n\n\nDriscoll, T. A. (2009). Learning MATLAB. Society for Industrial and Applied Mathematics.\n\n\nDriscoll, T. A., & Trefethen, L. N. (2002). Schwarz–Christoffel Mapping. Cambridge University Press.\n\n\n\n\n\n\n\n\n \n\nBraun, R. J., Driscoll, T. A., & Begley, C. G. (2019). Mathematical Models of the Tear Film. In Ocular Fluid Dynamics: Anatomy, Physiology, Imaging Techniques, and Mathematical Modeling (pp. 387–432). Springer-Birkhauser. https://www.springer.com/gp/book/9783030258856\n\n\nDriscoll, T. A., & Fornberg, B. (2007). Pade-based interpretation and correction of the Gibbs phenomenon. In Advances in the Gibbs Phenomenon, ed. By A. Jerri. Sigma Sampling Publishing.\n\n\n\n\n\n\n\n\n \n\nBraun, R. J., Driscoll, T., Begley, C., Situ, P., Tichenor, A., & Luke, R. (2023). Tear Breakup (TBU) Analysis with Fluorescence (FL) and Thermal (TH) imaging. Investigative Ophthalmology & Visual Science, 64(8), 186–186.\n\n\nBraun, R. J., Driscoll, T. A., Sinopoli, D., Dorsch, J., Hammond, C., Luke, R. A., & Begley, C. G. (2022). Data and Analysis from Tear Breakup (TBU) in Normal Subjects. Investigative Ophthalmology & Visual Science, 63(7), 3950-A0230.\n\n\nLuke, R. A., Braun, R. J., Driscoll, T., Sinopoli, D., Yawatkar, V., You, L., Phatak, A., & Begley, C. (2021). Fitting Simplified Models to Machine Learning-Identified Tear Film Breakup. Investigative Ophthalmology & Visual Science, 62(8), 1315–1315.\n\n\nBraun, R. J., Zhong, L., Driscoll, T. A., Begley, C. G., Antwi, D., & King-Smith, P. E. (2018, June). Models for Tear Break Up Dynamics and Imaging. 7th European Conference on Computational Fluid Dynamics.\n\n\nBrosch, J., Driscoll, T., & Braun, R. (2016). Simulation of Thin Film Equations on an Eye-Shaped Domain with Moving Boundary. APS March Meeting Abstracts.\n\n\nMaki, K., Henshaw, W., Barron, G., Chapp, D., Braun, R. J., & Driscoll, T. A. (2016). A theoretical investigation of the influence of a blink on the tear film dynamics. Investigative Ophthalmology & Visual Science, 57(12), 6173–6173.\n\n\nZhong, L., Ketelaar, C. F., Braun, R. J., Driscoll, T. A., King-Smith, P. E., & Begley, C. G. (2016). Mathematical Modeling of Glob-Driven Tear Film Breakup. Investigative Ophthalmology & Visual Science, 57.\n\n\nBraun, R., Li, L., Henshaw, W., Driscoll, T., & King-Smith, P. (2015). Solute Dynamics and Imaging in the Tear Film on an Eye-shaped Domain. APS Meeting Abstracts.\n\n\nKetelaar, C., Zhong, L., Braun, R., Driscoll, T., King-Smith, P., & Begley, C. (2015). Tear Film Dynamics Around a Rigid Model Blob. APS Meeting Abstracts.\n\n\nStapf, M., Braun, R., Begley, C., Driscoll, T., & King-Smith, P. E. (2015). Modeling Tear Film Evaporation and Breakup with Duplex Films. APS Meeting Abstracts.\n\n\nMcCulloch, M., Chen, L., Schleiniger, G., & Driscoll, T. (2014). DATA DRIVEN MATHEMATICAL MODELING OF THE SINGLE VENTRICLE ANATOMY AND PHYSIOLOGY. Critical Care Medicine, 42, A1414.\n\n\nLi, L., Braun, R., Driscoll, T., Henshaw, W., Banks, J., & King-Smith, P. (2013). Coupling Osmolarity Dynamics within Human Tear Film on an Eye-Shaped Domain. APS Meeting Abstracts.\n\n\nDeng, Q., Driscoll, T., & Braun, R. (2012). A Model Problem for Tear Film Distribution on a Moving Rectangular Domain. APS Meeting Abstracts.\n\n\nBraun, R., McFadden, J., Ranganathan, U., Driscoll, T., & King-Smith, E. (2008). Recent progress on Modeling the Human Tear Film. APS Division of Fluid Dynamics Meeting Abstracts.\n\n\nMaki, K. L., Braun, R., Driscoll, T., Heryudono, A., King-Smith, P., & Fast, P. (2007). A Overset Grid Method for Fourth Order Evolution Equations of Human Tear Film. APS Division of Fluid Dynamics Meeting Abstracts.\n\n\nDriscoll, T., & Rossi, L. (2003). Numerical examination of a model of thermo-acoustic instabilites in lean, pre-mixed combustors. APS Division of Fluid Dynamics Meeting Abstracts.\n\n\nDriscoll, T. A., & Fornberg, B. (1998). Uses of the Berenger PML in Pseudospectral Methods for Maxwell’s Equations. IUTAM Symposium on Computational Methods for Unbounded Domains, 95–102. https://doi.org/10.1007/978-94-015-9095-2_10\n\n\nTrefethen, L. N., & Driscoll, T. A. (1998). Schwarz-Christoffel mapping in the computer era. Proceedings of the International Congress of Mathematicians, 3, 533–542.\n\n\nWojcik, G., Fomberg, B., Waag, R., Carcione, L., Mould, J., Nikodym, L., & Driscoll, T. A. (1997). Pseudospectral methods for large-scale bioacoustic models. 1997 IEEE Ultrasonics Symposium Proceedings. An International Symposium (Cat. No. 97CH36118), 2, 1501–1506. https://doi.org/10.1109/ultsym.1997.661861\n\n\nDriscoll, T., & Dzielski, J. (1992). Computational efficiency of a functional expansion algorithm for linear quadratic optimal control. Proceedings of the 31st IEEE Conference on Decision and Control, 143–144. https://doi.org/10.1109/cdc.1992.371773\n\n\n\n\n\n\n\n\n \n\nDriscoll, T. A. (2023). RationalFunctionApproximation.jl: Rational function approximatoin in Julia (0.1.0) [Computer software]. Zenodo. 10.5281/ZENODO.8355790\n\n\nDriscoll, T. A. (2021). Cornea detection software package for Julia (0.1.0) [Computer software]. On GitHub\n\n\nDriscoll, T. A. (2019). ComplexRegions.jl: A Julia package for regions in the complex plane (Computer software). 10.5281/zenodo.3548866\n\n\nTrefethen, L., Hale, N., Platte, R., Driscoll, T., & Pachón, R. (2009). Chebfun version 2 (Computer software). chebfun.org\n\n\nDriscoll, T. A. (1994). Schwarz-Christoffel Toolbox for MATLAB (Computer software). [On GitHub[(https://github.com/tobydriscoll/sc-toolbox)"
  },
  {
    "objectID": "publications.html#journal-articles",
    "href": "publications.html#journal-articles",
    "title": "Publications",
    "section": "",
    "text": "Driscoll, T. A., Nakatsukasa, Y., & Trefethen, L. N. (2024). AAA Rational Approximation on a Continuum. SIAM Journal on Scientific Computing, 46(2), A929–A952. https://doi.org/10.1137/23M1570508\n\n\nDriscoll, T. A., Braun, R. J., Luke, R. A., Sinopoli, D., Phatak, A., Dorsch, J., Begley, C. G., & Awisi-Gyau, D. (2023). Fitting ODE models of tear film breakup. Modeling and Artificial Intelligence in Ophthalmology, 5(1), 1–36. 10.35119/maio.v5i1.128\n\n\nTaranchuk, M. J., Cummings, L. J., Driscoll, T. A., & Braun, R. J. (2023). Extensional flow of a free film of nematic liquid crystal with moderate elasticity. Physics of Fluids, 35(6), 062113. 10.1063/5.0151809\n\n\nDriscoll, T., Braun, R. J., & Begley, C. G. (2021). Automatic detection of the cornea location in video captures of fluorescence. Modeling and Artificial Intelligence in Ophthalmology, 3(1), Article 1. 10.35119/maio.v3i1.113\n\n\nLuke, R. A., Braun, R. J., Driscoll, T. A., Awisi-Gyau, D., & Begley, C. G. (2021). Parameter Estimation for Mixed-Mechanism Tear Film Thinning. Bulletin of Mathematical Biology, 83(5), 56. 10.1007/s11538-021-00871-x\n\n\nBraun, R. J., Luke, R. A., Driscoll, T. A., & Begley, C. G. (2021). Dynamics and mechanisms for tear breakup (TBU) on the ocular surface. Mathematical Biosciences and Engineering, 18(5), Article mbe-18-05-262. 10.3934/mbe.2021262\n\n\nLuke, R. A., Braun, R. J., Driscoll, T. A., Begley, C. G., & Awisi-Gyau, D. (2020). Parameter Estimation for Evaporation-Driven Tear Film Thinning. Bulletin of Mathematical Biology, 82(6), 71. 10.1007/s11538-020-00745-8\n\n\nAiton, K. W., & Driscoll, T. A. (2020). Preconditioned Nonlinear Iterations for Overlapping Chebyshev Discretizations with Independent Grids. SIAM Journal on Scientific Computing, 42(4), A2360–A2370. 10.1137/19m1242483\n\n\nDriscoll, T. (2019). ComplexRegions.jl: A Julia package for regions in the complex plane. Journal of Open Source Software, 4(44), 1811. 10.21105/joss.01811\n\n\nAiton, K. W., & Driscoll, T. A. (2019). An Adaptive Partition of Unity Method for Multivariate Chebyshev Polynomial Approximations. SIAM Journal on Scientific Computing, 41(5), A3230–A3245. 10.1137/18m1184904\n\n\nMaki, K. L., Henshaw, W. D., McManus, A., Braun, R. J., Chapp, D. M., & Driscoll, T. A. (2019). A model for tear film dynamics during a realistic blink. Journal for Modeling in Ophthalmology, 3, 21–27. 10.35119/maio.v2i3.91\n\n\nBraun, R. J., Driscoll, T. A., Begley, C. G., King-Smith, P. E., & Siddique, J. I. (2018). On tear film breakup (TBU): Dynamics and imaging. Mathematical Medicine and Biology, 35(2), 145–180. 10.1093/imammb/dqw023\n\n\nAiton, K. W., & Driscoll, T. A. (2018). An adaptive partition of unity method for Chebyshev polynomial interpolation. SIAM Journal on Scientific Computing, 40(1), A251–A265. 10.1137/17m112052x\n\n\nDriscoll, T. A., Braun, R. J., & Brosch, J. K. (2018). Simulation of parabolic flow on an eye-shaped domain with moving boundary. Journal of Engineering Mathematics, 111(1), 111–126. 10.1007/s10665-018-9957-7\n\n\nBrosch, J. K., Wu, Z., Begley, C. G., Driscoll, T. A., & Braun, R. J. (2017). Blink characterization using curve fitting and clustering algorithms. Journal for Modeling in Ophthalmology, 1(3), 60–81. 10.35119/maio.v1i3.38\n\n\nLi, L., Braun, R. J., Driscoll, T. A., Henshaw, W. D., Banks, J. W., & King-Smith, P. E. (2016). Computed tear film and osmolarity dynamics on an eye-shaped domain. Mathematical Medicine and Biology, 33(2), 123–157. 10.1093/imammb/dqv013\n\n\nDriscoll, T. A., Süli, E., & Townsend, A. (2016). New directions in numerical computation. Notices of the American Mathematical Society, 63(4), 398–400. 10.1090/noti1363\n\n\nDriscoll, T. A., & Hale, N. (2015). Rectangular spectral collocation. IMA Journal of Numerical Analysis, 36(1), 108–132. 10.1093/imanum/dru062\n\n\nDeng, Q., Braun, R., & Driscoll, T. A. (2014). Heat transfer and tear film dynamics over multiple blink cycles. Physics of Fluids, 26(7), 071901. 10.1063/1.4887341\n\n\nDriscoll, T. A., & Weideman, J. (2014). Optimal domain splitting for interpolation by Chebyshev polynomials. SIAM Journal on Numerical Analysis, 52(4), 1913–1927. 10.1137/130919428\n\n\nDeng, Q., Braun, R. J., Driscoll, T. A., & King-Smith, P. E. (2013). A model for the tear film and ocular surface temperature for partial blinks. Interfacial Phenomena and Heat Transfer, 1(4), 357–381. 10.1615/interfacphenomheattransfer.v1.i4.40\n\n\nBirkisson, A., & Driscoll, T. A. (2012). Automatic Fréchet Differentiation for the Numerical Solution of Boundary-Value Problems. ACM Transactions on Mathematical Software, 38(4), 1–29. 10.1145/2331130.2331134\n\n\nDeng, Q., & Driscoll, T. A. (2012). A Fast Treecode for Multiquadric Interpolation with Varying Shape Parameters. SIAM Journal on Scientific Computing, 34(2), A1126–A1140. 10.1137/110836225\n\n\nBraun, R., Usha, R., McFadden, G., Driscoll, T. A., Cook, L., & King-Smith, P. E. (2012). Thin film dynamics on a prolate spheroid with application to the cornea. Journal of Engineering Mathematics, 73(1), 121–138. 10.1007/s10665-011-9482-4\n\n\nReid, W. M., Driscoll, T. A., & Doty, M. F. (2012). Forming delocalized intermediate states with realistic quantum dots. Journal of Applied Physics, 111, 056102. 10.1063/1.3691113\n\n\nNeves, A. M. A., Driscoll, T. A., Heryudono, A. R. H., Ferreira, A. J. M., Soares, C. M. M., & Jorge, R. M. N. (2011). Adaptive Methods for Analysis of Composite Plates with Radial Basis Functions. Mechanics of Advanced Materials and Structures, 18(6), 420–430. 10.1080/15376494.2010.528155\n\n\nDriscoll, T. A. (2010). Automatic spectral collocation for integral, integro-differential, and integrally reformulated differential equations. Journal of Computational Physics, 229(17), 5980–5998. 10.1016/j.jcp.2010.04.029\n\n\nHeryudono, A. R. H., & Driscoll, T. A. (2010). Radial Basis Function Interpolation on Irregular Domain through Conformal Transplantation. Journal of Scientific Computing, 44(3), 286–300. 10.1007/s10915-010-9380-3\n\n\nUsher, D. C., Driscoll, T. A., Dhurjati, P., Pelesko, J. A., Rossi, L. F., Schleiniger, G., Pusecker, K., & White, H. B. (2010). A transformative model for undergraduate quantitative biology education. CBE Life Sciences Education, 9(3), 181–188. 10.1187/cbe.10-03-0029\n\n\nDeLillo, T. K., Driscoll, T. A., Elcrat, A. R., & Pfaltzgraff, J. A. (2008). Radial and circular slit maps of unbounded multiply connected circle domains. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences, 464(2095), 1719–1737. 10.1098/rspa.2008.0006\n\n\nDriscoll, T. A., Bornemann, F., & Trefethen, L. N. (2008). The chebop system for automatic solution of differential equations. BIT Numerical Mathematics, 48(4), 701–723. 10.1007/s10543-008-0198-4\n\n\nMaki, K. L., Braun, R. J., Driscoll, T. A., & King-Smith, P. E. (2008). An overset grid method for the study of reflex tearing. Math. Med. Biol., 25, 187–214. 10.1093/imammb/dqn013\n\n\nDriscoll, T. A., & Heryudono, A. R. H. (2007). Adaptive residual subsampling methods for radial basis function interpolation and collocation problems. Computers & Mathematics with Applications, 53(6), 927–939. 10.1016/j.camwa.2006.06.005\n\n\nDriscoll, T. A., & Maki, K. L. (2007). Searching for Rare Growth Factors Using Multicanonical Monte Carlo Methods. SIAM Review, 49(4), 673–692. 10.1137/050637662\n\n\nHeryudono, A., Braun, R. J., Driscoll, T. A., Maki, K. L., Cook, Lp., & PE, K.-S. (2007). Single-equation models for the tear film in a blink cycle: Realistic lid motion. Mathematical Medicine and Biology-a Journal of The Ima, 24(4), 347–377. 10.1093/imammb/dqm004\n\n\nDeLillo, T. K., Driscoll, T. A., Elcrat, A. R., & Pfaltzgraff, J. A. (2006). Computation of Multiply Connected Schwarz-Christoffel Maps for Exterior Domains. Computational Methods and Function Theory, 6(2), 301–315. 10.1007/BF03321616\n\n\nPelesko, J. A., & Driscoll, T. A. (2006). The effect of the small-aspect-ratio approximation on canonical electrostatic MEMS models. Journal of Engineering Mathematics, 53(3–4), 239–252. 10.1007/s10665-005-9013-2\n\n\nPlatte, R. B., & Driscoll, T. A. (2006). Eigenvalue stability of radial basis function discretizations for time-dependent problems. Computers & Mathematics with Applications, 51(8), 1251–1268. 10.1016/j.camwa.2006.04.007\n\n\nDriscoll, T. A. (2005). Algorithm 843: Improvements to the Schwarz-Christoffel toolbox for MATLAB. ACM Transactions on Mathematical Software (TOMS), 31(2), 239–251. 10.1145/1067967.1067971\n\n\nPlatte, R. B., & Driscoll, T. A. (2005). Polynomials and Potential Theory for Gaussian Radial Basis Function Interpolation. SIAM Journal on Numerical Analysis, 43(2), 750–766. 10.1137/040610143\n\n\nCollins, C. R., Driscoll, T. A., & Stephenson, K. (2004). Curvature flow in conformal mapping. Computational Methods and Function Theory, 3(1), 325–347. 10.1007/bf03321041\n\n\nDriscoll, T. (2004). Book Review: A Practical Guide to Boundary Element Methods With the Software Library BEMLIB. By C. P OZRIKIDIS. CRC Press, 2002. Journal of Fluid Mechanics, 505, 378–379. 10.1017/s0022112004008201\n\n\nPlatte, R. B., & Driscoll, T. A. (2004). Computing eigenmodes of elliptic operators using radial basis functions. Computers & Mathematics with Applications, 48(3–4), 561–576. 10.1016/j.camwa.2003.08.007\n\n\nDriscoll, T. A., & Gottlieb, H. (2003). Isospectral shapes with Neumann and alternating boundary conditions. Physical Review E, 68(1), 016702. 10.1103/physreve.68.016702\n\n\nDriscoll, T. A. (2002). A Composite Runge–Kutta Method for the Spectral Solution of Semilinear PDEs. Journal of Computational Physics, 182(2), 357–367. 10.1006/jcph.2002.7127\n\n\nDriscoll, T. A., & Fornberg, B. (2002). Interpolation in the limit of increasingly flat radial basis functions. Computers & Mathematics with Applications, 43(3–5), 413–422. 10.1016/s0898-1221(01)00295-4\n\n\nFornberg, B., Driscoll, T. A., Wright, G., & Charles, R. (2002). Observations on the behavior of radial basis function approximations near boundaries. Computers & Mathematics with Applications, 43(3–5), 473–490. 10.1016/s0898-1221(01)00299-1\n\n\nDriscoll, T. A., & Fornberg, B. (2001). A Padé-based algorithm for overcoming the Gibbs phenomenon. Numerical Algorithms, 26(1), 77–92. 10.1023/A:1016648530648\n\n\nGoano, M., Bertazzi, F., Caravelli, P., Ghione, G., & Driscoll, T. A. (2001). A general conformal-mapping approach to the optimum electrode design of coplanar waveguides with arbitrary cross section. IEEE Transactions on Microwave Theory and Techniques, 49(9), 1573–1580. 10.1109/22.942569\n\n\nDriscoll, T. A., & Fornberg, B. (2000). Note on nonsymmetric finite differences for Maxwell’s equations. Journal of Computational Physics, 161(2), 723–727. 10.1006/jcph.2000.6524\n\n\nGhrist, M., Fornberg, B., & Driscoll, T. A. (2000). Staggered time integrators for wave equations. SIAM Journal on Numerical Analysis, 38(3), 718–741. 10.1137/s0036142999351777\n\n\nDriscoll, T. A. (1999). A Nonoverlapping Domain Decomposition Method for Symm’s Equation for Conformal Mapping. SIAM Journal on Numerical Analysis, 36(3), 922–934. 10.1137/S0036142997324162\n\n\nDriscoll, T. A. (1999). Computational Conformal Mapping (book review). SIAM Review, 41(4), 832–834.\n\n\nDriscoll, T. A., & Fornberg, B. (1999). Block pseudospectral methods for Maxwell’s equations II: Two-dimensional, discontinuous-coefficient case. SIAM Journal on Scientific Computing, 21(3), 1146–1167. 10.1137/s106482759833320x\n\n\nFornberg, B., & Driscoll, T. A. (1999). A fast spectral algorithm for nonlinear wave equations with linear dispersion. Journal of Computational Physics, 155(2), 456–467. 10.1006/jcph.1999.6351\n\n\nDriscoll, T. A., Toh, K.-C., & Trefethen, L. N. (1998). From Potential Theory to Matrix Iterations in Six Steps. SIAM Review, 40(3), 547–578. 10.1137/S0036144596305582\n\n\nDriscoll, T. A., & Fornberg, B. (1998). A Block Pseudospectral Method for Maxwell’s Equations: I. One-Dimensional Case. J. Comput. Phys., 140, 1–19. 10.1006/jcph.1998.5883\n\n\nDriscoll, T. A., & Vavasis, S. A. (1998). Numerical conformal mapping using cross-ratios and Delaunay triangulation. SIAM Journal on Scientific Computing, 19(6), 1783–1803. 10.1137/s1064827596298580\n\n\nDriscoll, T. A. (1997). Eigenmodes of Isospectral Drums. SIAM Review, 39(1), 1–17. 10.1137/S0036144595285069\n\n\nDriscoll, T. A., & Trefethen, L. N. (1996). Pseudospectra for the wave equation with an absorbing boundary. Journal of Computational and Applied Mathematics, 69(1), 125–142. 10.1016/0377-0427(95)00021-6\n\n\nDriscoll, T. A. (1996). Algorithm 756: A MATLAB toolbox for Schwarz-Christoffel mapping. ACM Transactions on Mathematical Software (TOMS), 22(2), 168–186. 10.1145/229473.229475\n\n\nBaggett, J. S., Driscoll, T. A., & Trefethen, L. N. (1995). A mostly linear model of transition to turbulence. Physics of Fluids, 7(4), 833–838. 10.1063/1.868606\n\n\nDzielski, J. E., & Driscoll, T. A. (1993). Error bound on the solution of a linear differential equation in Chebyshev series. International Journal of Systems Science, 24(7), 1317–1327. 10.1080/00207729308949562\n\n\nTrefethen, L. N., Trefethen, A. E., Reddy, S. C., & Driscoll, T. A. (1993). Hydrodynamic stability without eigenvalues. Science, 261(5121), 578–584. 10.1126/science.261.5121.578"
  },
  {
    "objectID": "publications.html#books",
    "href": "publications.html#books",
    "title": "Publications",
    "section": "",
    "text": "Driscoll, T. A., & Braun, R. J. (2018). Fundamentals of Numerical Computation. Society for Industrial and Applied Mathematics.\n\n\nTrefethen, L. N., Birkisson, Á., & Driscoll, T. A. (2017). Exploring ODEs. SIAM. 10.1137/1.9781611975161\n\n\nDriscoll, T. A., Hale, N., & Trefethen, L. N. (2014). Chebfun guide. Pafnuty Publications, Oxford.\n\n\nDriscoll, T. A. (2009). Learning MATLAB. Society for Industrial and Applied Mathematics.\n\n\nDriscoll, T. A., & Trefethen, L. N. (2002). Schwarz–Christoffel Mapping. Cambridge University Press."
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Publications",
    "section": "",
    "text": "Braun, R. J., Driscoll, T. A., & Begley, C. G. (2019). Mathematical Models of the Tear Film. In Ocular Fluid Dynamics: Anatomy, Physiology, Imaging Techniques, and Mathematical Modeling (pp. 387–432). Springer-Birkhauser. https://www.springer.com/gp/book/9783030258856\n\n\nDriscoll, T. A., & Fornberg, B. (2007). Pade-based interpretation and correction of the Gibbs phenomenon. In Advances in the Gibbs Phenomenon, ed. By A. Jerri. Sigma Sampling Publishing."
  },
  {
    "objectID": "publications.html#conference-papers",
    "href": "publications.html#conference-papers",
    "title": "Publications",
    "section": "",
    "text": "Braun, R. J., Driscoll, T., Begley, C., Situ, P., Tichenor, A., & Luke, R. (2023). Tear Breakup (TBU) Analysis with Fluorescence (FL) and Thermal (TH) imaging. Investigative Ophthalmology & Visual Science, 64(8), 186–186.\n\n\nBraun, R. J., Driscoll, T. A., Sinopoli, D., Dorsch, J., Hammond, C., Luke, R. A., & Begley, C. G. (2022). Data and Analysis from Tear Breakup (TBU) in Normal Subjects. Investigative Ophthalmology & Visual Science, 63(7), 3950-A0230.\n\n\nLuke, R. A., Braun, R. J., Driscoll, T., Sinopoli, D., Yawatkar, V., You, L., Phatak, A., & Begley, C. (2021). Fitting Simplified Models to Machine Learning-Identified Tear Film Breakup. Investigative Ophthalmology & Visual Science, 62(8), 1315–1315.\n\n\nBraun, R. J., Zhong, L., Driscoll, T. A., Begley, C. G., Antwi, D., & King-Smith, P. E. (2018, June). Models for Tear Break Up Dynamics and Imaging. 7th European Conference on Computational Fluid Dynamics.\n\n\nBrosch, J., Driscoll, T., & Braun, R. (2016). Simulation of Thin Film Equations on an Eye-Shaped Domain with Moving Boundary. APS March Meeting Abstracts.\n\n\nMaki, K., Henshaw, W., Barron, G., Chapp, D., Braun, R. J., & Driscoll, T. A. (2016). A theoretical investigation of the influence of a blink on the tear film dynamics. Investigative Ophthalmology & Visual Science, 57(12), 6173–6173.\n\n\nZhong, L., Ketelaar, C. F., Braun, R. J., Driscoll, T. A., King-Smith, P. E., & Begley, C. G. (2016). Mathematical Modeling of Glob-Driven Tear Film Breakup. Investigative Ophthalmology & Visual Science, 57.\n\n\nBraun, R., Li, L., Henshaw, W., Driscoll, T., & King-Smith, P. (2015). Solute Dynamics and Imaging in the Tear Film on an Eye-shaped Domain. APS Meeting Abstracts.\n\n\nKetelaar, C., Zhong, L., Braun, R., Driscoll, T., King-Smith, P., & Begley, C. (2015). Tear Film Dynamics Around a Rigid Model Blob. APS Meeting Abstracts.\n\n\nStapf, M., Braun, R., Begley, C., Driscoll, T., & King-Smith, P. E. (2015). Modeling Tear Film Evaporation and Breakup with Duplex Films. APS Meeting Abstracts.\n\n\nMcCulloch, M., Chen, L., Schleiniger, G., & Driscoll, T. (2014). DATA DRIVEN MATHEMATICAL MODELING OF THE SINGLE VENTRICLE ANATOMY AND PHYSIOLOGY. Critical Care Medicine, 42, A1414.\n\n\nLi, L., Braun, R., Driscoll, T., Henshaw, W., Banks, J., & King-Smith, P. (2013). Coupling Osmolarity Dynamics within Human Tear Film on an Eye-Shaped Domain. APS Meeting Abstracts.\n\n\nDeng, Q., Driscoll, T., & Braun, R. (2012). A Model Problem for Tear Film Distribution on a Moving Rectangular Domain. APS Meeting Abstracts.\n\n\nBraun, R., McFadden, J., Ranganathan, U., Driscoll, T., & King-Smith, E. (2008). Recent progress on Modeling the Human Tear Film. APS Division of Fluid Dynamics Meeting Abstracts.\n\n\nMaki, K. L., Braun, R., Driscoll, T., Heryudono, A., King-Smith, P., & Fast, P. (2007). A Overset Grid Method for Fourth Order Evolution Equations of Human Tear Film. APS Division of Fluid Dynamics Meeting Abstracts.\n\n\nDriscoll, T., & Rossi, L. (2003). Numerical examination of a model of thermo-acoustic instabilites in lean, pre-mixed combustors. APS Division of Fluid Dynamics Meeting Abstracts.\n\n\nDriscoll, T. A., & Fornberg, B. (1998). Uses of the Berenger PML in Pseudospectral Methods for Maxwell’s Equations. IUTAM Symposium on Computational Methods for Unbounded Domains, 95–102. https://doi.org/10.1007/978-94-015-9095-2_10\n\n\nTrefethen, L. N., & Driscoll, T. A. (1998). Schwarz-Christoffel mapping in the computer era. Proceedings of the International Congress of Mathematicians, 3, 533–542.\n\n\nWojcik, G., Fomberg, B., Waag, R., Carcione, L., Mould, J., Nikodym, L., & Driscoll, T. A. (1997). Pseudospectral methods for large-scale bioacoustic models. 1997 IEEE Ultrasonics Symposium Proceedings. An International Symposium (Cat. No. 97CH36118), 2, 1501–1506. https://doi.org/10.1109/ultsym.1997.661861\n\n\nDriscoll, T., & Dzielski, J. (1992). Computational efficiency of a functional expansion algorithm for linear quadratic optimal control. Proceedings of the 31st IEEE Conference on Decision and Control, 143–144. https://doi.org/10.1109/cdc.1992.371773"
  },
  {
    "objectID": "publications.html#software",
    "href": "publications.html#software",
    "title": "Publications",
    "section": "",
    "text": "Driscoll, T. A. (2023). RationalFunctionApproximation.jl: Rational function approximatoin in Julia (0.1.0) [Computer software]. Zenodo. 10.5281/ZENODO.8355790\n\n\nDriscoll, T. A. (2021). Cornea detection software package for Julia (0.1.0) [Computer software]. On GitHub\n\n\nDriscoll, T. A. (2019). ComplexRegions.jl: A Julia package for regions in the complex plane (Computer software). 10.5281/zenodo.3548866\n\n\nTrefethen, L., Hale, N., Platte, R., Driscoll, T., & Pachón, R. (2009). Chebfun version 2 (Computer software). chebfun.org\n\n\nDriscoll, T. A. (1994). Schwarz-Christoffel Toolbox for MATLAB (Computer software). [On GitHub[(https://github.com/tobydriscoll/sc-toolbox)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Chebfun\n\n\n\n\n\nComputing with functions instead of numbers.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIsospectral drums\n\n\n\n\n\nOn not hearing the shapes of drums.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMATLAB software\n\n\n\n\n\nContributions to the MATLAB ecosystem.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSchwarz–Christoffel Toolbox for MATLAB\n\n\n\n\n\nConformal mapping to regions bounded by polygons.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/trefethen-bau-lecture-1/index.html",
    "href": "post/trefethen-bau-lecture-1/index.html",
    "title": "Trefethen & Bau, Lecture 1",
    "section": "",
    "text": "Have a look at the MATLAB and Julia versions of the notebooks for this lecture.\nThe first disappointment in Julia comes right at the start: no magic command in Julia! Why not? I love demonstrating with magic square matrices:\n\nThey are instantly familiar or at least understandable to any level of mathematician.\nThey have integer entries and thus display compactly.\nYou can demonstrate sum, transpose, and diag naturally. And getting the “antidiagonal” sum is a nice exercise.\nThe even-sized ones are singular.\nThey have the memorable eigenvector \\([1,\\;1,\\; \\cdots\\; 1]\\).\n\nWhat a shame. I substitute random matrices, which sacrifices some reproducibility. At least the same code would work in MATLAB.\nAnother gotcha comes in line 2: if you create a vector with all integer entries, they are stored as integers. Famously, in MATLAB every number is a complex float unless you specifically declare it otherwise. Julia’s approach is standard in computer science, and there are excellent practical reasons for using it. Nor is it difficult to force Julia to use floats. Nevertheless the issue illustrates one of the subtle ways that MATLAB caters to those who are immersed in scientific computing, where pure integer results are rare.\nNext up is a simple but significant change in the syntax: square brackets [ ]for indexing of matrices and vectors. In MATLAB parentheses () serve both this purpose and to make function calls. Julia makes sense to me here. It makes this expression unambiguous:\nF( [1 2 3] )\nIn MATLAB this could be an access to the first three elements of an array F, or (the Julia meaning) a call to a function F with a vector passed as the first argument. I can’t think of a reason to have those different actions be expressed identically. I imagine the clash would complicate parsing code, but that’s an area I know nothing about.\nFrom here things settle down. Julia wants me to say 1im rather than 1i or 1j for the imaginary unit; fine. And I must remember to use spaces to separate columns in a matrix construction or concatenation. I often use commas for this in MATLAB. I’m a little confused because commas are used to create tuples in Julia, so I would have expected\nx = [ 1, 2, 3 ] \nto create (if anything) an array whose single element is the tuple 1,2,3. Instead I get a column vector with entries 1, 2, and 3, which, while a lot more useful, was a small surprise to this newbie.\nEnumerating the differences like this makes the experience sound far more frustrating than I found it to be. It’s more like driving a car with a different-feeling clutch than going from an automatic to a manual transmission.\nAnd I really appreciate the Jupyter notebooks! More on them versus the MATLAB Publisher and new Live Editor another time."
  },
  {
    "objectID": "post/data-science-data-science/index.html",
    "href": "post/data-science-data-science/index.html",
    "title": "Data science? Data science!",
    "section": "",
    "text": "I just received a copy of SIAM News on a dead tree. It features a piece by Chris Johnson and Hans de Sterck about “Data Science: What Is It and How Is It Taught?” As usual in these articles, I find the specifics more interesting than the generalities of a panel discussion. I really liked this bit about the new program in Computational Modeling and Data Analytics at Virginia Tech:\n\nIn a sense, creating such a program offers the opportunity to rethink curricula on classical topics like calculus that have at many institutions not seen substantial change throughout most of the past century.\n\nThis! Well outside the context of data science, too.\nI’m so sick of teaching calculus as though it were still 1960. Not that calculus has changed, of course, but what we need from it has been utterly transformed. In the age of computing, knowledge of calculus is more useful for posing the right questions—as opposed to getting answers to mindless exercises that can be done in seconds on Wolfram Alpha. Don’t even get me started on teaching series convergence tests to engineering freshmen.\nAs far as how to teach data science…let me figure out how to learn it, first. I’m intrigued by this repository as a start."
  },
  {
    "objectID": "post/it-already-has-happenend-here/index.html",
    "href": "post/it-already-has-happenend-here/index.html",
    "title": "It already has happened here",
    "section": "",
    "text": "I’ve just finished one of the most remarkable fiction reading experiences I’ve had in quite some time: It Can’t Happen Here, by Sinclair Lewis.\nICHH is a satirical novel written and set in 1935 America. It describes the rise of a populist dictatorship modeled closely along the rise of the Nazis in Germany.(Lewis’ wife, Dorothy Thompson, was the first American journalist expelled from Nazi Germany and was clearly responsible for much of the shape of the book.)\nThis was near the height of the Depression, and FDR’s New Deal was controversial and, at that point unsuccessful (or not yet successful, if you want to look at it that way). There were multiple signs of unrest and populism, not least of which was Huey Long, then governor of Louisiana and apparently plotting to hijack the Democratic party to get elected President in 1936 or 1940. (Long was assassinated in 1935, as the book was being finished.)\nICHH is not great as a novel. The characters are essentially allegorical, and the plot barely needs them. The core of the book is as a dystopian speculation about the near future of the USA. Lewis himself called it “propaganda for American democracy.” As such, its relevance to 2016 is astonishing. I could pick out dozens of quotes. Here are just three extended ones.\n\n…what burns me up [isn’t] that old soap-boxer’s old chestnut about how one tenth of one percent of the population at the top have an aggregate income equal to 42 percent at the bottom….[It’s] the fact that even before this Depression, in what you folks called prosperous times, 7 per cent of all the families in the country earned $500 a year or less—remember, those weren’t the unemployed, on relief; those were the guys that still had the honor of doing honest labor.\n\n\nThe most confusing thing about the campaign of 1936 was the relationship of the two leading parties. Old-Guard Republicans were complaining that their proud party was begging for office, hat in hand; veteran Democrats that their traditional Covered Wagons were jammed with college professors, city slickers, and yachtsmen.\n\n\nMost Americans had learned in school that God had supplanted the Jews as chosen people by the Americans, and this time done the job much better, so that we were the richest, kindest, and cleverest nation living; that depressions were but passing headaches and that labor unions…must not set up an ugly class struggle by combining politically; that, though foreigners tried to make a bogus mystery of them, politics were really so simple that any village attorney or any clerk in the office of a metropolitan sheriff was quite adequately trained for them; and that if John D. Rockefeller or Henry Ford had set his mind to it, he could have become the most distinguished statesman, composer, physicist, or poet in the land.\n\nSo much is in the book: the rural/urban dynamic, the American disdain for and distrust of intellectualism, the cluelessness of intellectuals, racism, anti-Semitism, antifeminism, the lust for a “ringmaster-revolutionist”, the contrast between a boring, calculating candidate and a hot populist who draws big rallies, the failure of newspapers (i.e. the media in 1935), the use of radio for disintermediation (i.e., Twitter), the belief by the banking establishment that things would soon moderate and work in their favor, etc. Lewis also spells out the horrors of a concentration camp—familiar ground to us today, but sensational to much of the public in 1935.\nIn 1935 the bogeyman was communism, not Islam, and the threat seems to be the extreme left, not the right. But that hardly matters. What ICHH made so clear to me is how America, through its history, culture, and politics, is a host susceptible to a certain pattern of symptoms. And maybe it’s not just America, and not just liberal democracy, but literally part of our DNA.\nI’m quickly going over in my head in political science here. My reaction to the book is complicated. I recommend that you pick up a copy and at least get through the fictional election, though it’s neither easy nor fun to read.\nThe book was apparently adapted to a smash hit play in 1937. I would be surprised if it doesn’t undergo a revival in the near future."
  },
  {
    "objectID": "post/grades-and-motivation/index.html",
    "href": "post/grades-and-motivation/index.html",
    "title": "Grades and motivation",
    "section": "",
    "text": "Grading is weird in so many ways. In the U.S. system, we report a “letter” grade that is basically an integer from 0 to 10 or so. This value appears on the student’s transcript without comment or context, which is an inherently meaningless way to present any data.\nBut the raw value itself isn’t well defined anyway. When I give a student a C+ in calculus, does it mean that she mastered about 75% of the major topics in the course? Or does it mean that she understands about 3/4 of what is going on in any particular topic? Which of these is preferable? Would a C+ in bicycle riding be enough of a prerequisite to learn how to ride a motorcycle?\nThe closest analog to grades in the real world that I can think of is the annual or quarterly performance review. There are doubts being expressed about these too. In a piece on Bloomberg Business, long-time management consultant Aubrey Daniels says, “It’s a sadistic process for what purpose I don’t know….Think of a sports team: A coach doesn’t wait until the end of a season to give his players feedback.” So, we’re coming back around to continuous assessment.\nYet the form of the assessment needs to change too.\nWhat motivates people in the workplace? For one thing, being recognized for their successes. In math we tend to view perfection as the standard, and everything that falls short on homework or exams earns deductions. This is a notably dismal and discouraging viewpoint for learners. It emphasizes the negativity of errors both large and small. When you compare the (hopefully!) flawless and polished solutions on the answer key with your own stumbling attempts, how could you feel anything but foolish? Where is the upside?\nAnother thing that motivates us in the real world is a chance to fix our failures. If you’ve scored badly on two midterms in a calculus course, you’re probably wise to invest your effort elsewhere. The chances of pulling yourself out of the muck are small, in part because averages are heartless and have perfect memory. I always have disdained grading methods that forgive early bad scores or give “extra” credit chances, but I have to admit that a system that makes recovery from a bad start seem impossible is no way to maintain motivation.\nI don’t have answers yet, but I’m thinking about some things. More later."
  },
  {
    "objectID": "post/matlab-vs.-julia-vs.-python/index.html",
    "href": "post/matlab-vs.-julia-vs.-python/index.html",
    "title": "Matlab vs. Julia vs. Python",
    "section": "",
    "text": "I’ve used MATLAB for over 25 years. (And before that, I even used MATRIXx, a late, unlamented attempt at a spinoff, or maybe a ripoff.) It’s not the first language I learned to program in, but it’s the one that I came of age with mathematically. Knowing MATLAB has been very good to my career.\nHowever, it’s impossible to ignore the rise of Python in scientific computing. MathWorks must feel the same way: not only did they add the ability to call Python directly from within MATLAB, but they’ve adopted borrowed some of its language features, such as more aggressive broadcasting for operands of binary operators.\nIt’s reached a point where I have been questioning my continued use of MATLAB in both research and teaching. Yet so much comes easily to me there, and I have so much invested in materials for it, that it was hard to rally motivation to really learn something new.\nEnter the MATLAB-based textbook I’ve co-written for introductory computational math. The book has over 40 functions and 160 computational examples, and it covers what I think is a thorough grounding in the use of MATLAB for numerical scientific computing. So partly as self-improvement, and partly to increase the usefulness of the book, I set out this year to translate the codes into Julia and Python. This experience has led me to a particular perspective on the three languages in relation to scientific computing, which I attempt to capture below.\nI will mostly set aside the issues of cost and openness. MATLAB, unlike Python and Julia, is neither beer-free nor speech-free. This is indeed a huge distinction—for some, a dispositive one–but I want to consider the technical merits. For many years, MATLAB was well beyond any free product in a number of highly useful ways, and if you wanted to be productive, then cost be damned. It’s a separate consideration from the Platonic appeal of a language and ecosystem.\nWhen you do set cost aside, a useful frame for a lot of the differences among these languages lies in their origins. MATLAB, the oldest of the efforts, prioritized math, particularly numerically oriented math. Python, which began in earnest in the late 1980s, made computer science its central focus. Julia, which began in 2009, set out to strike more of a balance between these sides."
  },
  {
    "objectID": "post/matlab-vs.-julia-vs.-python/index.html#matlab",
    "href": "post/matlab-vs.-julia-vs.-python/index.html#matlab",
    "title": "Matlab vs. Julia vs. Python",
    "section": "MATLAB",
    "text": "MATLAB\nOriginally, every value in MATLAB was an array of double-precision floating point numbers. Both aspects of this choice, arrays and floating point, were inspired design decisions.\nThe IEEE 754 standard for floating point wasn’t even adopted until 1985, and memory was measured in K, not G. Floating point doubles weren’t the most efficient way to represent characters or integers, but they were what scientists, engineers, and, increasingly, mathematicians wanted to use most of the time. Furthermore, variables did not have to declared and memory did not have to be explicitly allocated. Letting the computer handle those tasks, and whisking data types out of the way, freed up your brain to think about the algorithms that would operate on the data.\nArrays were important because numerical algorithms in linear algebra were coming into their own, in the form of LINPACK and EISPACK. But accessing them with the standard bearer in scientific computing, FORTRAN 77, was a multistep process that involved declaring variables, calling cryptically named routines, compiling code, and then examining data and output files. Writing a matrix multiplication as A*B and getting the answer printed out right away was a game-changer.\nMATLAB also made graphics easy and far more accessible. No fiddly machine-specific libraries with low-level calls, just plot(x,y) and you saw pretty much what anyone else with MATLAB would see. There were more innovations, like baked-in complex numbers, sparse matrices, tools to build cross-platform graphical user interfaces, and a leading-edge suite of ODE solvers, that made MATLAB the place to do scientific computing at the speed of thought.\nHowever, design that was ideal for interactive computations, even lengthy ones, was not always conducive to writing good and performant software. Moving data around between many functions required juggling lots of variables and frequent consultation of documentation about input and output arguments. One function per disk file in a flat namespace was refreshingly simple for a small project, but a headache for a large one. Certain programming patterns (vectorization, memory preallocation) had to be applied if you wanted to avoid speed bottlenecks. Scientific computing was now being applied to far more domains, with vast amounts of different native types of data. Etc.\nMathWorks responded by continuing to innovate within MATLAB: inline functions, nested functions, variable closures, numerous data types, object-oriented features, unit testing frameworks, and on and on. Each innovation was probably the solution to an important problem. But the accumulation of 40 years of these changes has had the side effect of eroding the simplicity and unity of concept. In 2009 I wrote a book that pretty well covered what I considered the essentials of MATLAB in less than 100 pages. As far as I know, all of those things are still available. But you need to know a lot more now to call yourself proficient."
  },
  {
    "objectID": "post/matlab-vs.-julia-vs.-python/index.html#python",
    "href": "post/matlab-vs.-julia-vs.-python/index.html#python",
    "title": "Matlab vs. Julia vs. Python",
    "section": "Python",
    "text": "Python\nIn a sense the history of Python seems to be almost a mirror image of MATLAB’s. Both featured an interactive command line (now widely called a REPL, for “read-eval-print loop”) and freedom from variable declarations and compilation. But MATLAB was created as a playground for numerical analysts, while Python was created with hackers in mind. Each then grew toward the other audience through revisions and extensions.\nTo my eye, Python still lacks mathematical appeal. You have ugliness and small annoyances such as ** instead of ^, @ for matrix multiplication (a recent innovation!), a shape rather than size of a matrix, row-oriented storage, etc. If you believe that V.conj().T@D**3@V is an elegant way to write \\(V^*D^3V\\), then you may need to see a doctor. And there’s zero-indexing (as opposed to indexes that start at 1). I’ve read the arguments, and I don’t find them decisive. It’s clearly a matter of preference—the stuff of online holy wars—because you can cite ungainly examples for either convention. What I find decisive is that we have decades of mathematical practice indexing vectors and matrices from one, and most pseudocode makes that assumption.\nBeyond the petty annoyances, I find the Python+NumPy+SciPy ecosystem to be kludgy and inconsistent. Exhibit A is the fact that despite the language being rather devoted to object orientation, there exists a matrix class, and yet its use is discouraged and will be deprecated. Perhaps MATLAB has simply corrupted me, but I find matrices to be an important enough type of object to keep around and promote. Isn’t a major selling point of OOP that you can have * do different things for arrays and matrices? There are many other infelicities in this regard. (Why do I need a command called spsolve? Can’t I just call solve on a sparse matrix? And on and on.)\nThere are also places where the numerical ecosystem looks a little thin to me. For instance, the quadrature and ODE solvers look like a minimal set in 2019. AFAICT there are no methods for DAEs, DDEs, symplectic solvers, or implicit solvers that allow inner Krylov iterations. Have a look at the references for these functions; they’re mostly 30 or more years old—still good, but very far from complete. The Matplotlib package is an amazing piece of work, and for a while it looked better than MATLAB, but I find it quite lacking in 3D still.\nSome experts argue that there are deep reasons why Python code struggles to keep up in execution speed with compiled languages. I’m amused by the results of searching for “python is too slow”. The champions of Python make a lot of the same arguments/apologies that folks did for MATLAB back in the day. That doesn’t mean they’re wrong, but there’s more than just a perception problem.\nI think I get why Python has been so exciting to many people in scientific computing. It has a some MATLAB-ish syntax and power, available from a REPL. It has great tools around it and plays well with other languages and areas of computing. It offered that at no cost and with much better long-term reproducibility. Clearly, it works well for a lot of people who probably see little reason to change.\nBut for the things I know how to do in scientific computing, Python feels much more like a chore to learn and use than I’m used to. We won’t know for a while whether it will continue to sweep through the community or has already neared its peak. I have no special predictive powers, but I’m bearish."
  },
  {
    "objectID": "post/matlab-vs.-julia-vs.-python/index.html#julia",
    "href": "post/matlab-vs.-julia-vs.-python/index.html#julia",
    "title": "Matlab vs. Julia vs. Python",
    "section": "Julia",
    "text": "Julia\nJulia has the advantages and disadvantages of being a latecomer. I applaud the Julia creators for thinking they could do better:\n\nWe want a language that’s open source, with a liberal license. We want the speed of C with the dynamism of Ruby. We want a language that’s homoiconic, with true macros like Lisp, but with obvious, familiar mathematical notation like Matlab. We want something as usable for general programming as Python, as easy for statistics as R, as natural for string processing as Perl, as powerful for linear algebra as Matlab, as good at gluing programs together as the shell. Something that is dirt simple to learn, yet keeps the most serious hackers happy. We want it interactive and we want it compiled.\n\nTo a great extent, I believe they have succeeded. Late along the road to version 1.0 they seemed to downplay the REPL a bit, and there were some almost gratuitous lurches away from MATLAB. (How exactly is LinRange better than linspace?) These are quibbles, though.\nThis is the first language I’ve used that goes beyond ASCII. I still get an unreasonable amount of satisfaction from using variables like ϕ and operators like ≈. It’s more than cosmetic; being able to look more like the mathematical expressions we write is real plus, though it does complicate teaching and documentation a bit.\nWorking in Julia exposed to me that I picked up some programming habits because of MATLAB’s choices, not inherent superiority. Vectorization is not natural for many things. It’s eye-opening to find in Julia that you can vectorize any function just by adding a dot to its name. Constructing a matrix through a comprehension makes nested loops (or meshgrid tricks) look like buggy whips in comparison, and avoiding a matrix altogether via a generator for a simple summation feels like getting something for nothing. (I’m aware that Python has similar language features.)\nThe big feature of multiple dispatch makes some things a lot easier and clearer than object orientation does. For instance, suppose you have Wall and Ball classes in a traditional object-oriented language. Which class should detect a collision of a Ball with a Wall? Or do you need a Room class to play referee? These kinds of questions can drive me to distraction. With multiple dispatch, data is packaged into object types, but the methods that operate on data are not bound to a class. So\nfunction detect_collision(B::Ball,W::Wall)\nknows about the types but is defined independently of them. It’s taken quite a bit of programming for me to appreciate how interesting and potentially important the notion of multiple dispatch is for extending the language.\nThe numerical ecosystem has been evolving rapidly. My number one example is DifferentialEquations.jl, written by the amazing Chris Rackauckas. If this software doesn’t win the Wilkinson prize soon, the system is broken. Just go to the site and prepare to be converted.\nI have yet to see the big speed gains over MATLAB that Julia promises. Partly that’s my relative inexperience and the kinds of tasks I do, but it’s also partly because MathWorks has done an incredible job automatically optimizing code. It’s not an aspect of coding that I focus on most of the time, anyway.\nProgramming in Julia has taken me a while to feel comfortable with (perhaps I’m just getting old and crystallized). It makes me think about data types more than I would want, and there’s always the sneaking suspicion that I’ve missed the Right Way to do something. But for daily use, I’m about as likely to turn to Julia as MATLAB now."
  },
  {
    "objectID": "post/matlab-vs.-julia-vs.-python/index.html#the-bottom-line",
    "href": "post/matlab-vs.-julia-vs.-python/index.html#the-bottom-line",
    "title": "Matlab vs. Julia vs. Python",
    "section": "The bottom line",
    "text": "The bottom line\nMATLAB is the corporate solution, especially for engineering. It’s probably still the easiest to learn for basic numerical tasks. Meticulous documentation and decades of contributed learning tools definitely matter.\nMATLAB is the BMW sedan of the scientific computing world. It’s expensive, and that’s before you start talking about accessories (toolboxes). You’re paying for a rock-solid, smooth performance and service. It also attracts a disproportionate amount of hate.\nPython is a Ford pickup. It’s ubiquitous and beloved by many (in the USA). It can do everything you want, and it’s built to do some things that other vehicles can’t. Chances are you’re going to want to borrow one now and then. But it doesn’t offer a great pure driving experience.\nJulia is a Tesla. It’s built with an audacious goal of changing the future, and it might. It may also become just a footnote. But in the meantime you’ll get where you are going in style, and with power to spare."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lectures-24-29-eigenvalue-stuff/index.html",
    "href": "post/trefethen-bau-matlab-julia-lectures-24-29-eigenvalue-stuff/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia, Lectures 24-29: Eigenvalue stuff",
    "section": "",
    "text": "Part V of T&B is on dense methods for eigenvalue and singular value problems. For my course, this is the part of the text that I condense most severely. In part that’s due to the need to cover unconstrained nonlinear solving and optimization stuff later on. But I also find that this is the least compelling part of the text for my purposes.\nIt’s heavily weighted toward the hermitian case. That’s the cleanest situation, so I see the rationale. But it’s pretty surprising that the lead author of Spectra and Pseudospectra mentions eigenvalue conditioning and sensitivity only in a single exercise! (The exercises not in the lecture named “Eigenvalue problems,” nor the one named “Overview of eigenvalue algorithms.” It’s under “Reduction to Hessenberg or tridiagonal form.”) In contrast with the tone of earlier parts of the book, one could study the methods of these sections thoroughly and yet not appreciate when the answers are inaccurate, or possibly irrelevant. Because I took this course from Trefethen at a crucial time in the development of his thinking on the subject, my perception of the issues behind computing eigenvalues is quite different from what the text itself conveys.\n(EDIT: If I had but read a few sections more before writing the above, I would have recalled that there is discussion about this in Lecture 34, under “A Note of Caution: Nonnormality.” It’s all laid out in clear language, so mea culpa. The ordering still feels a little awkward. I’ll probably have a half or full class period just on nonnormality.)\nSo. In my class I touched on 24-29, and you can find my related MATLAB notebooks and Julia notebooks on them. (I’ve given up on using Gists for these. The web interface can’t seem to handle having a lot of notebooks in one Gist, the rendering is slow, and I see no advantage for me beyond static HTML.) They’re a little rough in places, as it’s been challenging to keep up the pace.\nThere aren’t big MATLAB/Julia issues to report. If anything, I think Julia has cleaned up and rationalized some of the quirkiness of the MATLAB versions. In MATLAB, one uses eig for everything. The results depend on the number of output arguments.\n&gt;&gt; A = hilb(3);\n&gt;&gt; lambda = eig(A)\nlambda =\n    0.0027\n    0.1223\n    1.4083\n&gt;&gt; [X,D] = eig(A)\nX =\n    -0.1277    0.5474    0.8270\n    0.7137   -0.5283    0.4599\n    -0.6887   -0.6490    0.3233\nD =\n    0.0027         0         0\n          0    0.1223         0\n          0         0    1.4083\nIt’s a bit awkward that the position of the eigenvalue output changes, and that it’s a vector in one case and a matrix in the other. And the difference goes beyond cosmetics: the calculation can be significantly faster if eigenvectors are not required. Julia gives you three variants, so you can retrieve exactly what you want.\njulia&gt; A = [1/(i+j) for i=1:3, j=1:3];\njulia&gt; (λ,X) = eig(A)\n([0.000646659,0.0409049,0.875115],\n[0.19925 -0.638787 -0.743136; ...  -0.411255])\n\njulia&gt; λ = eigvals(A)\n3-element Array{Float64,1}:\n  0.000646659\n  0.0409049\n  0.875115\n\njulia&gt; D = eigvecs(A)\n3×3 Array{Float64,2}:\n  0.19925   -0.638787  -0.743136\n  -0.761278   0.376612  -0.527843\n  0.617053   0.670906  -0.411255\nYou even have eigmax and eigmin when the spectrum is real. One thing neither language gives you is an easy way to specify a sort order for the results. In MATLAB, for instance, one ends up doing things like:\n&gt;&gt; [X,D] = eig(A);\n&gt;&gt; lambda = diag(D);\n&gt;&gt; [~,idx] = sort(real(lambda));\n&gt;&gt; X = X(:,idx);  lambda = lambda(idx)\nlambda =\n   -2.1898 + 1.4354i\n   -2.1898 - 1.4354i\n    0.0301 + 0.6095i\n    0.0301 - 0.6095i\n    1.2276 + 2.2020i\n    1.2276 - 2.2020i\n    1.8278 + 0.0000i\nMeh. It’s not a lot better in Julia, as far as I can tell.\njulia&gt; A = randn(7,7);\njulia&gt; (λ,X) = eig(A);\njulia&gt; idx = sortperm(real(λ));\njulia&gt; X = X[:,idx];  λ = λ[idx]\n7-element Array{Complex{Float64},1}:\n  -3.38359+0.0im\n  -2.33084+0.233909im\n  -2.33084-0.233909im\n  0.415007+0.0im\n  1.03098+0.0im\n  1.11426+2.34596im\n  1.11426-2.34596im\nAltogether, Julia is feeling less like a foreign country and more like a province. Sometimes I even remember to use square brackets on the first try."
  },
  {
    "objectID": "post/bio-breakfast/index.html",
    "href": "post/bio-breakfast/index.html",
    "title": "Bio Breakfast",
    "section": "",
    "text": "So here I am at the Delaware Bio Breakfast. Nobody is more surprised than I! Making a career out of math seems like an odd path to trying to improve human health, but here I sit."
  },
  {
    "objectID": "post/a-retrospective-look-at-college-math/index.html",
    "href": "post/a-retrospective-look-at-college-math/index.html",
    "title": "A retrospective look at college math",
    "section": "",
    "text": "I recommend the post What I Wish I Had Learned More About in College Mathematics, written by Sabrina Schmidt, a former math undergrad at Vassar who now works as a data manager. My favorite quote:\n\nI wish that I had been introduced earlier and more often to applications, as they would have provided me with a better idea of potential areas of specialization after graduation.\n\nShe goes on to mention PageRank (which I usually cover in my numerical computation courses) as an application of linear algebra, and e-commerce as an application of number theory. She also has other STEM courses, statistics, and computer science on her wish list for her former self.\nGood read."
  },
  {
    "objectID": "post/trefethen-bau-via-matlab-and-julia/index.html",
    "href": "post/trefethen-bau-via-matlab-and-julia/index.html",
    "title": "Trefethen & Bau, via MATLAB and Julia",
    "section": "",
    "text": "This semester I’m teaching MATH 612, which is numerical linear and nonlinear algebra for grad students. Linear algebra dominates the course, and for that I’m following the now classic textbook by Trefethen & Bau. This book has real meaning to me because I learned the subject from Nick Trefethen at Cornell, just a year or two before the book was written. It’s when numerical analysis became an appealing subject to me.\nThat course is also when I started to learn MATLAB. I’ve been using MATLAB for over 20 years and I’m damn good at it. I’ve written a book that teaches it, and another book largely based on asoftware package I wrote for conformal mapping, and I was an early and key contributor to the Chebfun project. I even dominated a game of MATLAB Jeopardy as a grad student at the 1995 MATLAB Conference (when version 4.2 of MATLAB ruled the Earth).\n(It isn’t quite contemporary, but the 1996 home page for the Cornell Center for Applied Mathematics has a banner graphic created in MATLAB—by yours truly.)\nThe tl;dr is that MATLAB has dominated my professional life since that course. It’s still a great tool to use for that course, too—in my mind, learning the theory and learning the numerics are inextricable. In the context of computing, it’s incredible to have a 25-year winning streak!\nBut while the pedagogical value remains as high as ever, MATLAB is a smaller part of the “desktop scientific computing” landscape than it was. It’s still a behemoth, but there are more good options than ever.  For some time I have felt neglectful toward options that are similar but different, namely SciPy and Julia. I’ve picked up bits and pieces of them, but not enough to do any serious work.\nThus I’ve decided to learn Julia the same way I did MATLAB: by using it as we cover elementary numerical linear algebra. The students will still get MATLAB, but I’ll be doing Julia in parallel. For each lecture (chapter) of Trefethen & Bau, I’ll make two Jupyter notebooks with identical text and two versions of the codes. I’m not rewriting T&B, just trying to illustrate some of the concrete ideas and conclusions in each lecture. I’m sure my early Julia efforts will be cringeworthy to the cognoscenti, but just as with learning a human language, you have to risk sounding stupid for a while in order to start sounding less stupid. If I can keep up the pace, I’ll blog about what I learn about porting to Julia with each new notebook."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lecture-2/index.html",
    "href": "post/trefethen-bau-matlab-julia-lecture-2/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia, Lecture 2",
    "section": "",
    "text": "Here are the matlab and julia notebooks.\nTwo things stood out this time. First, consider the following snippet.\nu = [ 4; -1; 2+2im ]\nv = [ -1; 1im; 1 ]\nprintln(\"dot(u,v) gives \", dot(u,v))\nprintln(\"u'*v gives \",u'*v)\nThe result is\ndot(u,v) gives -2 - 3im\nu'*v gives Complex{Int64}[-2 - 3im]\nUnlike in MATLAB, a scalar is not the same thing as a 1-by-1 matrix. This has consequences. The code (u'*v)*eye(3) throws a dimension mismatch error, while the equivalent with dot is fine. In the strict sense this is correct, and I suppose Julia made a decision to be strict in contrast to MATLAB’s typical laxity. The price is that little bump introduced into a transition that is normally seamless in the minds of users and programmers 99% of the time.\nThe other difference is in style more than anything else. Compare MATLAB’s\n[Q,~] = qr(A);\nto Julia’s\nQ = qr(A)[1]\nJulia’s version would be easier if you wanted to extract the \\(n\\)th output, where \\(n\\) is a variable, though you could manage it in MATLAB with cells. I’m not sure how common that situation is. Also, it could be a surprise in MATLAB that\nQ=qr(A)\ndoes not do the same thing, because the content and meaning of the outputs depend on the number of outputs.\nA distinction for QR factorization in particular in the two languages is that MATLAB returns the full version by default, while Julia defaults to the skinny form. The latter is nice because an unsuspecting student (or professor) who calls qr(A) in MATLAB for a really tall matrix might as well kill the process and restart MATLAB Julia makes you do something extra to get the memory-dangerous version."
  },
  {
    "objectID": "post/new-look-new-tech/index.html",
    "href": "post/new-look-new-tech/index.html",
    "title": "New look, new tech",
    "section": "",
    "text": "At long last, I’ve refreshed the look for this site. Previously it was based on a “Metro UI” style for HTML, which looked nice to me at the time. Actually it still looks pretty nice, but it was named for the Metro design introduced with Windows 8, which tells you that it wasn’t exactly a modern look.\nMore importantly to me, I’ve ditched writing raw HTML for creating a site using the Hugo content creation system. It’s such a headache trying to maintain menus, headers, footers, etc. in raw HTML that updating the old site became too painful to contemplate. On the blogging side, I had been using Wordpress, which I found incredibly slow, wonky, and frustrating. Including snippets of code or math was way too hard, even with extension modules loaded that were supposed to handle it for me, and the results were inconsistent.\nIn Hugo you create content within a folder hierarchy that holds plain text files. These can be in HTML as necessary, but the real workhorse is Markdown, which lets you enter content with simply annotated text. As with all markup languages, the idea is to label the structure of the content and let the formatting be specified separately. Unlike HTML, though, the syntax is concise and easy to type, though of course limited to just the most important element types.\nHugo then converts the Markdown sources into a working site. It’s static, not dynamic, meaning that all the pages are created at once rather than in response to a browser request. This keeps it fast and lean, which is appropriate for a modest personal site.\nOne strength of Hugo is that it supports tons of themes. Of these, the Academic theme quickly jumped out at me. It’s written to provide what a typical personal page in academia requires. To be honest, I feel as though it took me too long to grok how to use the theme, and Hugo in general. You can see the sources for this site if you want to get a feeling by example, which is what I recommend.\nThe last piece was to generate a screen-native CV. I’ve been getting increasingly frustrated with PDF online. It was a huge step forward when it was introduced in 1993, when hardcopy was still the norm. But while we haven’t gone fully paperless, lots of systems such as job applications or promotion documentation simply can’t function any way but fully online now. Yet as pointed out in this great Atlantic piece on the obsolescence of the scientific paper, PDF is intrinsically a simulation of a piece of paper. Because it prioritizes display over structure, it’s often hard to extract information from, and it can’t do something as fundamental as wrap lines of text to fit a screen width.\nFor decades I maintained my CV in LaTeX, with PDF as the output format. This was okay, but getting output from LaTeX in a truly screen-friendly form (basically, HTML) Is. Not. Fun. While the first 80% is easy, the last 20% can be excruciating. Fortunately, with a little regexp replacement, I was able to convert the source to Markdown with manageable effort. Then, thanks to markdown-cv, I’m able to render this as a great-looking document. Yes, I make a PDF version available still—by printing the “real” version from a browser.\nMy dream had been to have the publications part of the CV auto-generated from the data in my publications website section. If I were a more dedicated Hugo hacker, I’m sure I could do it, but I threw in the towel. Besides, there’s something to be said for making sure the CV really looks the way you expect it, and the work of double-entering the publications isn’t that odious.\nThis being summer, I haven’t yet migrated whatever teaching content from the old site I feel is still worthwhile. But it feels nice to have my online presence up to snuff once more. I hope it motivates me to renew a dedication to blogging."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lecture-19-stability-of-least-squares/index.html",
    "href": "post/trefethen-bau-matlab-julia-lecture-19-stability-of-least-squares/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia: Lecture 19, Stability of least squares",
    "section": "",
    "text": "Here are the notebooks in MATLAB and Julia.\nThe new wrinkle in these codes is extended precision. In MATLAB you need to have the Symbolic Math toolbox to do this in the form of vpa. In Julia, you have to use version 0.5 or (presumably) later, which had a surprising side effect I’ll get to below.\nThe reason for extended precision is that this lecture presents experiments on the accuracy of different algorithms for linear least squares problems. In order to demonstrate this on a fairly ill conditioned problem, the answer is supposed to be computed in extended precision, yielding a normalization constant that sets the desired quantity to be 1 for at least 16 significant digits.\nThe least squares problem comes from fitting exp(sin(4t)) to a polynomial of degree 14. I see two ways to define how extended precision is to be used. Option (1) is to form the matrix \\(A\\) and the vector \\(b\\) in double precision, then solve the least squares problem with them, but in extended precision. Option (2) is to build in extended precision from the beginning of the problem, creating \\(A\\) and \\(b\\) that differ in the extended digits. I was first attracted to option (1), but option (2) has the clear advantage that the result should be independent of machine and language, whereas in the other case the data could be rounded or computed differently to double precision.\nHere’s how this looks in MATLAB.\nt = vpa(0:m-1,64)'/vpa(m-1,64);  % 64 sig. digits!\nA = t.^0;\nfor j = 1:14, A=[A,t.*A(:,j)]; end\nb = exp(sin(4*t));\n\n[Q,R] = qr(A,0);     % Householder QR\nx1 = R\\ (Q'*b);\n[Q,R] = mgs([A b]);  % Gram-Schmidt QR\nx2 = R(1:15,1:15) \\ R(1:15,16);\nHere are the outputs for the last element of x in the four methods:\n2006.7874531048518338761038143559\n2006.7874531048518338761038143553\n2006.7874531048518338766907539159\n2006.7874531048518338761038143555\nIt’s not a problem that the third result disagrees in the last 10 or so digits, since that’s an unstable method.\nHere’s how it went in Julia.\nsetprecision(BigFloat,128);  # use 128-bit floats\nt = convert(Array{BigFloat},collect(0:m-1))/convert(BigFloat,m-1);\nA = [t[i].^j for i=1:m, j=0:n-1];\nb = exp(sin(4*t));\n\n(Q,R) = qr(A);\nx1 = R\\ (Q'*b);\n(Q,R) = mgs([A b]);\nx2 = R[1:15,1:15] \\ R[1:15,16];\nx3 = (A'*A)$$A'*b);\nx4 = A\\b;\nThat first line isn’t pretty, but after that it’s quite natural. I found Juila’s extended precision to be fast compared to MATLAB’s. The results:\n2.006787453104851833876103814338068195207e+03\n2.006787453104851833876103814355358077263e+03\n2.006787453104851834342923924263804001505e+03\n2.006787453104851833876103814376793404332e+03\nThese are the same up to the last couple of digits of MATLAB’s answer. Unfortunately, my values don’t agree with what’s in T&B, which is 2006.787453080206. The text doesn’t say much about how this was done, so it’s impossible for me to say why.\nI probably don’t pay enough attention to extended precision. I know some people in the radial basis function community who use it to overcome the very poor conditioning of those bases. They seem quite happy with it. It’s always felt like cheating to me, but that’s hardly a rational argument.\nAbove I said that there was an unexpected side effect related to my using extended precision in Julia. I discovered that (a) it became available in base Julia in version 0.5 and (b) the homebrew Julia I had installed was version 0.4.3, even though 0.5 had apparently been out for a while. Upon upgrading, I found that my MGS routine throwing an error! The offending line was\nA[:,j+1:n] -= Q[:,j]*R[j,j+1:n];\nThe issue is that now both of the references on the right-hand side are vectors, which have only one dimension. Therefore the implied outer product is considered undefined. I had to switch to\nA[:,j+1:n] -= Q[:,j:j]*R[j:j,j+1:n];\nBecause j:j is a range, not a scalar, the submatrix references are two-dimensional matrices with appropriate singleton dimensions, so the outer product proceeds.\nI’m not sure how to feel about this. It’s disturbing to extract a row of a matrix and get an object without a row shape. In fact you can even say it’s got a column shape, because you are allowed to transpose it into a 1-by-n matrix! On the other hand, there are consistent rules governing the indexing, and 0D, 1D, and 2D extractions are all possible. I’m starting to think that the true problem is that I learned and conceptualize linear algebra in a way that works up to dimension 2 but contains some implied hacks that break multilinear algebra. I wish I knew this stuff better."
  },
  {
    "objectID": "post/flipping-experiences/index.html",
    "href": "post/flipping-experiences/index.html",
    "title": "Flipping experiences",
    "section": "",
    "text": "In June I attended a MathWorks faculty research summit in Boston. The idea was to bring together academics and industry reps. As one of the very few non-engineers, it didn’t give me much fodder for research. But there was a parallel session for educators with a couple of crossover sessions. I spoke in one of those about what I have learned from flipping the classroom in my numerical computation course. You can view the slides online.\nThe executive summary: Connecting with students in person is the main thing separating me from a MOOC. Major challenges in this particular course are the wide variety of backgrounds of the students, and material that spans advanced mathematics as well as some skill with computer coding. My goal is to teach how to bridge the two, to become fluent enough in both types of thinking to at least know when to go the experts and what to ask. Flipping lets students have time to fill in soft spots in their knowledge while absorbing new material, and to get help from me and their peers while they wrestle with putting new ideas into practice. I have no data on whether they do better in this style of class. (They believe they do a bit better, though like it a bit less.) But I know that I am more engaged, and so I’m giving them the best that I have to offer."
  },
  {
    "objectID": "post/showall/index.html",
    "href": "post/showall/index.html",
    "title": "showall",
    "section": "",
    "text": "I just scratched a MATLAB itch. So many times I’ve seen—and experienced myself—people popping open figure windows in MATLAB, then trying to juggle them and move them around the screen so that you can see all of them at once. If you know what you’re doing, you can dock them into the desktop and lay them out there, but it’s still a lot of clicks.\nSo, I present showall.m, a little function that will automatically bring forward all the figures (or the ones you specify) and lay them out in a grid. Now you can master your figures!"
  },
  {
    "objectID": "post/addiction/index.html",
    "href": "post/addiction/index.html",
    "title": "Addiction",
    "section": "",
    "text": "Last week my 14-year-old son asked rhetorically, “How is it that more people aren’t addicted to math?”\nI know son, I know."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lecture-8-gram-schmidt/index.html",
    "href": "post/trefethen-bau-matlab-julia-lecture-8-gram-schmidt/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia, Lecture 8: Gram-Schmidt",
    "section": "",
    "text": "This lecture is about the modified Gram-Schmidt method and flop counting. The notebooks are here.\nI’m lost.\nAlmost as an afterthought I decided to add a demonstration of the timing of Gram-Schmidt compared to the asymptotic  flop count. Both MATLAB and Julia got very close to the trend as  got into the hundreds, using vectorized code:\nn_ = collect(50:50:500);\ntime_ = zeros(size(n_));\nfor k = 1:length(n_)\n    n = n_[k];\n    A = rand(1200,n);\n    Q = zeros(1200,n);  R = zeros(600,600); \n    \n    tic();\n    R[1,1] = norm(A[:,1]);\n    Q[:,1] = A[:,1]/R[1,1];\n    for j = 2:n\n        R[1:j-1,j] = Q[:,1:j-1]'*A[:,j];\n        v = A[:,j] - Q[:,1:j-1]*R[1:j-1,j];\n        R[j,j] = norm(v);\n        Q[:,j] = v/R[j,j];\n    end\n    time_[k] = toc();\nend\n\nusing PyPlot\nloglog(n_,time_,\"-o\",n_,(n_/500).^2,\"--\")\nxlabel(\"n\"), ylabel(\"elapsed time\")\nI noticed that while the timings were similar, Julia lagged MATLAB just a bit. I decided this would be a great chance for me to see Julia’s prowess with speedy loops firsthand.\nCompare the vectorized and unvectorized Julia versions here:\n\nLook at the last line–it’s allocating 1.4GB of memory to make the nested loop version happen! I thought perhaps I should use copy to create v in each pass, but that change didn’t help. I even tried writing my own loop for computing the dot product, to no avail.\nIt did help a little to replace the line in which v is updated with\nv = broadcast!(-,v,Q[:,i]*R[i,j])\nThe bang on the name of the function makes it operate in-place, overwriting the current storage. Apparently Julia will create some syntactic sugar for this maneuver in version 0.5. Here it reduced the memory usage to 1.1 GB.\nJulia’s reputation is that it’s great with loops, especially compared to MATLAB and Python. As a Julia newbie I recognize that there may still be only a small change I need to make in order to see this for myself. But I feel as though having to use that broadcast!, or even the more natural .= that may be coming, is already too much to ask. I’m frustrated, confused, and disappointed."
  },
  {
    "objectID": "books.html",
    "href": "books.html",
    "title": "Books",
    "section": "",
    "text": "Data Science 1\n\n\n\n\n\nFree course on Data Science at the sophomore level\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nExploring ODEs\n\n\n\n\n\nA different kind of introduction to ODEs.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFundamentals of Numerical Computation\n\n\n\n\n\nAn undergraduate textbook in computational mathematics.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLearning MATLAB\n\n\n\n\n\nStreamlined approach to MATLAB for veteran programmers.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSchwarz–Christoffel Mapping\n\n\n\n\n\nMonograph on a major conformal mapping method.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lectures-20-21-23-solving-square-systems/index.html",
    "href": "post/trefethen-bau-matlab-julia-lectures-20-21-23-solving-square-systems/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia: Lectures 20, 21, 23: Solving square systems",
    "section": "",
    "text": "Three in one this time: Lecture 20, which is on Gaussian elimination / LU factorization, Lecture 21, on row pivoting, and Lecture 23, on Cholesky factorization. I mainly skipped over Lecture 22, about the curious case of the stability of pivoted LU, but the main example is dropped into the end of my coverage of pivoting.\nThe Julia surprises are, not surprisingly, coming less frequently. In Lecture 20 I had some fun with rational representations. I like using MATLAB’s format rat when presenting Gaussian elimination, as it allows me to recall the way the process looks when learned by hand. It’s a fun trick, but of course the underlying values are still all double precision, and the rational approximations to them are found ex post facto. By contrast, Julia offers true rational numbers, constructed and shown using the // operation.\nCompare the MATLAB\nformat rat\nI = eye(4);\nL21 = I + (-5/17)*I(:,2)*I(:,1)';\nL31 = I + (-9/17)*I(:,3)*I(:,1)';\nL41 = I + (-4/17)*I(:,4)*I(:,1)';\nto the Julia\nI = eye(Rational,4);\nL21 = copy(I);  L21[2,1] = -5//17;\nL31 = copy(I);  L31[3,1] = -9//17;\nL41 = copy(I);  L41[4,1] = -4//17;\nThe MATLAB code requires only the format call, because it’s only the display of results that is affected. The Julia code is doing something deeper and needs more changes as a result.\nJulia could use something like a format command. I almost always find MATLAB’s terminal output more readable, or at least easier to manipulate into a good form. Here’s one example using the rational output. First, MATLAB:\n17              2              3             13             93       \n 0            194/17         155/17          71/17         963/17    \n 0            101/17          92/17          87/17         574/17    \n 0            230/17         243/17         -18/17        1158/17\nAnd the Julia:\n4x5 Array{Rational{T&lt;:Integer},2}:\n17//1    2//1     3//1    13//1    93//2 \n  0//1  194//17  155//17   71//17  963//34\n  0//1  101//17   92//17   87//17  287//17\n  0//1  230//17  243//17  -18//17  579//17\nI almost never need that header line that Julia gives. The numbers are already showing themselves to be Rational, and the shape of the array is self-evident. (Though I now see that MATLAB 2016b is adding such headers to non-float output.) The zero structure also jumps out more clearly in the MATLAB case, though it’s profligate with whitespace.\nAnother comparison, of MATLAB (using the default format):\n1     0     0     0     0     1\n0     1     0     0     0     2\n0     0     1     0     0     4\n0     0     0     1     0     8\n0     0     0     0     1    16\n0     0     0     0     0    32\nversus Julia:\n6×6 Array{Float64,2}:\n  1.0  0.0  0.0  0.0  0.0   1.0\n  0.0  1.0  0.0  0.0  0.0   2.0\n  0.0  0.0  1.0  0.0  0.0   4.0\n  0.0  0.0  0.0  1.0  0.0   8.0\n  0.0  0.0  0.0  0.0  1.0  16.0\n  0.0  0.0  0.0  0.0  0.0  32.0\nThere’s nothing wrong per se about Julia’s. But which version would you write down, or expect to see in print? One last case, of a matrix that is supposed to be triangular but for a little roundoff. First, Julia:\n4×4 Array{Float64,2}:\n  17.0           2.0      3.0          13.0    \n  0.0          13.5294  14.2941       -1.05882\n  0.0           0.0     -2.93913       5.06957\n  5.55112e-16   0.0     -4.44089e-16   4.09024\nAnd MATLAB, using the default format:\n17.0000    2.0000    3.0000   13.0000\n      0   13.5294   14.2941   -1.0588\n      0         0   -2.9391    5.0696\n0.0000         0   -0.0000    4.0902\nJulia has chosen to align on the decimal point. It’s also suppressing trailing zeros, except for the first, giving an odd and false impression of values that have a precise number of significant digits. MATLAB’s choice of right alignment is visually superior, and only exact zero gets a special display. True, you might want that exponential notation for the tiny values; you can get it by changing the format.\n&gt;&gt; format short e\n&gt;&gt; U\nU =\n    1.7000e+01   2.0000e+00   3.0000e+00   1.3000e+01\n            0   1.3529e+01   1.4294e+01  -1.0588e+00\n            0            0  -2.9391e+00   5.0696e+00\n    5.5511e-16            0  -4.4409e-16   4.0902e+00\n\n&gt;&gt; format short g\n&gt;&gt; U\nU =\n            17            2            3           13\n            0       13.529       14.294      -1.0588\n            0            0      -2.9391       5.0696\n    5.5511e-16            0  -4.4409e-16       4.0902\nIt’s nice to have options."
  },
  {
    "objectID": "post/length-of-papers/index.html",
    "href": "post/length-of-papers/index.html",
    "title": "Length of papers",
    "section": "",
    "text": "Nick Trefethen has posted a wonderful graph showing how the average length of papers published in several SIAM journals has doubled over the last 40 years."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lecture-11-least-squares/index.html",
    "href": "post/trefethen-bau-matlab-julia-lecture-11-least-squares/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia, Lecture 11: Least squares",
    "section": "",
    "text": "This week’s notebooks (MATLAB and Julia–now all lectures are together for each language) are about least squares polynomial fitting.\nThe computational parts are almost identical, except for how polynomials are represented. In MATLAB, a vector of coefficients is interpreted as a polynomial in the context of particular functions, such as polyval. The major pain is that the convention is for the coefficients to be ordered from high degree to low, which is almost always the opposite of what you really want. Hence I’ve gotten used to writing code like\np = @(x) polyval( c(end: -1:1), x-1955 );\nIt’s not a big deal, but it trips up some students every semester.\nJulia has a full-fledged polynomial type, if you care to add and load the package. And, it expects ordering from the constant term to the highest degree. So I came up with\np = Poly(c);  \nq = t -&gt; p(t-1955);\nSimple enough, but I find two disappointments. First, it’s a bare-bones class. For instance, the second object q above is also a polynomial, but we’ll never know it formally, or be able to get its coefficients. A shiftvar method or something similar would be nice. Second, in the effort to clone the MATLAB interface, a potential for serious confusion was introduced. The command p=poly(c) also works, but (like MATLAB’s counterpart) constructs a polynomial whose roots, not coefficients, are given. This is way too easy a mistake to make.\nAnother element this time was that I tried using the nascent Plots package for Julia. It’s an interesting attempt to graft a graceful interface onto the various graphics backends that already exist. I was motivated to try it because AFAIK, the PyPlots package lacks a counterpart to fplot from MATLAB. Perhaps in part because of my time with the Chebfun project, I have been putting more emphasis in my teaching on representing functions as such, rather than implicitly as vectors of numbers. It bothers me now, for example, that functions such as interp1 and ode45 return numbers or structures rather than callable functions, which is what their algorithms should be doing in the deep sense.\nAnyhow, I end up using fplot a lot because of my emphasis on functions, and couldn’t find a counterpart in PyPlot. In Plots, however, the plot command handles both numerical and functional arguments alike. Here’s a snippet from the notebook:\np = Poly(c);  \nplot( t-&gt;p(t-1955), 1955,2000 )\nplot!( year,anomaly, m=:o,l=nothing );\ntitle!(\"World temperature anomaly\");\nxlabel!(\"year\");  ylabel!(\"anomaly (deg C)\")\nNot bad! You can see a couple of quirks though. One is the use of keyword arguments in line 3; the arguments m=:o and l=nothing respectively set the point markers to circles and the lines connecting points to be suppressed. This takes getting used to, but it’s memorable and compact enough.\nThe other quirk that you see above is the use of the banged commands like plot! and title!. The bang in Julia is a convention meaning “operate in place” or “overwrite existing.” By default, the MATLAB-like commands replace the existing plot, so they have to be banged in order to build on top of it instead. This is a bit dubious in the case of titles and labels––why would I create a new plot by issuing a title?––but it is at least consistent, and, unlike the global state used in MATLAB by the hold command, works the same regardless of context and history.\nOne quirk––to me, a bug––that you don’t see is that the default in Plots is that every plot creates or adds to a legend. I’m not a big fan of plot legends in most contexts, but you’re welcome to them if you like them. However, I don’t find it reasonable to have one forced on me for a graph with a single curve that I didn’t give a label to! I turned off this travesty by starting off with\nusing Plots;  pyplot(legend=false);\nwhich at least is straightforward, though entangled with my choice of backend."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lecture-4-svd/index.html",
    "href": "post/trefethen-bau-matlab-julia-lecture-4-svd/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia, Lecture 4: SVD",
    "section": "",
    "text": "The notebooks: matlab and julia.\nToday is about some little conveniences/quirks in Julia. Starting here:\nt = linspace(0,2*pi,300);\nx1,x2 = (cos(t),sin(t));\nThe second line assigns to two variables simultaneously. It’s totally unnecessary here, but it helps to emphasize how the quantities are related.\nNext we have\nU,σ,V = svd(A)\nI’m unreasonably happy about having Greek letters as variable names. Just type in ‘’ and hit tab, and voila! It’s a reminder of how, in the U.S. at least, we’re so used to living within the limitations of ancient 128-character ASCII—telegraphs, really—that we can be surprised by expanded possibilities.\nLater on we have diagm(σ). In MATLAB, the diag function has two roles: convert a vector to a diagonal matrix, and extract the diagonal elements of a matrix. This creates a curious edge case for MATLAB: for example, \ndiag([1 2 3])\nreturns a 3-by-3 matrix, not the single element 1. This is almost always what you want, but I’ve run into gotchas wherein a program works perfectly until an input of the ‘wrong’ size silently changes the behavior of a function. In Julia the two functionalities are separated into diag and diagm, which avoids the edge case ambiguity. I think it’s worth the clarity here to have the extra command.\nThe one thing I missed having in the Julia version was MATLAB’s format command, which lets you set the default display of numbers in all following output. In this notebook I just had numbers as placeholders and really wanted just to show shapes and sizes. Julia’s full-length output obfuscates the sizes quite a bit, and I’d like to tell it to calm down with all those digits for a little while (rather than saying so with each new output). If that capability is there, I overlooked it."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lectures-12-13-conditioning-and-floating-point/index.html",
    "href": "post/trefethen-bau-matlab-julia-lectures-12-13-conditioning-and-floating-point/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia, Lectures 12-13: Conditioning and floating point",
    "section": "",
    "text": "I’ve run into trouble managing gists with lots of files in them, so I’m back to doing one per lecture. Here are Lecture 12 and Lecture 13.\nWe’ve entered Part 3 of the book, which is on conditioning and stability matters. The lectures in this part are heavily theoretical and often abstract, so I find a little occasional computer time helps to clear the cobwebs.\nRight off the top, in reproducing Figure 12.1, I ran right into the trap I worried about in my last post regarding polynomials in Julia. In MATLAB, the polynomial coefficients are just a plain vector. That makes perturbing them trivial:\np = poly([1,1,1,0.4,2.2]);    % polynomial with these roots\nq = p + 1e-9*randn(size(p));    % perturb its coefficients\nIn Julia, you can use the Polynomials package and get polynomial objects. Behold:\nusing Polynomials \np = poly([1,1,1,0.4,2.2]);  # polynomial with these roots\nq = p + Poly(1e-9*randn(6));    # perturb coefficients\nNote that poly constructs a polynomial from a vector of roots, while Poly constructs one from a vector of coefficients. Sure enough, I used poly in both lines the first time around. It’s a pernicious mistake, because it produces no error—the polynomials can be added no matter what. The mistake was mine, but I think this is an unfortunate design.\nThe only other notable usage in Lecture 12 is my first use of a comprehension:\nhilb(n) = [ 1.0/(i+j) for i=1:n, j=1:n ];\nThis is a pretty handy way to create a matrix.\nIn Lecture 13 I had some fun dissecting floating point numbers in both systems. There was only one area in which Julia didn’t go as smoothly as I would hope. MATLAB offers realmin and realmax , which give the smallest and largest normalized floating point numbers. While Julia has similar-sounding commands, they are interpreted differently:\njulia&gt; typemin(Float64), typemax(Float64)\n(-Inf,Inf)\nEh, not so much. There is even one more layer of subtlety. Consider\njulia&gt; (prevfloat(Inf),nextfloat(0.0))\n(1.7976931348623157e308,5.0e-324)\nThe first of these values is exactly the same as realmax, but the second is not realmin. IEEE 754 double precision has “denormalized” numbers that let you trade away bits of precision to get closer to zero in magnitude. Julia is reporting the smallest denormalized number, not the smallest full-precision number. Julia’s not wrong, but access to the extreme finite double precision values isn’t as straightforward as it could be.\nOne last observation. Trefethen & Bau refer to the value \\(2^{-53}\\) as “machine epsilon.” This isn’t what MATLAB and Julia use, which is \\(2^{-52}\\). Nick Higham’s Accuracy and Stability of Numerical Algorithms also has “machine epsilon” at \\(2^{-52}\\) and calls \\(2^{-53}\\) “unit roundoff.” Stoer and Bulirsch (2nd ed.) call \\(2^{-53}\\) “machine precision.” Corless and Fillion seem to agree with Higham. Golub and Van Loan (3rd ed.) don’t use “machine epsilon” at all, and in the index one finds\n\nMachine precision. See unit roundoff.\n\nSigh. The mathematical uses are, unsurprisingly, consistent. Frankly, I feel better about my personal inconsistencies at using those terms: at least I stood on the shoulders of giants."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lectures-6-7/index.html",
    "href": "post/trefethen-bau-matlab-julia-lectures-6-7/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia, Lectures 6-7",
    "section": "",
    "text": "Here are the Jupyter notebooks for Lecture 6 and Lecture 7. (I finally noticed that a Gist can hold more than one notebook…duh.)\nNot much happened in Lecture 6, but I got gobsmacked in Lecture 7. It happened when I tried to convert this boring MATLAB code for backward substitution.\nA = magic(9);   b = (1:9)';\n[Q,R] = qr(A);\nz = Q'*b;\nx(9,1) = z(9)/R(9,9);\nfor i = 8:-1:1\n    x(i) = (z(i) - R(i,i+1:9)*x(i+1:9)) / R(i,i);\nend\nHere is what I first tried in Julia.\nA = round(10*rand(9,9));  b = (1:9);\nm = 9;\n(Q,R) = qr(A);\nz = Q'*b;\nx = zeros(m);\nx[m] = z[m]/R[m,m];\nfor i = m-1:-1:1\n    x[i] = (z[i] - R[i,i+1:m]*x[i+1:m]) / R[i,i];\nend\nSeems straightforward, but line 4 gives an error. I’m not going to copy the error message here, in case you’re using mobile data right now. What I mean is that it is verbose, not to mention obscure. You don’t appreciate simple, clear error messages until you get something else!\nAnyhow, I then remembered that in Julia, the colon construction (1:9) produces a Range, not a Vector. As I understand it, Julia embraces a lazy design philosophy: it avoids evaluation of an expression until the last possible moment. Suppose the only use of that Range is to describe a loop iteration—in that case, why have a vector?\nI’m all for lazy philosophy. (Haw haw!) It’s not clear to me why the context Q'*b does not automatically convert the Range into a Vector. It’s even less clear why they have deprecated the idiom [1:9] to create a Vector; it works for now but gives a warning. Instead one should use collect:\nA = round(10*rand(9,9));  b = collect(1:9);\nm = 9;\n(Q,R) = qr(A);\nz = Q'*b;\nx = zeros(m);\nx[m] = z[m]/R[m,m];\nfor i = m-1:-1:1\n    x[i] = (z[i] - R[i,i+1:m]*x[i+1:m]) / R[i,i];\nend\nFeels very odd to me still, but okay.\nWe are not out of the woods yet. This version still fails in the loop body, again vomiting opaque error messages. Remember how, back in Lecture 2, I mentioned that scalars and 1x1 matrices are different things? Inside the loop above, z[i] is a scalar and the product is a length-1 vector. But the subtraction works anyway, as z[i] is silently promoted to a 1-vector also. No, the problem comes with the assignment: you can’t assign a 1-vector to an element of an array of numbers.\nThere’s a very long (space and time) discussion about this and related issues in Julia. Suffice it to say that what mathematicians do with scalars, vectors, matrices, and tensors isn’t rigorously consistent—or at least, there seem to be multiple, incompatible rigorous ways to use them.\nIn this particular case I have found two unsatisfying workarounds. The idiom x[i:i] produces a Vector, not a scalar, so the assignment goes through. Or we can work on the other side of the assignment and pull out the scalar from the vector:\n(z[i] - R[i,i+1:m]*x[i+1:m])[1] / R[i,i]\nNow, it’s pleasing that this syntax does work, as there is no good MATLAB equivalent for indexing into a temporary expression. I just wish it was in the service of something less dismal.\nAgain: Julia’s designers have solid reasons for doing things this way. I wouldn’t consider it a dealbreaker for research codes, but this episode is not something I would want to explain to undergrads who are just wrapping their heads around LU factorization. It pulls you right out of thinking about math and into thinking about strict-typing, pinhead-dancing angels. How unfortunate."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lecture-5-more-on-the-svd/trefethen-bau-matlab-julia-lecture-5-more-on-the-svd.html",
    "href": "post/trefethen-bau-matlab-julia-lecture-5-more-on-the-svd/trefethen-bau-matlab-julia-lecture-5-more-on-the-svd.html",
    "title": "Trefethen & Bau & MATLAB & Julia Lecture 5: More on the SVD",
    "section": "",
    "text": "Notebooks are viewable for matlab and julia.\nThis is one of my favorite demos. It illustrates low-rank approximation by the SVD to show patterns in voting behavior for the U.S. Congress. With no a priori models, project onto two singular vectors and pow–meaning and insight jump out.\nI took one shortcut. I have a MATLAB script that reads the raw voting data from voteview.com and converts it to a matrix. No doubt I would learn a lot about I/O in Julia if I translated it, but I got short on time and instead saved it locally from MATLAB. Then load it using the MAT package for Julia and Bob’s your uncle.\nI did stumble into a nasty gotcha, though. I decided to make histograms for the distributions of the “partisan” and “bipartisan” coordinate values. Unfortunately, there’s a name clash: MATLAB’s best known histogram plotter is called hist, but Julia has a built-in function by that name that just bins the data. I knew there was also a hist() in PyPlot, but to my bafflement the access for it was not PyPlot.hist(), which does exist:\nhelp?&gt; PyPlot.hist\n hist(v, e) -&gt; e, counts\n\nCompute the histogram of v using a vector/range e as the edges...\nhist(v[, n]) -&gt; e, counts\n\nCompute the histogram of v, optionally using approximately...\nThis is Julia’s built-in function. The next thing I tried was typing in Pyplot. and hitting tab for a list of completions. Most of the familiar MATLAB-style plotting functions are there, but no hist, just hist2D, which is not equivalent. I don’t remember now where I found it, but the way to call the function I want is the bizarre plt[:hist]. Neither ?plt nor tab completion gives any whiff of this syntax or possibility. Obviously there’s some logic at work here, and no doubt my Julia and Python ignorance are showing, but this was the most frustrating Julia experience I’ve had yet.\n(Ironically, MATLAB has a newer plotting function called histogram, which does not seem to conflict with any Julia names!)"
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lecture-9-matlab/index.html",
    "href": "post/trefethen-bau-matlab-julia-lecture-9-matlab/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia, Lecture 9: MATLAB",
    "section": "",
    "text": "For today’s notebooks I got caught on a problem I anticipated in theory but failed to spot in practice for longer than I would like to admit.\nFirst let me mention how interesting this Lecture is to me personally. The title of the lecture is “MATLAB”, and it details three numerical experiments. The first of these uses QR factorization of a discretization of monomials in order to approximate the Legendre polynomials. I skipped this one here because I opted in class to show how Gram-Schmidt looks using Chebfun. (It’s awesome.)\nThe other two numerical experiments show different aspects of numerical instability of the Gram-Schmidt algorithm, classical and modified. The MATLAB version looks just like I would have written it 20 years ago:\n[U,S,V] = svd(randn(80));\ns = 2.^(-1:-1:-80);\nA = U*diag(s)*V';\nsemilogy(s,'.')\n[Qc,Rc] = gs(A); &nbsp;% classical\nhold on, semilogy(diag(Rc),'o')\n[Qm,Rm] = mgs(A); % modified\nsemilogy(diag(Rm),'s')\nThe idea is that the diagonal elements of R descend exponentially just like the singular values do. If you run this code (or peek at the link at the top), you see that MGS stops tracking them right around machine precision, whereas the less stable classical version wanders off at about half of the available digits.\nI did introduce my own wrinkle here. I can’t believe I haven’t thought of using this for teaching before, but by the conversion A=single(A) I can simulate a different value of machine epsilon without changing anything else! It backs up the observations from the first graph.\nIn Julia this gambit ran into a big snag. Here was my first code for MGS:\nfunction mgs(A)\n  m,n = size(A);\n  Q = zeros(m,n); R = zeros(n,n);\n  for j = 1:n\n    R[j,j] = norm(A[:,j]);\n    Q[:,j] = A[:,j]/R[j,j];\n    R[j,j+1:n] = Q[:,j]'*A[:,j+1:n];\n    A[:,j+1:n] -= Q[:,j]*R[j,j+1:n];\n  end\n  return Q,R\nend\nEverything was fine in double precision. After a couple of missteps, I figured out how to make `A`` single precision:\nA = convert(Array{Float32},A)\nNot beautiful, but it works. However, while it had the desired effect on MGS, it did nothing to classical GS! It finally came down to a surprise:\njulia&gt; typeof( 1.0f0 + 1.0f0 )\nFloat32\n\njulia&gt; typeof( 1.0f0 + 1.0 )\nFloat64\nA single plus a double is double. The rule in Julia is that the operands are converted to a type that can represent them both. MATLAB gives a different outcome, converting both numbers to single:\n&gt;&gt; class( single(1) + 1 )\nans =\nsingle\nI suppose the philosophy here is that there’s no point padding the numbers with meaningless digits—the moment you introduce a single precision value, you’ve chosen that level of precision. I think that’s the more sensible choice for floating point; Julia is concerned with the consistency of its much more intricate and far-reaching type system. For Julia I changed the initialization of Q and R to\nQ = zeros(A);\nR = zeros(Q[1:n,1:n]);\nThat way they are initialized with the correct type in either case.\nNow for the bonehead move of the day. I seemed to get inconsistent and nonreproducible results in the single precision cases. I went away, did other things, came back into a fresh session, and…no difference between double and single precision. I may have said a few things I now come to regret. Finally I remembered the key: Julia passes by reference, not value. MGS alters the input matrix, which has no effect outside the function in MATLAB but changes the ‘master copy’ in Julia. A little switch to\nfunction mgs(B)\n  A = copy(B);\nand all was well. This is an example of how much MATLAB has shaped my thinking about programming. IIRC, MATLAB doesn’t always pass by value; if an input argument is not altered, it is not copied. But it’s handled by the compiler, not the programmer.\nIf nothing else I’m getting ever more clarity on the ways MATLAB keeps things simple. Variables are bound to their values, period. Single precision is an irrevocable choice. Scalars and 1x1 matrices are the same thing. Don’t it always seem to go that you don’t know what you’ve got ’til it’s gone?"
  },
  {
    "objectID": "post/making-continuous-assessment-work/index.html",
    "href": "post/making-continuous-assessment-work/index.html",
    "title": "Making continuous assessment work",
    "section": "",
    "text": "I’ve come to think that in math at least, continuous learning and assessment may be more important even than active learning. The traditional model of chunking assessments into weekly or monthly batches encourages the cram-and-dump style of “learning.” Since students are allowed to delay work on assignments that are crucial to their understanding of incoming material, it’s impossible for them to build that understanding in real time. Instead they copy and hope to parse later, when assessment is demanded.\nIt’s tempting to say that students should suck it up and organize their time better. This attitude ignores human nature, especially the nature of people in late adolescence and early adulthood. Even a large part of my own work is deadline-driven rather than proactive. And I love math!\nAny big change in expectations encounters resistance. Fortunately, breaking through that resistance sometimes spills over into breaking resistance to the tough job of learning itself. The trick is doing so in a way that feels fair to the students and manageable to the instructor. It’s hard to overthrow everything at once.\nHere’s what I’m thinking for my fall course on numerical computing. Each class meeting (3 times a week) has a cycle associated with it:\nBefore class:\n\n(them) Read/watch and reflect.\n(them) Take an online quiz on the new material.\n\nIn class:\n\n(mostly me) Review problem spots. Fill in some of the details.\n(us) Work to produce one graph or one table relevant to the new material.\n(them) Turn in a description of what is still not clear.\n\nAfter class:\n\n(me) While everything is fresh, I take one last try at explaining material that is still confusing.\n(them) Do a couple of homework problems. Before the next meeting, for full credit; before the following meeting, for partial credit.\n\nAs you can tell, this is a lot of work for everyone, and—by design—it’s not flexible. To compensate, I won’t give exams. There will be some group projects for summative assessments instead."
  },
  {
    "objectID": "post/the-sorry-state-of-teaching-odes/index.html",
    "href": "post/the-sorry-state-of-teaching-odes/index.html",
    "title": "The sorry state of teaching ODEs",
    "section": "",
    "text": "I’ve spent the last two spring semesters teaching ODEs (ordinary differential equations) to a total of about 170 biomedical and chemical engineering majors. The content is dictated by a number of constraints: the perceived desires of the client departments, multiple instructors, all of whom have more experience with the course than I do, and traditional expectations. Based on a limited survey of popular textbooks (this, this, and our choice, Brannan and Boyce), many courses like this are quite similar. (You can look through my notes, rough as they are.)\nWhat I find particularly disheartening about these books, and the courses they imply, is that they share an overwhelming emphasis on hand computations of formulas to produce “solutions” of particular problems. I scare-quote “solutions” here because it’s become clear that the majority of students can’t readily comprehend their own outputs. And really, why should they? Most homework (and, by implication, exam) problems are going to simply ask them to produce the output, not interpret it.\nNot long ago, this approach was not hard to justify. What could be more important to building a bridge or going to the Moon than getting the right answer? Of course it’s absurd to suggest that getting the right answer no longer matters. But what does no longer matter is whether the student can crank that answer out by hand.\nI’m old enough to remember a middle school lecture on finding values of logarithms by linearly interpolating values from a table. I sincerely hope this is no longer done, though I can imagine the objections when it was proposed for elimination: “Isn’t it good for them to learn linear interpolation? What if they’re stuck on a desert island without a calculator? How will they recognize when the calculator gives a clearly wrong answer?” You’ll hear much the same said about many other dubious lessons that are still very much alive today, such as days of calculus devoted to drawing (bad) graphs by hand.\nLong after the original driving need to compute or do something by hand has vanished, we’re able to supply alternative reasons to do it. These reasons seem to come in two flavors: hypothetical utility and tradition. Claims that students “learn concepts better” by hand computing are almost never substantiated by evidence, and in any case tend to beg the question of which concepts are in play. I think we can conclusively retire the “they may not always have a calculator” form of manufactured need. As for grimly continuing a tradition, sometimes disguised within a beguiling “what’s the harm?” phrasing, what you get is a recipe for stagnation and irrelevance. The cost is always a lost opportunity to teach something else.\nI would not suggest that students never solve an example problem by hand. An ODE course graduate who cannot solve, say, \\(y^{\\prime\\prime} + 4y=1\\) has missed something vital. But I don’t see what’s to be gained by practicing on \\(y^{\\prime\\prime} + 5y' + 4y=t^2e^{-2t}\\). All the time and focus needed to wrest an answer from that problem is purely mechanical, never going beyond the application of a rigid algorithm.\nNot only are such algorithms much better performed by a computer, they are incredibly fragile. The analytical solution of \\(y^{\\prime\\prime} + ty=1\\) is so much harder to produce and interpret than \\(y^{\\prime\\prime} + 4y=1\\) that it might as well be of a different universe altogether. And that’s still just a linear problem!\nIt’s sensible to focus on the most fundamental ODE problems with analytical solutions, like the linearly damped oscillator and the logistic equation. We can totally dissect those problems, and they represent the simplest form of more complicated and realistic models. But when we give the impression that analytical solutions are the primary objective of the ODE world, we grossly distort the true picture. Already when I was a graduate student over 25 years ago, there were great computational ODE tools that would give you a fast and detailed understanding of dynamics. If you were inclined and able, you might go on to make some exact analytical (qualitative) conclusions suggested by the numerical and graphical explorations. But that’s a pursuit for a modestly sized cadre of mathematically advanced academics. It’s not the use case for over 99% of our undergraduates taking their one lifetime ODE course.\nRather than trying to turn our students into slow and error-prone Mathematica simulators, we ought to be equipping our students with ODE fluency. Modeling, nondimensionalization, stability, and resonance are all more fundamental and vital than knowing every last case of the method of undetermined coefficients. Knowing the stability implications of poles in a transfer function is more important than performing partial fraction decompositions to invert Laplace transforms by hand.\nWhat’s a more relevant use of time: solving artificial scalar problems carefully selected to have compact analytical solutions, or learning how to simulate ODE models of beer fermentation or passive walking down a ramp?\nSomeday, perhaps, I will get to teach an ODE course that follows the textbook I co-authored with Trefethen and Birkisson. We used Chebfun to do the heavy lifting of solving problems and discussed at length how to read an ODE and investigate its behavior thoroughly. By not dwelling on now-archaic analytical solution methods, we were able to include introductions to chaos, stochastics, eigenvalues, bifurcations, and boundary layers, all essential phenomena that never make it into a traditional class.\nUntil that happy day arrives, I guess you can find me trying to motivate 70 engineers to learn how to compute the exponential of 2-by-2 matrices."
  },
  {
    "objectID": "post/why-not-zoidberg/index.html",
    "href": "post/why-not-zoidberg/index.html",
    "title": "Why not Zoidberg?",
    "section": "",
    "text": "Something fun for Friday?\nMy older son binge-watched Futurama on Netflix a few months ago. This was one of the funniest shows of at least recent TV history. Especially if you like nerdy, cultural-reference, rapid-fire style humor like a real Gen-Xer.\nIt’s also probably the first and only time in television history that a new mathematical theorem was proved for and first presented in a series episode. The whole run of the series had numerous mathematical references. This may have something to do with the fact that co-creator and writer Ken Keeler has a PhD in applied math from Harvard."
  },
  {
    "objectID": "post/quantum-weirdness/index.html",
    "href": "post/quantum-weirdness/index.html",
    "title": "Quantum weirdness",
    "section": "",
    "text": "I’m a little late getting this news, but I’m fascinated by an experiment at Australian National University showing (once again) in a vivid way how strange the quantum mechanical world is.\nThe experiment was a variation on the celebrated double slit experiment that shows how photons are both particles and waves, at least in some interpretations of the universe. That’s freakish in a not-news kind of way, as is the fact that the same is true of good old atoms, which might seem more as though they should stay particular all the time.\nOne amusing view of the ANU experiment is that an atom can “decide” to be either particle-like or wave-like based on information from the future. Just like the restaurant at the end of the universe, that is, of course, impossible.\nIf I were a better physicist I could explain this to you, but in all honesty it was just the lack of intuitiveness about high energy physics (and maybe the contemporaneous demise of the Superconducting Super Collider) that turned me off to the subject as an undergraduate. Still, what a universe to live in, eh Horatio?"
  },
  {
    "objectID": "post/promotion-system/index.html",
    "href": "post/promotion-system/index.html",
    "title": "Promotion system",
    "section": "",
    "text": "In keeping with my post on how grades in a course affect student motivation, I’ve been pondering alternatives to the classic mean-them-and-mean-it model.\nAll of my family members have spent time studying karate. (I’m a brown belt, FYI, which is like an A.B.D.) One thing I’ve always liked about the dojos I’ve known is how the belt promotion system works. It’s what I would now call a mastery based learning concept. Students are tested to advance to the next belt when they are ready, regardless of time spent in the system (of course there are practical limitations on the speed of progression). The tests themselves are rigorous but the results are typically foregone conclusions, by design.\nTruly self-paced mastery learning is difficult to fit into the college grading model. With a technology assist it’s possible in topics like pre-calculus and at least some calculus, and probably a few other introductory courses I’m not familiar with. I don’t see how I could do it in my advanced course this fall.\nI could also think of a more corporate model, which is where most of the students will end up. So the first few weeks would be like an interview to determine the initial job rank (i.e., final grade). Based on performance I would give personal feedback and update their ranks accordingly throughout the term. This goes hand in hand with continuous assessment, which I plan to do anyway.\nBecause the later material in part builds on earlier concepts, I could argue that progress later on could make up for early struggles. In any case students would be free to fight for grade promotions to the very end of the course. Unlike the karate model, demotions are possible, so they couldn’t reach an acceptable level and just lay back.\nA radical realization of this concept would include doing away with the numerical grades on each assignment! I admit, that excites me—I can’t stand the arbitrariness of deciding how many “points” each mistake is worth. I see no reason why a grading rubric can’t be precise without being applied quantitively.\nThis would be a huge culture shift for me and for the students. It’s risky. I’d love to hear opinions and experiences trying to do this sort of thing in math."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-lecture-3-norms/index.html",
    "href": "post/trefethen-bau-matlab-julia-lecture-3-norms/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia, Lecture 3: Norms",
    "section": "",
    "text": "Here are the MATLAB and julia notebooks.\nThe big issue this time around was graphics. This topic dramatically illustrates the advantages on both sides of the commercial/open source fence. On the MATLAB side, it’s perfectly clear what you should do. There are many options that have been well constructed, and it’s all under a relatively consistent umbrella. There are things to learn and options to choose, but it’s clear what functions you will be using to make, say, a scatter plot, and a lot of similarity across commands.\nJulia graphics are another story. At this writing, there are two options recommended on Julia’s official page about plotting packages: PyPlot and Gadfly. It doesn’t take much exploration to decide that the former is favored by MATLAB veterans and the latter, by R devotees. Confusingly, the general download page for Julia mentions a third package called Plots that is supposed to integrate all of the backends. It’s still early days for Julia, and I’m sure much remains in flux.\nMoreover, because you can (quite easily) import and run Python code in Julia, in principle you have access to all Python plotting packages. One of the big players is matplotlib, which is more or less what Julia’s PyPlot is supposed to provide. But there are also Bokeh, plotly, and pyqtgraph—for all I know, many more besides. All of these can make gorgeous graphics, often highly interactive and even hosted in the cloud. The relative merits are not at all clear.\nHere we run into the paradox of choice: having many options, even good ones, can provoke anxiety rather than satisfaction. Which package do I invest time in learning? MATLAB limits choice but provides a sort of editorial, almost paternal, reassurance.\nMy personal goal is to learn Julia from the standpoint of a MATLAB user, so PyPlot it is. All in all, the transition isn’t bad, though there are some twists.\nIn the last few years I’ve been more often turning to automatic function plotting in MATLAB, using fplot, ezsurf, and ezcontour. If PyPlot supports those, I have yet to find out about them. So it’s back to the world of evaluating functions on tensor product grids.  A MATLAB veteran turns to meshgrid, but Julia supports broadcasting across singleton dimensions. For example:\nusing PyPlot\nx = linspace(-1,1,90);\ny = x';\ncontour(x[:],y[:],sqrt(x.^2 .+ y.^2))&lt;/pre&gt;\nBecause x has a column shape while y has a row shape, the .+ operator broadcasts each along the “missing” dimension. It’s a clever shortcut once you know it. It works just as well for contours of the vector 1-norm, but for the max norm I had to broadcast manually:\ncontour(x[:],y[:],broadcast(max,abs(x),abs(y)))\nIt’s not clear to me why that broadcast should not happen automatically, given that max is a dedicated elementwise operator.\nThere’s more Julia subtlety hiding in this notebook, but those issues will wait for another time."
  },
  {
    "objectID": "post/jekyll-for-clicker-questions/index.html",
    "href": "post/jekyll-for-clicker-questions/index.html",
    "title": "Jekyll for clicker questions",
    "section": "",
    "text": "For a few years, I’ve been a fan of clickers (aka personal response systems) for large lecture sections. Clickers are a simple–and scalable–way to incorporate a little widespread active learning in the classroom. They can’t work miracles, but they do allow me to reward attendance, rouse the students once in a while, and give good feedback to all of us about how well the latest concepts are sinking in. I like the accountability: If you got the question wrong when 80% of the class got it right, that’s on you, but if 20% of the class got it right, that’s on me.\nUD is an iclicker shop. When I want to poll the class, I click a “go” button on a small toolbar that overlays any other application. When I’m done, I click “stop.” I can show the results and designate the correct answer on the spot, or I can go back later and pick the right answer while looking at a screenshot from when the question started.\nIn the past I’ve used clickers with handwritten questions projected using a document camera. I don’t get the screenshot this way, but it works fine. However, in the best case I’m left to manage 50-100 sheets of paper for a course. That’s something I’m increasingly cranky about doing in my life overall, and I’m likely to fail at it during the heat of a lecture, especially when (as I like to do) I start replaying questions from past weeks or months. Plus, if I later decide to tweak a question or the answer choices, I’ve got to scrap a page and rewrite it.\nEnter Jekyll. This is a brilliant software t0ol that converts lightly marked data files into a website. It’s blog-centric, but it can be used for other kinds of data as well, and I’ve customized it for collecting clicker questions. You can get it for yourself from this Github repo. It requires being comfortable with a command line, but it’s not otherwise technically challenging.\nFor instance, in one file I have\n---\nlayout: question\nchapter: Introduction\ntitle: Derivative\n\n\n---\n{::comment}\nThe \\dd macro is defined in /_includes/texmacros.md.\n{:/comment}\nWhat is $\\dd{}{x}\\left(e^x\\right)$?\n\n1. *$e^x$*{: #correct}\n1. $x$\n1. $1$\n1. $\\ln(x)$\n1. $\\tan(x)$\nThe page that results can be viewed here.  It’s pretty easy to see how the output arises from the input. All I do is make one file per question, putting them into subdirectories if I want. They’re collected and indexed by the “chapter” property at the top of the file.\nHaving maintained more than one website of HTML files by hand, I found Jekyll to be a revelation. Headers and footers can be included automatically on all pages set to a certain style. (I use this to define MathJax macros in one file that get copied into all the output question pages.)  Content, such as an index or table of contents, can be generated programmatically based on properties of the data. There’s a nice step-by-step series for getting started with Jekyll on the ProfHacker blog.\nJekyll versus raw HTML is like using a power drill/driver versus the Craftsman screwdriver with the hard plastic handle that digs divots into your palm when there’s a job of any decent size. I’ll probably move my personal site this blog over to Jekyll at some point."
  },
  {
    "objectID": "post/trefethen-bau-matlab-julia-iterative-methods/index.html",
    "href": "post/trefethen-bau-matlab-julia-iterative-methods/index.html",
    "title": "Trefethen & Bau & MATLAB & Julia: Iterative methods",
    "section": "",
    "text": "I’m going to wrap up the long-paused MATLAB versus Julia comparison on Trefethen & Bau by chugging through all the lectures on iterative methods in one post.\nI’m back to using gists–not thrilled with any of the mechanisms for sharing this stuff.\n\nLecture 32 (sparse matrices and simple iterations)\nLecture 33 (Arnoldi iteration)\nLecture 34 (Arnoldi eigenvalues)\n\nThese are remarkable mainly in that they have such striking similarity in both languages. Aside from square brackets and working around the 1x1/scalar distinction in Julia, little differs besides the syntax of the eigs command.\nOne frustration, though. I decided to try an interesting alternative to PyPlot in Julia, the Plots package. Actually Plots tries to be a generalization of and alternative route to using PyPlot/matplotlib. I decided to try the PlotlyJS backend instead, however. It makes lovely graphics with very responsive interaction. Since the rendering is in Javascript, I thought it would be perfectly portable, but you can’t see the output in the gist above, even though it should be embedded in the notebook.\nI liked using Plots OK; for the most part it’s just different, not better or worse that I could see. I found it awkward to work with subplots. I ended up creating 4 plots individually and then displaying them in a table using another call to plot. I find MATLAB’s setup more convenient. I also could not figure out how to coax a contour plot with a contour at a specified value, which seems like a big lack.\n\nLecture 35 (GMRES)\nLecture 36 (Lanczos and MINRES)\nLecture 37 (Conjugate gradients)\nLecture 40 (Preconditioning)\n\nAgain the differences are minor. In sparse and iterative methods I found Julia to place a greater emphasis on keyword arguments. For example,\n(xCG,~,~,~,resnorm) = cg(A,b,tol=1e-14,maxIter=100);\nThere are default values for tol and maxIter, but if you want to override them you must type the keyword. On the other hand, MATLAB’s arguments are purely positional:\n[xCG,~,~,~,resnorm] = pcg(A,b,1e-14,100);\nIf I wanted to specify the maximum number of iterations without changing the default tolerance, then I would need to use an empty matrix in the third position. When one uses a command that does take named parameters as inputs, it’s typically done using 'propname',propval pairs. Except when it isn’t, such as for ODEs and optimization. Confusing! As a user I don’t love typing out the keywords, but Julia at least lets me skip the quote marks. I also know from experience that Julia’s version is a lot easier and clearer to implement on the other side.\nSo that’s that. I feel that I am at least ready to get off the bunny slopes with Julia. I haven’t found a compelling reason to switch to it, aside from supporting open source software for science (no small thing). Of course I’ve barely scratched the surface. On the flip side, MATLAB has a lot of well-designed and -maintained packages, and its environment still makes a smoother experience for newcomers. If you can afford it, it’s still a great option for interactive numerical computing.\nI wonder about the future of Julia. Had Python not gotten a head start, I could see an outpouring of effort to make high-quality Julia packages and Julia being a complete MATLAB reboot. But numpy and scipy do exist, and despite their flaws, they have a huge first-mover advantage. It’s a snap to use Python packages in Julia, so there’s not a dichotomy here. But if the package you want to use a lot exists only in Python, the case for Julia weakens. Overall though, it’s a nice thing that we have several strong, expressive high-level environments for numerical computing. Happy coding!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Toby Driscoll",
    "section": "",
    "text": "Toby Driscoll is a Unidel Chaired Professor in the Department of Mathematical Sciences at the University of Delaware.\nCV"
  },
  {
    "objectID": "index.html#ud-affiliations",
    "href": "index.html#ud-affiliations",
    "title": "Toby Driscoll",
    "section": "UD Affiliations",
    "text": "UD Affiliations\n\nDepartment of Biomedical Engineering\nData Science Institute\nDirector, Master of Science in Data Science program"
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "Toby Driscoll",
    "section": "Contact info",
    "text": "Contact info\n\nEmail: driscoll@udel.edu\nOffice: Ewing Hall 515\nZoom: https://udel.zoom.us/my/tobydriscoll"
  },
  {
    "objectID": "index.html#send-a-message",
    "href": "index.html#send-a-message",
    "title": "Toby Driscoll",
    "section": "Send a message",
    "text": "Send a message\n\n \n\nYour name\n  \n\n\nEmail\n\n\n\nMessage"
  },
  {
    "objectID": "project/drums/index.html",
    "href": "project/drums/index.html",
    "title": "Isospectral drums",
    "section": "",
    "text": "In 1991, mathematicians Gordon, Webb, and Wolpert (Invent. Math. 110, pp 1–22) solved a famous problem posed by M. Kac: “Can one hear the shape of a drum?” That is, do the Dirichlet eigenvalues of a membrane determine the shape of the membrane? Their answer was “No!”, and they used a powerful mathematical technique to produce a counterexample, which in its simplest form is a pair of eight-sided nonconvex polygons. Gordon, Webb, and Wolpert also noted that a more elementary technique known as transplantation can be used to show that the spectrum of the Laplacian with Dirichlet conditions is the same for both regions. However, actually finding what the eigenvalues are is far more difficult; it’s essentially impossible to do analytically.\nWu, Sprung, and Martorell (Physical Review E , Jan 1995) were among the first to present the results of computations of modes for these shapes, but the accuracy of their results was not clear. Physicists Sridhar and Kudrolli at Northeastern University built microwave cavities in the shapes of the two polygons and determined 54 eigenvalues experimentally (Physical Review Letters, April 4, 1994,Science News, September 17, 1994). Their results were accurate only to about 0.1%.\nA little-known method due to Descloux and Tolley performed more accurately than all of these. The underlying principle is to exploit the well-known expansions of an eigenfunction near the corners. The criterion that selects eigenvalues is the matching of values and normal derivatives of the local expansions at interfaces within the polygons. Using this method in MATLAB, and incorporating a crucial improvement, in 1997 I published eigenvalues and modes 1–25 for both regions that were accurate to twelve digits.\nOne of the most basic uses of eigenmodes is to represent vibrations governed by the wave equation. Below are a few animations of such vibrations, based on different combinations of the first sixteen modes. Each movie runs for three periods of the first mode. (These don’t look great now, but they were made in 1995.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe story of these and other isospectral shapes continued to develop from there, mathematically and computationally, but I was not much involved with most of it."
  },
  {
    "objectID": "project/sc-toolbox/index.html",
    "href": "project/sc-toolbox/index.html",
    "title": "Schwarz–Christoffel Toolbox for MATLAB",
    "section": "",
    "text": "The SC Toolbox is a problem-solving environment for computation and interaction with conformal maps to regions bounded by polygons, including unbounded regions, logical quadrilaterals, and channels. It includes a module for the solution of the Laplace equation on such regions with piecewise constant boundary conditions, to ten digits or more in seconds. Nearly all of the Toolbox functions are accessible through a graphical interface.\nThe software is distributed primarily on Github. You can directly download a zip archive of the latest release.\nThere is an old user guide that remains fairly relevant. There are also many usage examples in my book, which has a lot of mathematical and computational information about S–C maps.\nIf you need a copy for an older version of MATLAB (major versions 4,5,6), please contact me.\n\nGallery\nForgive me: These pictures are old, made in the 1900’s, and they lack modern pizazz."
  },
  {
    "objectID": "book/learning-matlab/index.html",
    "href": "book/learning-matlab/index.html",
    "title": "Learning MATLAB",
    "section": "",
    "text": "Buy at the SIAM bookstore. Members of SIAM, including student members, get a 30% discount.\n\nBuy at Amazon in the U.S.\n\n\n\n\n\n\n\nLearning MATLAB is a concise, essentials-only introduction to MATLAB for those who have programming experience in other procedural languages. At about 100 pages, it’s meant as a supplemental guide in a numerical analysis or scientific computing course or as a standalone tutorial for those who need to get started quickly in MATLAB. Learning MATLAB does not cover every feature or calling syntax, but focuses on those parts of MATLAB that have proven themselves indispensible to me in my 20 years as a MATLAB programmer. (Fun fact: I was a runaway winner of “MATLAB Jeopardy” at the 1995 MATLAB Conference.) The book grew out of a summer workshop that I taught to grad students for 8 years at Delaware.\nThe chapters in Learning MATLAB are: Introduction, Arrays and matrices, Scripts and functions, More on functions, Graphics, Advanced techniques, and Scientific computing. Each feature is demonstrated by examples, and every chapter includes exercises ranging from routine to very challenging."
  },
  {
    "objectID": "book/explODE/index.html",
    "href": "book/explODE/index.html",
    "title": "Exploring ODEs",
    "section": "",
    "text": "Buy from the SIAM bookstore. Members of SIAM, including student members, get a 30% discount.\nDownload the PDF for free.\n\n\n\n\n\n\n\nWritten by Nick Trefethen, Asgeir Birkisson, and myself, Exploring ODEs takes a look at ordinary differential equations unlike any other text. Rather than focusing on the mechanics of finding solutions in a limited number of special problems, we use Chebfun to illustrate the wide range of behavior of those solutions for a variety of linear, nonlinear, and multidimensional problems, including initial-value, boundary-value, and eigenvalue problems. Each short chapter includes a detailed application and a favorite reference. The text is accessible as an accompaniment or follow-up to a standard first undergraduate text on ODEs. All of the MATLAB codes generating the examples and figures are available for download, as is the full text in PDF, from the book’s website.\n\nContents\n\nIntroduction\nFirst-order scalar linear ODEs\nFirst-order scalar nonlinear ODEs\nSecond-order equations and damped oscillations\nBoundary-value problems\nEigenvalues of linear BVPs\nVariable coefficients and adjoints\nResonance\nSecond-order equations in the phase plane\nSystems of equations\nThe fundamental existence theorem\nRandom functions and random ODEs\nChaos\nLinear systems and linearization\nStable and unstable fixed points\nMultiple solutions of nonlinear BVPs\nBifurcation\nContinuation and path-following\nPeriodic ODEs\nBoundary and interior layers\nInto the complex plane\nTime-dependent PDEs\nAppendix A: Chebfun and its ODE algorithms\nAppendix B: 100 more examples"
  },
  {
    "objectID": "book/schwarz-christoffel-mapping/index.html",
    "href": "book/schwarz-christoffel-mapping/index.html",
    "title": "Schwarz–Christoffel Mapping",
    "section": "",
    "text": "See the table of contents of this book\nBuy the book directly from CUP\nBuy the book at Amazon (U.S.)\n\n\n\n\n\n\n\nSchwarz-Christoffel Mapping is a monograph by myself and Nick Trefethen on the constructive and computational aspects of Schwarz–Christoffel conformal maps. These maps transform the interior or exterior of a region bounded by a polygon to the interior of a disk, half-plane, strip, rectangle, or more exotic canonical region. Unlike many other numerical conformal mapping methods, they are substantially faster to compute than the full solution of an elliptic partial differential equation on the same domain. Because the maps are conformal, they offer a powerful way to solve certain classical problems for the Laplacian operator.\nThe book includes 76 quantitatively precise figures illustrating theoretical and applied aspects of Schwarz–Christoffel maps. The appendix includes code snippets that produce some of these figures using my free Schwarz-Christoffel Toolbox for MATLAB."
  }
]